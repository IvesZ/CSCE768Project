{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1168daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "TensorFlow version: 2.5.0\n",
      "GPU info: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpu_devices) > 0:\n",
    "    for gpu in gpu_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print('GPU is available')\n",
    "else:\n",
    "    print('GPU is not available')\n",
    "\n",
    "# 打印TensorFlow版本和GPU信息\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "print('GPU info:', tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab7fb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\woodwork\\__init__.py:23: FutureWarning: Woodwork may not support Python 3.7 in next non-bugfix release.\n",
      "  \"Woodwork may not support Python 3.7 in next non-bugfix release.\", FutureWarning\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\featuretools\\__init__.py:67: FutureWarning: Featuretools may not support Python 3.7 in next non-bugfix release.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from deeptables.models import deeptable, deepnets\n",
    "from deeptables.datasets import dsutils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import deeptables.datasets as dt_datasets\n",
    "from deeptables.models.deeptable import DeepTable, ModelConfig\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2e0e59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3553: DtypeWarning: Columns (56,58,63) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "data0 = pd.read_csv(\"D:/Download/Android_Malware.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8173bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0.head()\n",
    "data0.columns = [col[1:] if col.startswith(' ') else col for col in data0.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0c13010",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0=data0.dropna()\n",
    "data0=data0.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a002fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Counts and Percentages:\n",
      "\n",
      "                 Label  Counts  Percentage\n",
      "0       Android_Adware  147443   41.460129\n",
      "1    Android_Scareware  117081   32.922508\n",
      "2  Android_SMS_Malware   67394   18.950808\n",
      "3               Benign   23708    6.666554\n",
      "\n",
      "Labels: ['Android_Adware', 'Android_SMS_Malware', 'Android_Scareware', 'Benign']\n"
     ]
    }
   ],
   "source": [
    "label_counts = data0['Label'].value_counts().rename_axis('Label').reset_index(name='Counts')\n",
    "label_percentages = data0['Label'].value_counts(normalize=True).rename_axis('Label').reset_index(name='Percentage')\n",
    "\n",
    "result = pd.merge(label_counts, label_percentages, on='Label')\n",
    "result['Percentage'] = result['Percentage'] * 100\n",
    "\n",
    "print(\"Label Counts and Percentages:\\n\")\n",
    "print(result)\n",
    "\n",
    "names = sorted(data0['Label'].unique().tolist())\n",
    "print(\"\\nLabels:\", names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "715c7d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=list(range(len(names)))\n",
    "normal_mapping=dict(zip(names,N)) \n",
    "reverse_mapping=dict(zip(N,names))       \n",
    "data0['Label']=data0['Label'].map(normal_mapping)\n",
    "\n",
    "data0['Down/Up Ratio']=data0['Down/Up Ratio'].apply(lambda x:int(x))\n",
    "data0=data0.drop('Timestamp',axis=1)\n",
    "data0=data0.drop('CWE Flag Count',axis=1)\n",
    "data0=data0.drop('Fwd Avg Bytes/Bulk',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "574459c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelencoder(df):\n",
    "    for c in df.columns:\n",
    "        #print(c)\n",
    "        if df[c].dtype=='object': \n",
    "            #df[c] = df[c].fillna('missing')\n",
    "            df[c] = df[c].fillna('N')\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(df[c].values))\n",
    "            df[c] = lbl.transform(df[c].values)\n",
    "    return df\n",
    "\n",
    "data0=labelencoder(data0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "759c8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0=data0.drop('Flow ID',axis=1)\n",
    "# data0=data0.drop('Source IP',axis=1)\n",
    "# data0=data0.drop('Destination IP',axis=1)\n",
    "# data0=data0.drop('Source Port',axis=1)\n",
    "# data0=data0.drop('Destination Port',axis=1)\n",
    "dataY=data0['Label']\n",
    "dataX=data0.drop(['Label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c02f5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import IsolationForest\n",
    "# clf = IsolationForest(random_state=42, contamination='auto')\n",
    "# clf.fit(data0)\n",
    "# outliers = clf.predict(data0) == -1\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# sns.boxplot(data=data0)\n",
    "# plt.title('Boxplot of Data')\n",
    "# plt.show()\n",
    "# print('离群点数:', np.sum(outliers))\n",
    "# print('数据集中有{}个数据点。'.format(len(data0)))\n",
    "# data0 = data0[~outliers]\n",
    "# print('删除离群点后，数据集中剩余{}个数据点。'.format(len(data0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28b2ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 20))\n",
    "# sns.boxplot(data=data0)\n",
    "# plt.title('Boxplot of Data')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "429c317a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Counts and Percentages:\n",
      "\n",
      "   Label  Counts  Percentage\n",
      "0      0  147443   41.460129\n",
      "1      2  117081   32.922508\n",
      "2      1   67394   18.950808\n",
      "3      3   23708    6.666554\n",
      "\n",
      "Labels: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "label_counts = data0['Label'].value_counts().rename_axis('Label').reset_index(name='Counts')\n",
    "label_percentages = data0['Label'].value_counts(normalize=True).rename_axis('Label').reset_index(name='Percentage')\n",
    "\n",
    "result = pd.merge(label_counts, label_percentages, on='Label')\n",
    "result['Percentage'] = result['Percentage'] * 100\n",
    "\n",
    "print(\"Label Counts and Percentages:\\n\")\n",
    "print(result)\n",
    "\n",
    "names2 = sorted(data0['Label'].unique().tolist())\n",
    "print(\"\\nLabels:\", names2)\n",
    "\n",
    "\n",
    "# Label Counts and Percentages:\n",
    "\n",
    "#                  Label  Counts  Percentage\n",
    "# 0       Android_Adware  147443   41.460129\n",
    "# 1    Android_Scareware  117081   32.922508\n",
    "# 2  Android_SMS_Malware   67394   18.950808\n",
    "# 3               Benign   23708    6.666554"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da18e5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source IP</th>\n",
       "      <th>Source Port</th>\n",
       "      <th>Destination IP</th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <th>Total Backward Packets</th>\n",
       "      <th>Total Length of Fwd Packets</th>\n",
       "      <th>Total Length of Bwd Packets</th>\n",
       "      <th>...</th>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>50004</td>\n",
       "      <td>1283</td>\n",
       "      <td>443.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>37027</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>35455</td>\n",
       "      <td>1283</td>\n",
       "      <td>443.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>36653</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>51775</td>\n",
       "      <td>648</td>\n",
       "      <td>443.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>534099</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>11924.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>51775</td>\n",
       "      <td>648</td>\n",
       "      <td>443.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9309</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>51776</td>\n",
       "      <td>648</td>\n",
       "      <td>443.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19890496</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>430.0</td>\n",
       "      <td>5679.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355625</th>\n",
       "      <td>717</td>\n",
       "      <td>80</td>\n",
       "      <td>17</td>\n",
       "      <td>38405.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>126711</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355626</th>\n",
       "      <td>9</td>\n",
       "      <td>7632</td>\n",
       "      <td>15</td>\n",
       "      <td>53.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>48012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355627</th>\n",
       "      <td>9</td>\n",
       "      <td>45970</td>\n",
       "      <td>99</td>\n",
       "      <td>443.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20028018</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>6335.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>367528.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>367528.0</td>\n",
       "      <td>367528.0</td>\n",
       "      <td>19660490.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19660490.0</td>\n",
       "      <td>19660490.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355628</th>\n",
       "      <td>9</td>\n",
       "      <td>51982</td>\n",
       "      <td>15</td>\n",
       "      <td>53.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>347926</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355629</th>\n",
       "      <td>9</td>\n",
       "      <td>9320</td>\n",
       "      <td>15</td>\n",
       "      <td>53.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>125473</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>355626 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Source IP  Source Port  Destination IP  Destination Port  Protocol  \\\n",
       "0               9        50004            1283             443.0       6.0   \n",
       "1               9        35455            1283             443.0       6.0   \n",
       "2               9        51775             648             443.0       6.0   \n",
       "3               9        51775             648             443.0       6.0   \n",
       "4               9        51776             648             443.0       6.0   \n",
       "...           ...          ...             ...               ...       ...   \n",
       "355625        717           80              17           38405.0       6.0   \n",
       "355626          9         7632              15              53.0      17.0   \n",
       "355627          9        45970              99             443.0       6.0   \n",
       "355628          9        51982              15              53.0      17.0   \n",
       "355629          9         9320              15              53.0      17.0   \n",
       "\n",
       "        Flow Duration  Total Fwd Packets  Total Backward Packets  \\\n",
       "0               37027                  1                       1   \n",
       "1               36653                  1                       1   \n",
       "2              534099                  8                      12   \n",
       "3                9309                  3                       0   \n",
       "4            19890496                  8                       6   \n",
       "...               ...                ...                     ...   \n",
       "355625         126711                  1                       1   \n",
       "355626          48012                  1                       1   \n",
       "355627       20028018                 11                       8   \n",
       "355628         347926                  1                       1   \n",
       "355629         125473                  1                       1   \n",
       "\n",
       "        Total Length of Fwd Packets  Total Length of Bwd Packets  ...  \\\n",
       "0                               0.0                          0.0  ...   \n",
       "1                               0.0                          0.0  ...   \n",
       "2                            1011.0                      11924.0  ...   \n",
       "3                               0.0                          0.0  ...   \n",
       "4                             430.0                       5679.0  ...   \n",
       "...                             ...                          ...  ...   \n",
       "355625                          0.0                          0.0  ...   \n",
       "355626                         30.0                        140.0  ...   \n",
       "355627                        339.0                       6335.0  ...   \n",
       "355628                         32.0                         48.0  ...   \n",
       "355629                         30.0                         92.0  ...   \n",
       "\n",
       "        act_data_pkt_fwd  min_seg_size_forward  Active Mean  Active Std  \\\n",
       "0                    0.0                  32.0          0.0         0.0   \n",
       "1                    0.0                  32.0          0.0         0.0   \n",
       "2                    3.0                  20.0          0.0         0.0   \n",
       "3                    0.0                  20.0          0.0         0.0   \n",
       "4                    2.0                  20.0          0.0         0.0   \n",
       "...                  ...                   ...          ...         ...   \n",
       "355625               0.0                  32.0          0.0         0.0   \n",
       "355626               0.0                  32.0          0.0         0.0   \n",
       "355627               2.0                  20.0     367528.0         0.0   \n",
       "355628               0.0                  20.0          0.0         0.0   \n",
       "355629               0.0                  32.0          0.0         0.0   \n",
       "\n",
       "        Active Max  Active Min   Idle Mean  Idle Std    Idle Max    Idle Min  \n",
       "0              0.0         0.0         0.0       0.0         0.0         0.0  \n",
       "1              0.0         0.0         0.0       0.0         0.0         0.0  \n",
       "2              0.0         0.0         0.0       0.0         0.0         0.0  \n",
       "3              0.0         0.0         0.0       0.0         0.0         0.0  \n",
       "4              0.0         0.0         0.0       0.0         0.0         0.0  \n",
       "...            ...         ...         ...       ...         ...         ...  \n",
       "355625         0.0         0.0         0.0       0.0         0.0         0.0  \n",
       "355626         0.0         0.0         0.0       0.0         0.0         0.0  \n",
       "355627    367528.0    367528.0  19660490.0       0.0  19660490.0  19660490.0  \n",
       "355628         0.0         0.0         0.0       0.0         0.0         0.0  \n",
       "355629         0.0         0.0         0.0       0.0         0.0         0.0  \n",
       "\n",
       "[355626 rows x 80 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataY=data0['Label']\n",
    "dataX=data0.drop(['Label'],axis=1)\n",
    "all_columns = list(dataX.columns)\n",
    "dataX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7db62a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "dataX_std = scaler.fit_transform(dataX)\n",
    "dataX = pd.DataFrame(dataX_std, columns=dataX.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e8d4f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source IP</th>\n",
       "      <th>Source Port</th>\n",
       "      <th>Destination IP</th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <th>Total Backward Packets</th>\n",
       "      <th>Total Length of Fwd Packets</th>\n",
       "      <th>Total Length of Bwd Packets</th>\n",
       "      <th>...</th>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.270776</td>\n",
       "      <td>0.608170</td>\n",
       "      <td>0.041928</td>\n",
       "      <td>-0.331393</td>\n",
       "      <td>-0.511329</td>\n",
       "      <td>-0.499473</td>\n",
       "      <td>-0.094880</td>\n",
       "      <td>-0.049534</td>\n",
       "      <td>-0.034977</td>\n",
       "      <td>-0.040864</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102047</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.184644</td>\n",
       "      <td>-0.07675</td>\n",
       "      <td>-0.181619</td>\n",
       "      <td>-0.172910</td>\n",
       "      <td>-0.279270</td>\n",
       "      <td>-0.110393</td>\n",
       "      <td>-0.284337</td>\n",
       "      <td>-0.268581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.270776</td>\n",
       "      <td>-0.206160</td>\n",
       "      <td>0.041928</td>\n",
       "      <td>-0.331393</td>\n",
       "      <td>-0.511329</td>\n",
       "      <td>-0.499490</td>\n",
       "      <td>-0.094880</td>\n",
       "      <td>-0.049534</td>\n",
       "      <td>-0.034977</td>\n",
       "      <td>-0.040864</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102047</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.184644</td>\n",
       "      <td>-0.07675</td>\n",
       "      <td>-0.181619</td>\n",
       "      <td>-0.172910</td>\n",
       "      <td>-0.279270</td>\n",
       "      <td>-0.110393</td>\n",
       "      <td>-0.284337</td>\n",
       "      <td>-0.268581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.270776</td>\n",
       "      <td>0.707296</td>\n",
       "      <td>-0.445489</td>\n",
       "      <td>-0.331393</td>\n",
       "      <td>-0.511329</td>\n",
       "      <td>-0.476680</td>\n",
       "      <td>0.009590</td>\n",
       "      <td>0.007854</td>\n",
       "      <td>0.017087</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098779</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>-0.184644</td>\n",
       "      <td>-0.07675</td>\n",
       "      <td>-0.181619</td>\n",
       "      <td>-0.172910</td>\n",
       "      <td>-0.279270</td>\n",
       "      <td>-0.110393</td>\n",
       "      <td>-0.284337</td>\n",
       "      <td>-0.268581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.270776</td>\n",
       "      <td>0.707296</td>\n",
       "      <td>-0.445489</td>\n",
       "      <td>-0.331393</td>\n",
       "      <td>-0.511329</td>\n",
       "      <td>-0.500744</td>\n",
       "      <td>-0.065031</td>\n",
       "      <td>-0.054751</td>\n",
       "      <td>-0.034977</td>\n",
       "      <td>-0.040864</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102047</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>-0.184644</td>\n",
       "      <td>-0.07675</td>\n",
       "      <td>-0.181619</td>\n",
       "      <td>-0.172910</td>\n",
       "      <td>-0.279270</td>\n",
       "      <td>-0.110393</td>\n",
       "      <td>-0.284337</td>\n",
       "      <td>-0.268581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.270776</td>\n",
       "      <td>0.707352</td>\n",
       "      <td>-0.445489</td>\n",
       "      <td>-0.331393</td>\n",
       "      <td>-0.511329</td>\n",
       "      <td>0.410875</td>\n",
       "      <td>0.009590</td>\n",
       "      <td>-0.023449</td>\n",
       "      <td>-0.012833</td>\n",
       "      <td>-0.020316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031837</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>-0.184644</td>\n",
       "      <td>-0.07675</td>\n",
       "      <td>-0.181619</td>\n",
       "      <td>-0.172910</td>\n",
       "      <td>-0.279270</td>\n",
       "      <td>-0.110393</td>\n",
       "      <td>-0.284337</td>\n",
       "      <td>-0.268581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355621</th>\n",
       "      <td>1.475602</td>\n",
       "      <td>-2.186156</td>\n",
       "      <td>-0.929835</td>\n",
       "      <td>2.313878</td>\n",
       "      <td>-0.511329</td>\n",
       "      <td>-0.495361</td>\n",
       "      <td>-0.094880</td>\n",
       "      <td>-0.049534</td>\n",
       "      <td>-0.034977</td>\n",
       "      <td>-0.040864</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102047</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.184644</td>\n",
       "      <td>-0.07675</td>\n",
       "      <td>-0.181619</td>\n",
       "      <td>-0.172910</td>\n",
       "      <td>-0.279270</td>\n",
       "      <td>-0.110393</td>\n",
       "      <td>-0.284337</td>\n",
       "      <td>-0.268581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355622</th>\n",
       "      <td>-0.270776</td>\n",
       "      <td>-1.763458</td>\n",
       "      <td>-0.931370</td>\n",
       "      <td>-0.358569</td>\n",
       "      <td>1.897171</td>\n",
       "      <td>-0.498969</td>\n",
       "      <td>-0.094880</td>\n",
       "      <td>-0.049534</td>\n",
       "      <td>-0.033432</td>\n",
       "      <td>-0.040357</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102047</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.184644</td>\n",
       "      <td>-0.07675</td>\n",
       "      <td>-0.181619</td>\n",
       "      <td>-0.172910</td>\n",
       "      <td>-0.279270</td>\n",
       "      <td>-0.110393</td>\n",
       "      <td>-0.284337</td>\n",
       "      <td>-0.268581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355623</th>\n",
       "      <td>-0.270776</td>\n",
       "      <td>0.382381</td>\n",
       "      <td>-0.866893</td>\n",
       "      <td>-0.331393</td>\n",
       "      <td>-0.511329</td>\n",
       "      <td>0.417180</td>\n",
       "      <td>0.054363</td>\n",
       "      <td>-0.013014</td>\n",
       "      <td>-0.017519</td>\n",
       "      <td>-0.017943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031837</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>0.232161</td>\n",
       "      <td>-0.07675</td>\n",
       "      <td>0.178888</td>\n",
       "      <td>0.263395</td>\n",
       "      <td>1.084606</td>\n",
       "      <td>-0.110393</td>\n",
       "      <td>1.014238</td>\n",
       "      <td>1.128714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355624</th>\n",
       "      <td>-0.270776</td>\n",
       "      <td>0.718882</td>\n",
       "      <td>-0.931370</td>\n",
       "      <td>-0.358569</td>\n",
       "      <td>1.897171</td>\n",
       "      <td>-0.485217</td>\n",
       "      <td>-0.094880</td>\n",
       "      <td>-0.049534</td>\n",
       "      <td>-0.033329</td>\n",
       "      <td>-0.040690</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102047</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>-0.184644</td>\n",
       "      <td>-0.07675</td>\n",
       "      <td>-0.181619</td>\n",
       "      <td>-0.172910</td>\n",
       "      <td>-0.279270</td>\n",
       "      <td>-0.110393</td>\n",
       "      <td>-0.284337</td>\n",
       "      <td>-0.268581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355625</th>\n",
       "      <td>-0.270776</td>\n",
       "      <td>-1.668978</td>\n",
       "      <td>-0.931370</td>\n",
       "      <td>-0.358569</td>\n",
       "      <td>1.897171</td>\n",
       "      <td>-0.495417</td>\n",
       "      <td>-0.094880</td>\n",
       "      <td>-0.049534</td>\n",
       "      <td>-0.033432</td>\n",
       "      <td>-0.040531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102047</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>-0.184644</td>\n",
       "      <td>-0.07675</td>\n",
       "      <td>-0.181619</td>\n",
       "      <td>-0.172910</td>\n",
       "      <td>-0.279270</td>\n",
       "      <td>-0.110393</td>\n",
       "      <td>-0.284337</td>\n",
       "      <td>-0.268581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>355626 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Source IP  Source Port  Destination IP  Destination Port  Protocol  \\\n",
       "0       -0.270776     0.608170        0.041928         -0.331393 -0.511329   \n",
       "1       -0.270776    -0.206160        0.041928         -0.331393 -0.511329   \n",
       "2       -0.270776     0.707296       -0.445489         -0.331393 -0.511329   \n",
       "3       -0.270776     0.707296       -0.445489         -0.331393 -0.511329   \n",
       "4       -0.270776     0.707352       -0.445489         -0.331393 -0.511329   \n",
       "...           ...          ...             ...               ...       ...   \n",
       "355621   1.475602    -2.186156       -0.929835          2.313878 -0.511329   \n",
       "355622  -0.270776    -1.763458       -0.931370         -0.358569  1.897171   \n",
       "355623  -0.270776     0.382381       -0.866893         -0.331393 -0.511329   \n",
       "355624  -0.270776     0.718882       -0.931370         -0.358569  1.897171   \n",
       "355625  -0.270776    -1.668978       -0.931370         -0.358569  1.897171   \n",
       "\n",
       "        Flow Duration  Total Fwd Packets  Total Backward Packets  \\\n",
       "0           -0.499473          -0.094880               -0.049534   \n",
       "1           -0.499490          -0.094880               -0.049534   \n",
       "2           -0.476680           0.009590                0.007854   \n",
       "3           -0.500744          -0.065031               -0.054751   \n",
       "4            0.410875           0.009590               -0.023449   \n",
       "...               ...                ...                     ...   \n",
       "355621      -0.495361          -0.094880               -0.049534   \n",
       "355622      -0.498969          -0.094880               -0.049534   \n",
       "355623       0.417180           0.054363               -0.013014   \n",
       "355624      -0.485217          -0.094880               -0.049534   \n",
       "355625      -0.495417          -0.094880               -0.049534   \n",
       "\n",
       "        Total Length of Fwd Packets  Total Length of Bwd Packets  ...  \\\n",
       "0                         -0.034977                    -0.040864  ...   \n",
       "1                         -0.034977                    -0.040864  ...   \n",
       "2                          0.017087                     0.002279  ...   \n",
       "3                         -0.034977                    -0.040864  ...   \n",
       "4                         -0.012833                    -0.020316  ...   \n",
       "...                             ...                          ...  ...   \n",
       "355621                    -0.034977                    -0.040864  ...   \n",
       "355622                    -0.033432                    -0.040357  ...   \n",
       "355623                    -0.017519                    -0.017943  ...   \n",
       "355624                    -0.033329                    -0.040690  ...   \n",
       "355625                    -0.033432                    -0.040531  ...   \n",
       "\n",
       "        act_data_pkt_fwd  min_seg_size_forward  Active Mean  Active Std  \\\n",
       "0              -0.102047              0.005114    -0.184644    -0.07675   \n",
       "1              -0.102047              0.005114    -0.184644    -0.07675   \n",
       "2               0.098779              0.005112    -0.184644    -0.07675   \n",
       "3              -0.102047              0.005112    -0.184644    -0.07675   \n",
       "4               0.031837              0.005112    -0.184644    -0.07675   \n",
       "...                  ...                   ...          ...         ...   \n",
       "355621         -0.102047              0.005114    -0.184644    -0.07675   \n",
       "355622         -0.102047              0.005114    -0.184644    -0.07675   \n",
       "355623          0.031837              0.005112     0.232161    -0.07675   \n",
       "355624         -0.102047              0.005112    -0.184644    -0.07675   \n",
       "355625         -0.102047              0.005114    -0.184644    -0.07675   \n",
       "\n",
       "        Active Max  Active Min  Idle Mean  Idle Std  Idle Max  Idle Min  \n",
       "0        -0.181619   -0.172910  -0.279270 -0.110393 -0.284337 -0.268581  \n",
       "1        -0.181619   -0.172910  -0.279270 -0.110393 -0.284337 -0.268581  \n",
       "2        -0.181619   -0.172910  -0.279270 -0.110393 -0.284337 -0.268581  \n",
       "3        -0.181619   -0.172910  -0.279270 -0.110393 -0.284337 -0.268581  \n",
       "4        -0.181619   -0.172910  -0.279270 -0.110393 -0.284337 -0.268581  \n",
       "...            ...         ...        ...       ...       ...       ...  \n",
       "355621   -0.181619   -0.172910  -0.279270 -0.110393 -0.284337 -0.268581  \n",
       "355622   -0.181619   -0.172910  -0.279270 -0.110393 -0.284337 -0.268581  \n",
       "355623    0.178888    0.263395   1.084606 -0.110393  1.014238  1.128714  \n",
       "355624   -0.181619   -0.172910  -0.279270 -0.110393 -0.284337 -0.268581  \n",
       "355625   -0.181619   -0.172910  -0.279270 -0.110393 -0.284337 -0.268581  \n",
       "\n",
       "[355626 rows x 80 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70d4280c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Header Length.1', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max']\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = ['Protocol','Fwd PSH Flags','Source IP','Destination IP','Source Port','Destination Port',\n",
    "                       'Bwd PSH Flags','Fwd URG Flags','Bwd URG Flags']\n",
    "# categorical_columns = ['Protocol','Fwd PSH Flags', 'Bwd PSH Flags','Fwd URG Flags','Bwd URG Flags']\n",
    "numerical_columns = [col for col in all_columns if col not in categorical_columns]\n",
    "numerical_columns = numerical_columns[:-1]\n",
    "print(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98a4e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(dataX, dataY, test_size=0.2, stratify=dataY, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size=0.5, stratify=y_val, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "045ff685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    147443\n",
      "2    117081\n",
      "1     67394\n",
      "3     23708\n",
      "Name: Label, dtype: int64\n",
      "{0: 1.0, 1: 2.1877763599133453, 2: 1.2593247409912796, 3: 6.219124346212249}\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "batch_size=1024\n",
    "counts = dataY.value_counts()\n",
    "print(counts)\n",
    "class_weight = {i: max(counts) / counts[i] for i in range(len(counts))}\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaadfdbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(x_train, y_train)\n",
    "y_pred = rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a871a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61246239068695\n",
      "============val_data===================\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "     Android_Adware     0.6584    0.7594    0.7053     14745\n",
      "Android_SMS_Malware     0.6539    0.5263    0.5832      6739\n",
      "  Android_Scareware     0.5417    0.5669    0.5540     11708\n",
      "             Benign     0.4540    0.1687    0.2460      2371\n",
      "\n",
      "           accuracy                         0.6125     35563\n",
      "          macro avg     0.5770    0.5053    0.5221     35563\n",
      "       weighted avg     0.6055    0.6125    0.6017     35563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", rf.score(x_test, y_test))\n",
    "print('============val_data===================')\n",
    "t_names = [str(name) for name in names]\n",
    "print(classification_report(y_test, y_pred, target_names=t_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b7ae00af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.43702724742007143\n",
      "============val_data===================\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "     Android_Adware     0.4889    0.7428    0.5896     14745\n",
      "Android_SMS_Malware     0.8214    0.2211    0.3484      6739\n",
      "  Android_Scareware     0.4030    0.3906    0.3967     11708\n",
      "             Benign     0.0000    0.0000    0.0000      2371\n",
      "\n",
      "           accuracy                         0.4784     35563\n",
      "          macro avg     0.4283    0.3386    0.3337     35563\n",
      "       weighted avg     0.4910    0.4784    0.4411     35563\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights = np.sum(class_counts) / (len(class_counts) * class_counts)\n",
    "sample_weights = class_weights[y_train]\n",
    "adaboost = AdaBoostClassifier()\n",
    "adaboost = AdaBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "adaboost.fit(x_train, y_train,sample_weight=sample_weights)\n",
    "\n",
    "print(\"Accuracy:\", adaboost.score(x_test, y_test))\n",
    "print('============val_data===================')\n",
    "t_names = [str(name) for name in names]\n",
    "print(classification_report(y_test, y_pred, target_names=t_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ef3980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(\n",
    "    nets=['dnn_nets'],\n",
    "    categorical_columns=categorical_columns,\n",
    "#     apply_gbm_features=True,\n",
    "#     auto_discrete=True,\n",
    "#     auto_categorize=True,\n",
    "#     cat_exponent=0.7,\n",
    "    metrics=[ 'accuracy','AUC'],\n",
    "    pnn_params={\n",
    "        'outer_product_kernel_type': 'mat',\n",
    "    },\n",
    "    afm_params={\n",
    "        'attention_factor': 5,\n",
    "        'dropout_rate': 0.3,\n",
    "    },\n",
    "    fgcnn_params={\n",
    "        'fg_filters': (14, 16),\n",
    "        'fg_widths': (7, 7),\n",
    "        'fg_pool_widths': (2, 2),\n",
    "        'fg_new_feat_filters': (2, 2),\n",
    "    },\n",
    "    fibinet_params={\n",
    "        'senet_pooling_op': 'mean',\n",
    "        'senet_reduction_ratio': 3,\n",
    "        'bilinear_type': 'field_interaction',\n",
    "    },\n",
    "    cross_params={\n",
    "        'num_cross_layer': 10,\n",
    "    },\n",
    "    autoint_params={\n",
    "        'num_attention': 5,\n",
    "        'num_heads': 1,\n",
    "        'dropout_rate': 0.2,\n",
    "        'use_residual': True,\n",
    "    },\n",
    "    apply_class_weight=True,\n",
    "    stacking_op='concat',\n",
    "    dnn_params={\n",
    "        'hidden_units':((128, 0.2, False),(128, 0.2, False),(128, 0.2, False),(128, 0.2, False),(128, 0.2, False),\n",
    "                       ),\n",
    "        'activation': 'relu',\n",
    "    },\n",
    "    cin_params={\n",
    "        'cross_layer_size': (128, 128),\n",
    "        'activation': 'relu',\n",
    "        'use_residual': True,\n",
    "        'use_bias': False,\n",
    "        'direct': True,\n",
    "        'reduce_D': True,\n",
    "    },\n",
    "    earlystopping_patience=5,\n",
    "    embeddings_output_dim=100,\n",
    "    embedding_dropout=0,\n",
    "    dense_dropout=0.2,\n",
    "    home_dir='E:/temp',\n",
    "#     embeddings_initializer='he_uniform',\n",
    "#     optimizer='Adagrad',\n",
    "#     embeddings_regularizer='l2',\n",
    "    \n",
    ")\n",
    "\n",
    "dt = deeptable.DeepTable(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5654995d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-27 17:08:50 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['dnn_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['accuracy', 'AUC'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0, stacking_op='concat', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False), (128, 0.2, False), (128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 5, 'num_heads': 1, 'dropout_rate': 0.2, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 10}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': True, 'use_bias': False, 'direct': True, 'reduce_D': True}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-27 17:08:50 I deeptables.m.deeptable.py 339 - metrics:['accuracy', 'AUC']\n",
      "04-27 17:08:50 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-27 17:08:50 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-27 17:08:51 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.37151050567626953s\n",
      "04-27 17:08:51 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-27 17:08:51 I deeptables.m.preprocessor.py 383 - Imputation taken 0.4597799777984619s\n",
      "04-27 17:08:51 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-27 17:08:51 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.25382566452026367s\n",
      "04-27 17:08:52 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.3045427799224854s\n",
      "04-27 17:08:52 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-27 17:08:52 I deeptables.m.preprocessor.py 249 - transform_X taken 0.12765812873840332s\n",
      "04-27 17:08:52 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-27 17:08:52 I deeptables.m.preprocessor.py 236 - transform_y taken 0.001994609832763672s\n",
      "04-27 17:08:52 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-27 17:08:52 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_accuracy, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-27 17:08:52 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-27 17:08:52 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-27 17:08:52 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-27 17:08:53 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['dnn_nets']\n",
      "---------------------------------------------------------\n",
      "dnn: input_shape (None, 664), output_shape (None, 128)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-27 17:08:53 I deeptables.m.deepmodel.py 105 - training...\n",
      "WARNING:tensorflow:From F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 13s 38ms/step - loss: 2.0893 - accuracy: 0.3679 - auc: 0.6533 - val_loss: 1.3120 - val_accuracy: 0.3834 - val_auc: 0.6540\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 10s 37ms/step - loss: 1.6894 - accuracy: 0.5170 - auc: 0.7930 - val_loss: 1.0377 - val_accuracy: 0.5682 - val_auc: 0.8234\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 1.2854 - accuracy: 0.6407 - auc: 0.8790 - val_loss: 0.8759 - val_accuracy: 0.6082 - val_auc: 0.8609\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.9369 - accuracy: 0.7348 - auc: 0.9310 - val_loss: 0.8677 - val_accuracy: 0.6349 - val_auc: 0.8750\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.7128 - accuracy: 0.7962 - auc: 0.9580 - val_loss: 0.9142 - val_accuracy: 0.6423 - val_auc: 0.8789\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.5695 - accuracy: 0.8362 - auc: 0.9724 - val_loss: 0.9630 - val_accuracy: 0.6589 - val_auc: 0.8870\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.4721 - accuracy: 0.8630 - auc: 0.9805 - val_loss: 1.0509 - val_accuracy: 0.6857 - val_auc: 0.8901\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.4099 - accuracy: 0.8806 - auc: 0.9852 - val_loss: 1.1116 - val_accuracy: 0.6898 - val_auc: 0.8899\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.3606 - accuracy: 0.8934 - auc: 0.9882 - val_loss: 1.2135 - val_accuracy: 0.6945 - val_auc: 0.8900\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.3221 - accuracy: 0.9033 - auc: 0.9905 - val_loss: 1.2197 - val_accuracy: 0.6961 - val_auc: 0.8892\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.2994 - accuracy: 0.9097 - auc: 0.9918 - val_loss: 1.3843 - val_accuracy: 0.7048 - val_auc: 0.8891\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.2751 - accuracy: 0.9154 - auc: 0.9929 - val_loss: 1.3264 - val_accuracy: 0.7034 - val_auc: 0.8893\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.2627 - accuracy: 0.9186 - auc: 0.9936 - val_loss: 1.4335 - val_accuracy: 0.7072 - val_auc: 0.8858\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.2486 - accuracy: 0.9226 - auc: 0.9941 - val_loss: 1.5124 - val_accuracy: 0.7124 - val_auc: 0.8855\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.2354 - accuracy: 0.9251 - auc: 0.9947 - val_loss: 1.4428 - val_accuracy: 0.7096 - val_auc: 0.8865\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.2291 - accuracy: 0.9270 - auc: 0.9949 - val_loss: 1.4193 - val_accuracy: 0.6974 - val_auc: 0.8863\n",
      "Epoch 17/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/277 [==============================] - 10s 36ms/step - loss: 0.2199 - accuracy: 0.9291 - auc: 0.9953 - val_loss: 1.5100 - val_accuracy: 0.7122 - val_auc: 0.8854\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.2164 - accuracy: 0.9297 - auc: 0.9954 - val_loss: 1.5612 - val_accuracy: 0.7145 - val_auc: 0.8833\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.2058 - accuracy: 0.9318 - auc: 0.9958 - val_loss: 1.6140 - val_accuracy: 0.7122 - val_auc: 0.8832\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.2052 - accuracy: 0.9324 - auc: 0.9959 - val_loss: 1.6288 - val_accuracy: 0.7172 - val_auc: 0.8831\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.2005 - accuracy: 0.9339 - auc: 0.9960 - val_loss: 1.5857 - val_accuracy: 0.7111 - val_auc: 0.8823\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.1964 - accuracy: 0.9343 - auc: 0.9962 - val_loss: 1.5351 - val_accuracy: 0.7097 - val_auc: 0.8829\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.1926 - accuracy: 0.9352 - auc: 0.9963 - val_loss: 1.7091 - val_accuracy: 0.6986 - val_auc: 0.8788\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.1877 - accuracy: 0.9363 - auc: 0.9964 - val_loss: 1.6584 - val_accuracy: 0.7101 - val_auc: 0.8810\n",
      "Epoch 25/200\n",
      "277/277 [==============================] - 10s 36ms/step - loss: 0.1848 - accuracy: 0.9367 - auc: 0.9966 - val_loss: 1.6410 - val_accuracy: 0.7108 - val_auc: 0.8807\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "04-27 17:13:06 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-27 17:13:06 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-27 17:13:07 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230427170850_dnn_nets/dnn_nets.h5\n"
     ]
    }
   ],
   "source": [
    "model, history = dt.fit(x_train, y_train, epochs=epochs, validation_data=(x_val, y_val),class_weight=class_weight,\n",
    "                        batch_size=batch_size, max_queue_size=10, workers=4, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d81632b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-27 17:13:22 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-27 17:13:22 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13115549087524414s\n",
      "04-27 17:13:22 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-27 17:13:22 I deeptables.m.preprocessor.py 236 - transform_y taken 0.000997304916381836s\n",
      "04-27 17:13:22 I deeptables.m.deepmodel.py 158 - Performing evaluation...\n",
      "04-27 17:13:22 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=256, shuffle=False, drop_remainder=False\n",
      "{'loss': 1.6330766677856445, 'accuracy': 0.7170935869216919, 'auc': 0.8837042450904846}\n",
      "04-27 17:13:23 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "04-27 17:13:23 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-27 17:13:23 I deeptables.m.preprocessor.py 249 - transform_X taken 0.1301577091217041s\n",
      "04-27 17:13:23 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "04-27 17:13:23 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "04-27 17:13:23 I deeptables.m.deeptable.py 559 - predict_proba taken 0.6717195510864258s\n",
      "04-27 17:13:23 I deeptables.m.deeptable.py 594 - Reverse indicators to labels.\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "result = dt.evaluate(x_test,y_test)\n",
    "print(result)\n",
    "\n",
    "#scoring\n",
    "preds = dt.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dd67599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6330766677856445, 'accuracy': 0.7170935869216919, 'auc': 0.8837042450904846}\n",
      "============val_data===================\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "     Android_Adware     0.7820    0.7703    0.7761     14745\n",
      "Android_SMS_Malware     0.6765    0.7558    0.7139      6739\n",
      "  Android_Scareware     0.7175    0.6520    0.6832     11708\n",
      "             Benign     0.4939    0.5976    0.5408      2371\n",
      "\n",
      "           accuracy                         0.7171     35563\n",
      "          macro avg     0.6674    0.6939    0.6785     35563\n",
      "       weighted avg     0.7215    0.7171    0.7180     35563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result)\n",
    "print('============val_data===================')\n",
    "print(classification_report(y_test, preds, target_names=names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "53d39d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(\n",
    "    nets=['dnn_nets'],\n",
    "    dnn_params={\n",
    "#         'hidden_units': ((64, 0.2, True), (40, 0.2, True),(32, 0.2, True),(20, 0.2, True),(16, 0.2, True),(8, 0.2, True)),\n",
    "        'hidden_units':((300, 0.3, True),(300, 0.3, True)),\n",
    "        'activation': 'relu',\n",
    "    },\n",
    "    apply_gbm_features=True,\n",
    "    categorical_columns=categorical_columns,\n",
    "    auto_discrete=True,\n",
    "#     auto_categorize=True,\n",
    "    metrics=['AUC'],\n",
    "    earlystopping_patience=5,\n",
    "    embeddings_output_dim=100,\n",
    "    embedding_dropout=0.3,\n",
    "    optimizer='Adam',\n",
    "    stacking_op='add',\n",
    "    apply_class_weight=True,\n",
    "    embeddings_regularizer='l2',\n",
    "\n",
    ")\n",
    "\n",
    "dt = deeptable.DeepTable(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e29d6b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 18:48:18 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['dnn_nets', 'linear', 'autoint_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=True, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='Adam', loss='auto', dnn_params={'hidden_units': ((300, 0.3, True), (300, 0.3, True)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 18:48:18 I deeptables.m.deeptable.py 339 - metrics:['AUC']\n",
      "04-25 18:48:18 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-25 18:48:18 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 18:48:19 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.39694833755493164s\n",
      "04-25 18:48:19 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-25 18:48:19 I deeptables.m.preprocessor.py 383 - Imputation taken 0.483717679977417s\n",
      "04-25 18:48:19 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 18:48:20 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.3301537036895752s\n",
      "04-25 18:48:20 I deeptables.m.preprocessor.py 398 - Data discretization...\n",
      "04-25 18:48:20 I hypernets.t.sklearn_ex.py 716 - 64 variables to discrete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 18:48:22 I deeptables.m.preprocessor.py 404 - Discretization taken 2.2286579608917236s\n",
      "04-25 18:48:22 I deeptables.m.preprocessor.py 423 - Extracting GBM features...\n",
      "04-25 18:48:22 I hypernets.t.sklearn_ex.py 640 - LightGBM task:multiclass\n",
      "04-25 18:48:27 I deeptables.m.preprocessor.py 434 - Extracting gbm features taken 5.0382585525512695s\n",
      "04-25 18:48:32 I deeptables.m.preprocessor.py 196 - fit_transform taken 13.999603033065796s\n",
      "04-25 18:48:32 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 18:48:33 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5984175205230713s\n",
      "04-25 18:48:33 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 18:48:33 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0019943714141845703s\n",
      "04-25 18:48:34 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 18:48:34 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 18:48:34 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 18:48:34 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 18:48:34 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 18:48:36 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (174)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4, 24, 7, 7, 12, 14, 8, 6, 15, 16, 8, 7, 15, 16, 22, 24, 24, 21, 23, 20, 22, 22, 21, 22, 19, 19, 19, 19, 19, 13, 8, 9, 24, 22, 5, 8, 17, 17, 17, 3, 3, 3, 3, 3, 4, 16, 15, 15, 8, 7, 12, 7, 14, 10, 10, 6, 4, 15, 10, 15, 15, 15, 11, 15, 15, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 17464)\n",
      "---------------------------------------------------------\n",
      "nets: ['dnn_nets', 'linear', 'autoint_nets']\n",
      "---------------------------------------------------------\n",
      "dnn: input_shape (None, 17464), output_shape (None, 300)\n",
      "linear: input_shape (None, 238), output_shape (None, 1)\n",
      "autoint: input_shape (None, 174, 100), output_shape (None, 17400)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 18:48:36 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      " 20/277 [=>............................] - ETA: 54s - loss: 2.6261 - auc: 0.5265"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8564\\1561360229.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model, history = dt.fit(x_train, y_train, epochs=epochs, validation_data=(x_val, y_val),class_weight=class_weight,\n\u001b[1;32m----> 2\u001b[1;33m                         batch_size=batch_size,  max_queue_size=10, workers=4, use_multiprocessing=True,)\n\u001b[0m",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\deeptables\\models\\deeptable.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    366\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m                             \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m                             max_queue_size=max_queue_size, workers=workers, use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    369\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'{\"+\".join(self.nets)}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training finished.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\deeptables\\models\\deepmodel.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    118\u001b[0m                                  \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                                  \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                                  \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m                                  )\n\u001b[0;32m    122\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training finished.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3024\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1961\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, history = dt.fit(x_train, y_train, epochs=epochs, validation_data=(x_val, y_val),class_weight=class_weight,\n",
    "                        batch_size=batch_size,  max_queue_size=10, workers=4, use_multiprocessing=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60de621f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-20 12:09:49 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-20 12:09:49 I deeptables.m.preprocessor.py 249 - transform_X taken 0.4867076873779297s\n",
      "04-20 12:09:49 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-20 12:09:49 I deeptables.m.preprocessor.py 236 - transform_y taken 0.001995086669921875s\n",
      "04-20 12:09:50 I deeptables.m.deepmodel.py 158 - Performing evaluation...\n",
      "04-20 12:09:50 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=256, shuffle=False, drop_remainder=False\n",
      "{'loss': 3.120810031890869, 'auc': 0.821916401386261}\n",
      "04-20 12:09:52 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "04-20 12:09:52 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-20 12:09:53 I deeptables.m.preprocessor.py 249 - transform_X taken 0.46627116203308105s\n",
      "04-20 12:09:53 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "04-20 12:09:53 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "04-20 12:09:57 I deeptables.m.deeptable.py 559 - predict_proba taken 4.2547032833099365s\n",
      "04-20 12:09:57 I deeptables.m.deeptable.py 594 - Reverse indicators to labels.\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "result = dt.evaluate(x_test,y_test)\n",
    "print(result)\n",
    "\n",
    "#scoring\n",
    "preds = dt.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "989f892f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(6, 6))\n",
    "axes[0].plot(history.history['loss'], label='train_loss')\n",
    "axes[0].plot(history.history['val_loss'], label='val_loss')\n",
    "axes[0].set_title('Loss')\n",
    "# axes[1].plot(history.history['accuracy'], label='train_accuracy')\n",
    "# axes[1].plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "# axes[1].set_title('Accuracy')\n",
    "axes[2].plot(history.history['AUC'], label='train_AUC')\n",
    "axes[2].plot(history.history['val_AUC'], label='val_AUC')\n",
    "# axes[2].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[2].set_title('ROC')\n",
    "# axes[2].plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve')\n",
    "# axes[2].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# axes[2].set_title('ROC')\n",
    "\n",
    "# 添加整个图表的标题和横纵坐标标签\n",
    "fig.suptitle('Loss and Accuracy')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5f248e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.120810031890869, 'auc': 0.821916401386261}\n",
      "============val_data===================\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "     Android_Adware     0.8163    0.6109    0.6988     13905\n",
      "Android_SMS_Malware     0.7480    0.6152    0.6751      6141\n",
      "  Android_Scareware     0.5778    0.6333    0.6043     10823\n",
      "             Benign     0.2261    0.5930    0.3274      2187\n",
      "\n",
      "           accuracy                         0.6178     33056\n",
      "          macro avg     0.5920    0.6131    0.5764     33056\n",
      "       weighted avg     0.6865    0.6178    0.6389     33056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result)\n",
    "print('============val_data===================')\n",
    "print(classification_report(y_test, preds, target_names=names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7780dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(\n",
    "    nets=['autoint_nets'],\n",
    "    categorical_columns=categorical_columns,\n",
    "    auto_discrete=True,\n",
    "    auto_categorize=True,\n",
    "    metrics=['AUC'],\n",
    "    earlystopping_patience=5,\n",
    "    embeddings_output_dim=100,\n",
    "    embedding_dropout=0.3,\n",
    "    autoint_params={\n",
    "        'num_attention': 5,\n",
    "        'num_heads': 2,\n",
    "        'dropout_rate': 0.2,\n",
    "        'use_residual': True\n",
    "    },\n",
    "#     optimizer=keras.optimizers.RMSprop(),\n",
    "\n",
    ")\n",
    "dt = deeptable.DeepTable(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc520eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-20 12:09:57 I deeptables.m.deeptable.py 338 - X.Shape=(264440, 80), y.Shape=(264440,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['autoint_nets'], categorical_columns=['Source Port', 'Destination Port', 'Protocol', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC'], auto_categorize=True, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=True, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 5, 'num_heads': 2, 'dropout_rate': 0.2, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-20 12:09:57 I deeptables.m.deeptable.py 339 - metrics:['AUC']\n",
      "04-20 12:09:57 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-20 12:09:57 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-20 12:09:58 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.3261280059814453s\n",
      "04-20 12:09:58 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-20 12:09:58 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5774655342102051s\n",
      "04-20 12:09:58 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-20 12:09:58 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.17054390907287598s\n",
      "04-20 12:09:58 I deeptables.m.preprocessor.py 398 - Data discretization...\n",
      "04-20 12:09:58 I hypernets.t.sklearn_ex.py 716 - 66 variables to discrete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-20 12:10:00 I deeptables.m.preprocessor.py 404 - Discretization taken 1.9273722171783447s\n",
      "04-20 12:10:01 I deeptables.m.preprocessor.py 196 - fit_transform taken 3.718108654022217s\n",
      "04-20 12:10:01 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-20 12:10:01 I deeptables.m.preprocessor.py 249 - transform_X taken 0.3131742477416992s\n",
      "04-20 12:10:01 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-20 12:10:01 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009975433349609375s\n",
      "04-20 12:10:01 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-20 12:10:01 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-20 12:10:01 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-20 12:10:02 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-20 12:10:02 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-20 12:10:03 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (70)', 'input_continuous_all: (66)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [48234, 16794, 5, 4, 9, 10, 23, 5, 5, 11, 13, 8, 5, 14, 15, 8, 6, 14, 15, 21, 23, 23, 20, 23, 20, 22, 22, 20, 22, 19, 18, 19, 18, 18, 13, 7, 6, 23, 22, 5, 8, 16, 16, 16, 3, 3, 3, 3, 3, 4, 15, 13, 14, 7, 5, 11, 5, 13, 10, 9, 4, 4, 13, 7, 13, 13, 14, 8, 14, 14]\n",
      "output_dims: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 7066)\n",
      "---------------------------------------------------------\n",
      "nets: ['autoint_nets']\n",
      "---------------------------------------------------------\n",
      "autoint: input_shape (None, 70, 100), output_shape (None, 7000)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-20 12:10:03 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "258/258 [==============================] - 48s 143ms/step - loss: 2.4042 - auc: 0.5995 - val_loss: 1.4906 - val_auc: 0.3257\n",
      "Epoch 2/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 2.1042 - auc: 0.6784 - val_loss: 1.2191 - val_auc: 0.7264\n",
      "Epoch 3/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 1.8014 - auc: 0.7704 - val_loss: 1.2658 - val_auc: 0.7081\n",
      "Epoch 4/100\n",
      "258/258 [==============================] - 37s 142ms/step - loss: 1.5623 - auc: 0.8232 - val_loss: 1.2403 - val_auc: 0.7473\n",
      "Epoch 5/100\n",
      "258/258 [==============================] - 37s 143ms/step - loss: 1.4024 - auc: 0.8530 - val_loss: 1.3776 - val_auc: 0.7388\n",
      "Epoch 6/100\n",
      "258/258 [==============================] - 37s 142ms/step - loss: 1.2858 - auc: 0.8719 - val_loss: 1.4216 - val_auc: 0.7469\n",
      "Epoch 7/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 1.2074 - auc: 0.8844 - val_loss: 1.4714 - val_auc: 0.7569\n",
      "Epoch 8/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 1.1397 - auc: 0.8950 - val_loss: 1.5276 - val_auc: 0.7571\n",
      "Epoch 9/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 1.0827 - auc: 0.9040 - val_loss: 1.6214 - val_auc: 0.7627\n",
      "Epoch 10/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 1.0299 - auc: 0.9119 - val_loss: 1.6123 - val_auc: 0.7708\n",
      "Epoch 11/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.9926 - auc: 0.9176 - val_loss: 1.6826 - val_auc: 0.7693\n",
      "Epoch 12/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.9487 - auc: 0.9242 - val_loss: 1.7539 - val_auc: 0.7692\n",
      "Epoch 13/100\n",
      "258/258 [==============================] - 36s 141ms/step - loss: 0.9057 - auc: 0.9304 - val_loss: 1.8476 - val_auc: 0.7742\n",
      "Epoch 14/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.8770 - auc: 0.9348 - val_loss: 1.8581 - val_auc: 0.7748\n",
      "Epoch 15/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.8333 - auc: 0.9405 - val_loss: 1.9218 - val_auc: 0.7779\n",
      "Epoch 16/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.7984 - auc: 0.9453 - val_loss: 1.9628 - val_auc: 0.7808\n",
      "Epoch 17/100\n",
      "258/258 [==============================] - 36s 141ms/step - loss: 0.7653 - auc: 0.9494 - val_loss: 2.0004 - val_auc: 0.7808\n",
      "Epoch 18/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.7306 - auc: 0.9539 - val_loss: 1.9659 - val_auc: 0.7836\n",
      "Epoch 19/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.7045 - auc: 0.9572 - val_loss: 2.0670 - val_auc: 0.7832\n",
      "Epoch 20/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.6720 - auc: 0.9607 - val_loss: 2.1003 - val_auc: 0.7865\n",
      "Epoch 21/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.6414 - auc: 0.9639 - val_loss: 2.1790 - val_auc: 0.7866\n",
      "Epoch 22/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.6169 - auc: 0.9666 - val_loss: 2.1472 - val_auc: 0.7906\n",
      "Epoch 23/100\n",
      "258/258 [==============================] - 36s 141ms/step - loss: 0.5965 - auc: 0.9689 - val_loss: 2.1507 - val_auc: 0.7932\n",
      "Epoch 24/100\n",
      "258/258 [==============================] - 36s 141ms/step - loss: 0.5675 - auc: 0.9715 - val_loss: 2.2736 - val_auc: 0.7901\n",
      "Epoch 25/100\n",
      "258/258 [==============================] - 36s 141ms/step - loss: 0.5450 - auc: 0.9737 - val_loss: 2.2496 - val_auc: 0.7951\n",
      "Epoch 26/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.5236 - auc: 0.9754 - val_loss: 2.2833 - val_auc: 0.7962\n",
      "Epoch 27/100\n",
      "258/258 [==============================] - 36s 141ms/step - loss: 0.5074 - auc: 0.9771 - val_loss: 2.2627 - val_auc: 0.7961\n",
      "Epoch 28/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.4830 - auc: 0.9789 - val_loss: 2.3912 - val_auc: 0.7948\n",
      "Epoch 29/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.4669 - auc: 0.9803 - val_loss: 2.4310 - val_auc: 0.7981\n",
      "Epoch 30/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.4518 - auc: 0.9816 - val_loss: 2.4061 - val_auc: 0.7985\n",
      "Epoch 31/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.4365 - auc: 0.9828 - val_loss: 2.4875 - val_auc: 0.7976\n",
      "Epoch 32/100\n",
      "258/258 [==============================] - 36s 141ms/step - loss: 0.4192 - auc: 0.9840 - val_loss: 2.5489 - val_auc: 0.7973\n",
      "Epoch 33/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.4090 - auc: 0.9847 - val_loss: 2.5859 - val_auc: 0.7956\n",
      "Epoch 34/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.3963 - auc: 0.9857 - val_loss: 2.5826 - val_auc: 0.7963\n",
      "Epoch 35/100\n",
      "258/258 [==============================] - 36s 141ms/step - loss: 0.3799 - auc: 0.9867 - val_loss: 2.5904 - val_auc: 0.7990\n",
      "Epoch 36/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.3728 - auc: 0.9872 - val_loss: 2.7022 - val_auc: 0.7995\n",
      "Epoch 37/100\n",
      "258/258 [==============================] - 36s 141ms/step - loss: 0.3627 - auc: 0.9879 - val_loss: 2.6052 - val_auc: 0.8020\n",
      "Epoch 38/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.3529 - auc: 0.9885 - val_loss: 2.7014 - val_auc: 0.7993\n",
      "Epoch 39/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.3411 - auc: 0.9891 - val_loss: 2.6602 - val_auc: 0.8033\n",
      "Epoch 40/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.3428 - auc: 0.9891 - val_loss: 2.6838 - val_auc: 0.8029\n",
      "Epoch 41/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.3234 - auc: 0.9902 - val_loss: 2.8810 - val_auc: 0.8000\n",
      "Epoch 42/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.3161 - auc: 0.9906 - val_loss: 2.7416 - val_auc: 0.8033\n",
      "Epoch 43/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.3104 - auc: 0.9910 - val_loss: 2.7912 - val_auc: 0.8039\n",
      "Epoch 44/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.3031 - auc: 0.9914 - val_loss: 2.7907 - val_auc: 0.8013\n",
      "Epoch 45/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.2926 - auc: 0.9919 - val_loss: 2.8148 - val_auc: 0.8008\n",
      "Epoch 46/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.2888 - auc: 0.9922 - val_loss: 2.7790 - val_auc: 0.8006\n",
      "Epoch 47/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.3004 - auc: 0.9917 - val_loss: 2.8320 - val_auc: 0.8008\n",
      "Epoch 48/100\n",
      "258/258 [==============================] - 36s 140ms/step - loss: 0.2791 - auc: 0.9927 - val_loss: 2.8019 - val_auc: 0.8035\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00048: early stopping\n",
      "04-20 12:39:15 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-20 12:39:15 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-20 12:39:16 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20230420120957_autoint_nets/autoint_nets.h5\n"
     ]
    }
   ],
   "source": [
    "model, history = dt.fit(x_train, y_train, epochs=epochs, validation_data=(x_val, y_val),class_weight=class_weight,\n",
    "                        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3fa1d75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-20 12:39:16 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-20 12:39:16 I deeptables.m.preprocessor.py 249 - transform_X taken 0.3261270523071289s\n",
      "04-20 12:39:16 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-20 12:39:16 I deeptables.m.preprocessor.py 236 - transform_y taken 0.000997304916381836s\n",
      "04-20 12:39:16 I deeptables.m.deepmodel.py 158 - Performing evaluation...\n",
      "04-20 12:39:16 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=256, shuffle=False, drop_remainder=False\n",
      "{'loss': 2.8248157501220703, 'auc': 0.8027924299240112}\n",
      "04-20 12:39:19 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "04-20 12:39:19 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-20 12:39:20 I deeptables.m.preprocessor.py 249 - transform_X taken 0.3171517848968506s\n",
      "04-20 12:39:20 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "04-20 12:39:20 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "04-20 12:39:24 I deeptables.m.deeptable.py 559 - predict_proba taken 4.087628602981567s\n",
      "04-20 12:39:24 I deeptables.m.deeptable.py 594 - Reverse indicators to labels.\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "result = dt.evaluate(x_test,y_test)\n",
    "print(result)\n",
    "\n",
    "#scoring\n",
    "preds = dt.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d81507f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(6, 6))\n",
    "axes[0].plot(history.history['loss'], label='train_loss')\n",
    "axes[0].plot(history.history['val_loss'], label='val_loss')\n",
    "axes[0].set_title('Loss')\n",
    "# axes[1].plot(history.history['accuracy'], label='train_accuracy')\n",
    "# axes[1].plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[2].plot(history.history['AUC'], label='train_AUC')\n",
    "axes[2].plot(history.history['val_AUC'], label='val_AUC')\n",
    "axes[2].set_title('AUC')\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=0.5)\n",
    "fig.suptitle('Loss and Accuracy')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d75768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8248157501220703, 'auc': 0.8027924299240112}\n",
      "============val_data===================\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "     Android_Adware     0.6982    0.6731    0.6854     13905\n",
      "Android_SMS_Malware     0.5544    0.5890    0.5712      6141\n",
      "  Android_Scareware     0.5823    0.5304    0.5552     10823\n",
      "             Benign     0.3078    0.4600    0.3688      2187\n",
      "\n",
      "           accuracy                         0.5967     33056\n",
      "          macro avg     0.5357    0.5631    0.5451     33056\n",
      "       weighted avg     0.6077    0.5967    0.6006     33056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result)\n",
    "print('============val_data===================')\n",
    "print(classification_report(y_test, preds, target_names=names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c599e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(\n",
    "    metrics=['AUC'],\n",
    "#     auto_discrete=True,\n",
    "    auto_categorize=True,\n",
    "#     auto_imputation=False,\n",
    "    embeddings_output_dim=100, \n",
    "    dense_dropout=0.3,\n",
    "#     embeddings_initializer='he_normal',\n",
    "    embeddings_regularizer='l2',\n",
    "#     categorical_columns=categorical_columns,\n",
    "    optimizer='Adagrad',\n",
    "    stacking_op='concat',\n",
    "    apply_gbm_features=True,\n",
    "    nets =['fm_nets'],\n",
    "    earlystopping_patience=5,\n",
    ")\n",
    "dt = deeptable.DeepTable(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e09da57c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-23 01:44:32 I deeptables.m.deeptable.py 338 - X.Shape=(263373, 76), y.Shape=(263373,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fm_nets'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['AUC'], auto_categorize=True, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer='l2', embeddings_activity_regularizer=None, dense_dropout=0.3, embedding_dropout=0.3, stacking_op='concat', output_use_bias=True, apply_class_weight=False, optimizer='Adagrad', loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-23 01:44:32 I deeptables.m.deeptable.py 339 - metrics:['AUC']\n",
      "04-23 01:44:32 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-23 01:44:32 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-23 01:44:36 I deeptables.m.preprocessor.py 336 - Preparing features taken 4.259350061416626s\n",
      "04-23 01:44:36 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-23 01:44:39 I deeptables.m.preprocessor.py 383 - Imputation taken 2.127898693084717s\n",
      "04-23 01:44:39 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-23 01:44:43 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 4.028414726257324s\n",
      "04-23 01:44:43 I deeptables.m.preprocessor.py 423 - Extracting GBM features...\n",
      "04-23 01:44:47 I hypernets.t.sklearn_ex.py 640 - LightGBM task:multiclass\n",
      "04-23 01:44:51 I deeptables.m.preprocessor.py 434 - Extracting gbm features taken 8.592936992645264s\n",
      "04-23 01:44:54 I deeptables.m.preprocessor.py 196 - fit_transform taken 21.65766167640686s\n",
      "04-23 01:44:54 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-23 01:44:56 I deeptables.m.preprocessor.py 249 - transform_X taken 1.905984878540039s\n",
      "04-23 01:44:56 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-23 01:44:56 I deeptables.m.preprocessor.py 236 - transform_y taken 0.001994609832763672s\n",
      "04-23 01:44:56 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-23 01:44:56 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-23 01:44:56 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-23 01:44:57 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-23 01:44:57 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-23 01:44:57 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (124)', 'input_continuous_all: (66)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [5, 76, 100, 150, 387, 4, 419, 437, 72, 4, 4, 4, 4, 4, 15, 419, 76, 100, 33, 19, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.3\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 12466)\n",
      "---------------------------------------------------------\n",
      "nets: ['fm_nets']\n",
      "---------------------------------------------------------\n",
      "fm: input_shape (None, 124, 100), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adagrad\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-23 01:44:57 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "257/257 [==============================] - 33s 81ms/step - loss: 7.0247 - auc: 0.5317 - val_loss: 5.8198 - val_auc: 0.5806\n",
      "Epoch 2/100\n",
      "257/257 [==============================] - 18s 71ms/step - loss: 6.7809 - auc: 0.5622 - val_loss: 5.6504 - val_auc: 0.5984\n",
      "Epoch 3/100\n",
      "257/257 [==============================] - 18s 71ms/step - loss: 6.6002 - auc: 0.5729 - val_loss: 5.5017 - val_auc: 0.6031\n",
      "Epoch 4/100\n",
      "257/257 [==============================] - 18s 72ms/step - loss: 6.4396 - auc: 0.5784 - val_loss: 5.3594 - val_auc: 0.6077\n",
      "Epoch 5/100\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 6.2880 - auc: 0.5837 - val_loss: 5.2272 - val_auc: 0.6093\n",
      "Epoch 6/100\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 6.1475 - auc: 0.5863 - val_loss: 5.0984 - val_auc: 0.6127\n",
      "Epoch 7/100\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 6.0132 - auc: 0.5901 - val_loss: 4.9766 - val_auc: 0.6132\n",
      "Epoch 8/100\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 5.8842 - auc: 0.5923 - val_loss: 4.8582 - val_auc: 0.6149\n",
      "Epoch 9/100\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 5.7630 - auc: 0.5950 - val_loss: 4.7421 - val_auc: 0.6185\n",
      "Epoch 10/100\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 5.6474 - auc: 0.5958 - val_loss: 4.6379 - val_auc: 0.6154\n",
      "Epoch 11/100\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 5.5332 - auc: 0.5972 - val_loss: 4.5308 - val_auc: 0.6174\n",
      "Epoch 12/100\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 5.4255 - auc: 0.5998 - val_loss: 4.4306 - val_auc: 0.6174\n",
      "Epoch 13/100\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 5.3235 - auc: 0.6004 - val_loss: 4.3315 - val_auc: 0.6188\n",
      "Epoch 14/100\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 5.2238 - auc: 0.6017 - val_loss: 4.2373 - val_auc: 0.6205\n",
      "Epoch 15/100\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 5.1271 - auc: 0.6031 - val_loss: 4.1464 - val_auc: 0.6209\n",
      "Epoch 16/100\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 5.0370 - auc: 0.6038 - val_loss: 4.0609 - val_auc: 0.6198\n",
      "Epoch 17/100\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 4.9469 - auc: 0.6048 - val_loss: 3.9757 - val_auc: 0.6203\n",
      "Epoch 18/100\n",
      "257/257 [==============================] - 19s 72ms/step - loss: 4.8617 - auc: 0.6058 - val_loss: 3.8931 - val_auc: 0.6208\n",
      "Epoch 19/100\n",
      "257/257 [==============================] - 19s 72ms/step - loss: 4.7772 - auc: 0.6074 - val_loss: 3.8119 - val_auc: 0.6226\n",
      "Epoch 20/100\n",
      "257/257 [==============================] - 19s 72ms/step - loss: 4.6994 - auc: 0.6069 - val_loss: 3.7364 - val_auc: 0.6220\n",
      "Epoch 21/100\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 4.6221 - auc: 0.6073 - val_loss: 3.6616 - val_auc: 0.6233\n",
      "Epoch 22/100\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 4.5473 - auc: 0.6087 - val_loss: 3.5919 - val_auc: 0.6218\n",
      "Epoch 23/100\n",
      "257/257 [==============================] - 19s 72ms/step - loss: 4.4750 - auc: 0.6089 - val_loss: 3.5230 - val_auc: 0.6213\n",
      "Epoch 24/100\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 4.4039 - auc: 0.6113 - val_loss: 3.4566 - val_auc: 0.6214\n",
      "Epoch 25/100\n",
      "257/257 [==============================] - 19s 72ms/step - loss: 4.3385 - auc: 0.6105 - val_loss: 3.3914 - val_auc: 0.6213\n",
      "Epoch 26/100\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 4.2741 - auc: 0.6105 - val_loss: 3.3284 - val_auc: 0.6220\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00026: early stopping\n",
      "04-23 01:53:19 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-23 01:53:19 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-23 01:53:19 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20230423014431_fm_nets/fm_nets.h5\n"
     ]
    }
   ],
   "source": [
    "model, history = dt.fit(x_train, y_train, epochs=epochs, validation_data=(x_val, y_val),class_weight=class_weight,\n",
    "                        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cd381221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-23 01:53:22 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-23 01:53:24 I deeptables.m.preprocessor.py 249 - transform_X taken 1.8301548957824707s\n",
      "04-23 01:53:24 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-23 01:53:24 I deeptables.m.preprocessor.py 236 - transform_y taken 0.002991914749145508s\n",
      "04-23 01:53:25 I deeptables.m.deepmodel.py 158 - Performing evaluation...\n",
      "04-23 01:53:25 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=256, shuffle=False, drop_remainder=False\n",
      "{'loss': 3.6614394187927246, 'auc': 0.6239424347877502}\n",
      "04-23 01:53:29 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "04-23 01:53:29 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-23 01:53:31 I deeptables.m.preprocessor.py 249 - transform_X taken 1.8795161247253418s\n",
      "04-23 01:53:31 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "04-23 01:53:31 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "04-23 01:53:33 I deeptables.m.deeptable.py 559 - predict_proba taken 4.328027248382568s\n",
      "04-23 01:53:33 I deeptables.m.deeptable.py 594 - Reverse indicators to labels.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6614394187927246, 'auc': 0.6239424347877502}\n",
      "============val_data===================\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "     Android_Adware     0.5739    0.4597    0.5105     13842\n",
      "Android_SMS_Malware     0.3280    0.5382    0.4076      6109\n",
      "  Android_Scareware     0.3702    0.2341    0.2868     10790\n",
      "             Benign     0.0804    0.1839    0.1119      2181\n",
      "\n",
      "           accuracy                         0.3821     32922\n",
      "          macro avg     0.3381    0.3540    0.3292     32922\n",
      "       weighted avg     0.4288    0.3821    0.3917     32922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = dt.evaluate(x_test,y_test)\n",
    "print(result)\n",
    "\n",
    "#scoring\n",
    "preds = dt.predict(x_test)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(6, 6))\n",
    "axes[0].plot(history.history['loss'], label='train_loss')\n",
    "axes[0].plot(history.history['val_loss'], label='val_loss')\n",
    "axes[0].set_title('Loss')\n",
    "# axes[1].plot(history.history['accuracy'], label='train_accuracy')\n",
    "# axes[1].plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "# axes[1].set_title('Accuracy')\n",
    "axes[2].plot(history.history['AUC'], label='train_AUC')\n",
    "axes[2].plot(history.history['val_AUC'], label='val_AUC')\n",
    "axes[2].set_title('AUC')\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=0.5)\n",
    "fig.suptitle('Loss and Accuracy')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()\n",
    "\n",
    "print(result)\n",
    "print('============val_data===================')\n",
    "print(classification_report(y_test, preds, target_names=names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "01c24557",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(\n",
    "    metrics=['AUC'],\n",
    "    auto_discrete=False,\n",
    "    auto_categorize=False,\n",
    "    embeddings_output_dim=100, \n",
    "    nets =['dcn_nets'],\n",
    "    earlystopping_patience=5,\n",
    "    apply_gbm_features=True,\n",
    "    dnn_params={\n",
    "        'hidden_units': ((64, 0.2, True), (64, 0.2, True),(64, 0.2, True),(64, 0.2, True),\n",
    "                         (64, 0.2, True),(64, 0.2, True),(64, 0.2, True),(64, 0.2, True),\n",
    "                         (64, 0.2, True),(64, 0.2, True)),\n",
    "#         'hidden_units':((300, 0.3, True),(300, 0.3, True)),\n",
    "        'use_bn': True,\n",
    "        'learning_rate': 0.01,\n",
    "        'activation': 'relu',\n",
    "    },\n",
    "#     pnn_params={\n",
    "#         'outer_product_kernel_type': 'vec',\n",
    "#     },\n",
    "#     optimizer='Adamax'\n",
    ")\n",
    "dt = deeptable.DeepTable(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34380675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-20 12:49:07 I deeptables.m.deeptable.py 338 - X.Shape=(264440, 80), y.Shape=(264440,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['dcn_nets'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['AUC'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((64, 0.2, True), (64, 0.2, True), (64, 0.2, True), (64, 0.2, True), (64, 0.2, True), (64, 0.2, True), (64, 0.2, True), (64, 0.2, True), (64, 0.2, True), (64, 0.2, True)), 'use_bn': True, 'learning_rate': 0.01, 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-20 12:49:07 I deeptables.m.deeptable.py 339 - metrics:['AUC']\n",
      "04-20 12:49:07 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-20 12:49:07 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-20 12:49:08 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.3341066837310791s\n",
      "04-20 12:49:08 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-20 12:49:08 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5839438438415527s\n",
      "04-20 12:49:08 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-20 12:49:08 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.0s\n",
      "04-20 12:49:08 I deeptables.m.preprocessor.py 423 - Extracting GBM features...\n",
      "04-20 12:49:08 I hypernets.t.sklearn_ex.py 640 - LightGBM task:multiclass\n",
      "04-20 12:49:11 I deeptables.m.preprocessor.py 434 - Extracting gbm features taken 2.451469659805298s\n",
      "04-20 12:49:13 I deeptables.m.preprocessor.py 196 - fit_transform taken 5.449517250061035s\n",
      "04-20 12:49:13 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-20 12:49:13 I deeptables.m.preprocessor.py 249 - transform_X taken 0.17852258682250977s\n",
      "04-20 12:49:13 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-20 12:49:13 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0019941329956054688s\n",
      "04-20 12:49:13 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-20 12:49:13 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-20 12:49:13 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-20 12:49:14 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-20 12:49:14 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-20 12:49:15 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (104)', 'input_continuous_all: (70)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 10470)\n",
      "---------------------------------------------------------\n",
      "nets: ['dcn_nets']\n",
      "---------------------------------------------------------\n",
      "dcn-widecross: input_shape (None, 10470), output_shape (None, 10470)\n",
      "dcn-dnn2: input_shape (None, 10470), output_shape (None, 64)\n",
      "dcn: input_shape (None, 10470), output_shape (None, 10534)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-20 12:49:15 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "258/258 [==============================] - 33s 75ms/step - loss: 19.5004 - auc: 0.5846 - val_loss: 1.3525 - val_auc: 0.5007\n",
      "Epoch 2/100\n",
      "258/258 [==============================] - 18s 70ms/step - loss: 2831.0405 - auc: 0.6030 - val_loss: 47946.8867 - val_auc: 0.5692\n",
      "Epoch 3/100\n",
      "258/258 [==============================] - 18s 70ms/step - loss: 3822.3992 - auc: 0.5554 - val_loss: 416.8653 - val_auc: 0.5623\n",
      "Epoch 4/100\n",
      "258/258 [==============================] - 18s 70ms/step - loss: 275.7296 - auc: 0.5791 - val_loss: 30.1913 - val_auc: 0.6142\n",
      "Epoch 5/100\n",
      "258/258 [==============================] - 18s 70ms/step - loss: 87.0757 - auc: 0.5918 - val_loss: 16.1984 - val_auc: 0.6265\n",
      "Epoch 6/100\n",
      "258/258 [==============================] - 18s 70ms/step - loss: 75.6235 - auc: 0.5954 - val_loss: 12.7482 - val_auc: 0.6215\n",
      "Epoch 7/100\n",
      "258/258 [==============================] - 18s 70ms/step - loss: 56.6793 - auc: 0.5982 - val_loss: 10.9156 - val_auc: 0.6323\n",
      "Epoch 8/100\n",
      "258/258 [==============================] - 18s 70ms/step - loss: 42.4698 - auc: 0.6042 - val_loss: 6.9339 - val_auc: 0.6427\n",
      "Epoch 9/100\n",
      "258/258 [==============================] - 18s 71ms/step - loss: 29.9363 - auc: 0.6082 - val_loss: 5.7380 - val_auc: 0.6663\n",
      "Epoch 10/100\n",
      "258/258 [==============================] - 18s 70ms/step - loss: 30.0415 - auc: 0.6118 - val_loss: 6.5807 - val_auc: 0.6739\n",
      "Epoch 11/100\n",
      "258/258 [==============================] - 18s 70ms/step - loss: 25.8646 - auc: 0.6152 - val_loss: 4.8919 - val_auc: 0.7022\n",
      "Epoch 12/100\n",
      "258/258 [==============================] - 18s 70ms/step - loss: 19.6651 - auc: 0.6184 - val_loss: 3.7694 - val_auc: 0.6455\n",
      "Epoch 13/100\n",
      "258/258 [==============================] - 18s 71ms/step - loss: 17.5827 - auc: 0.6205 - val_loss: 3.2853 - val_auc: 0.6684\n",
      "Epoch 14/100\n",
      "258/258 [==============================] - 18s 71ms/step - loss: 15.6491 - auc: 0.6237 - val_loss: 2.7331 - val_auc: 0.6418\n",
      "Epoch 15/100\n",
      "258/258 [==============================] - 18s 70ms/step - loss: 16.1363 - auc: 0.6257 - val_loss: 3.6905 - val_auc: 0.6574\n",
      "Epoch 16/100\n",
      "258/258 [==============================] - 18s 70ms/step - loss: 20.6339 - auc: 0.6253 - val_loss: 12.0272 - val_auc: 0.6226\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "04-20 12:54:21 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-20 12:54:21 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-20 12:54:21 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20230420124907_dcn_nets/dcn_nets.h5\n"
     ]
    }
   ],
   "source": [
    "model, history = dt.fit(x_train, y_train, epochs=epochs, validation_data=(x_val, y_val),class_weight=class_weight,\n",
    "                        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "99d1beb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-22 23:49:07 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-22 23:49:09 I deeptables.m.preprocessor.py 249 - transform_X taken 1.8431055545806885s\n",
      "04-22 23:49:09 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-22 23:49:09 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0029916763305664062s\n",
      "04-22 23:49:09 I deeptables.m.deepmodel.py 158 - Performing evaluation...\n",
      "04-22 23:49:09 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=256, shuffle=False, drop_remainder=False\n",
      "{'loss': 1.2928005456924438, 'auc': 0.6218370795249939}\n",
      "04-22 23:49:11 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "04-22 23:49:11 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-22 23:49:13 I deeptables.m.preprocessor.py 249 - transform_X taken 1.8246698379516602s\n",
      "04-22 23:49:13 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "04-22 23:49:13 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "04-22 23:49:15 I deeptables.m.deeptable.py 559 - predict_proba taken 4.260739326477051s\n",
      "04-22 23:49:15 I deeptables.m.deeptable.py 594 - Reverse indicators to labels.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAJJCAYAAACJTmwAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAACJPUlEQVR4nO3deVxUVeMG8GdmYIZ93wUFd03FXEDNNUm0MjXLNRfSelO0jBazRbTlpf3nW1lWZlauWW6ZmYZpmbhr7juKCyCgMCzCwMz9/XFgYAQUcJi5wPP9fOYzd+6ce+fMhWEezj33HIUkSRKIiIiIZExp7QoQERER3QkDCxEREckeAwsRERHJHgMLERERyR4DCxEREckeAwsRERHJHgMLERERyR4DCxEREckeAwsRERHJHgMLEVmNQqHAnDlzrF0NIqoDGFiILGTx4sVQKBTYt2+ftatSZ504cQIKhQJ2dnbIzMy0dnWIyIIYWIiozliyZAn8/PwAAD/99JOVa0NElsTAQkR1giRJWLZsGcaMGYMHH3wQS5cutXaVKpWbm2vtKhDVOwwsRDJz8OBBDBo0CC4uLnByckL//v2xa9cukzKFhYWYO3cuWrRoATs7O3h6eqJnz57YsmWLsUxKSgqioqIQGBgIjUYDf39/DBkyBBcuXLjt6x8+fBgTJ05E06ZNYWdnBz8/Pzz55JPIyMgwKTdnzhwoFAqcPXsWEydOhJubG1xdXREVFYW8vDyTsgUFBXj++efh7e0NZ2dnPPLII7h8+XK1jss///yDCxcuYNSoURg1ahT++uuvCvdhMBjwv//9D+3bt4ednR28vb0xcODAcqfilixZgrCwMDg4OMDd3R29e/fG5s2bjc9X1r8mODgYEydOND4uOdW3fft2TJ06FT4+PggMDAQAXLx4EVOnTkWrVq1gb28PT09PPP744xX+DDIzM/H8888jODgYGo0GgYGBGD9+PNLT05GTkwNHR0c899xz5ba7fPkyVCoV4uLiqngkieomG2tXgIhKHTt2DL169YKLiwtefvll2Nra4ssvv0Tfvn2xfft2hIeHAxBhIS4uDpMnT0ZYWBi0Wi327duHAwcO4IEHHgAADB8+HMeOHcP06dMRHByMa9euYcuWLUhKSkJwcHClddiyZQvOnz+PqKgo+Pn54dixY/jqq69w7Ngx7Nq1CwqFwqT8iBEjEBISgri4OBw4cAALFy6Ej48P3nvvPWOZyZMnY8mSJRgzZgx69OiBrVu34qGHHqrWsVm6dCmaNWuGrl27ol27dnBwcMDy5cvx0ksvmZSbNGkSFi9ejEGDBmHy5MkoKirC33//jV27dqFLly4AgLlz52LOnDno0aMH3nzzTajVauzevRtbt27FgAEDqlWvElOnToW3tzdmz55tbGHZu3cvdu7ciVGjRiEwMBAXLlzAF198gb59++L48eNwcHAAAOTk5KBXr144ceIEnnzySXTq1Anp6elYv349Ll++jI4dO2LYsGFYuXIlPv74Y6hUKuPrLl++HJIkYezYsTWqN1GdIRGRRXz77bcSAGnv3r2Vlhk6dKikVqulc+fOGdddvXpVcnZ2lnr37m1cFxoaKj300EOV7ufGjRsSAOmDDz6odj3z8vLKrVu+fLkEQPrrr7+M62JjYyUA0pNPPmlSdtiwYZKnp6fx8aFDhyQA0tSpU03KjRkzRgIgxcbG3rFOOp1O8vT0lF577TWT7UNDQ03Kbd26VQIgPfvss+X2YTAYJEmSpDNnzkhKpVIaNmyYpNfrKywjSVKldWvSpIk0YcIE4+OSn2vPnj2loqIik7IVHcuEhAQJgPT9998b182ePVsCIK1evbrSev/+++8SAOm3334zeb5Dhw5Snz59ym1HVN/wlBCRTOj1emzevBlDhw5F06ZNjev9/f0xZswY7NixA1qtFgDg5uaGY8eO4cyZMxXuy97eHmq1Gtu2bcONGzeqVQ97e3vjcn5+PtLT09GtWzcAwIEDB8qVf+aZZ0we9+rVCxkZGca6bty4EQDw7LPPmpSbMWNGlev022+/ISMjA6NHjzauGz16NP79918cO3bMuO7nn3+GQqFAbGxsuX2UtAytXbsWBoMBs2fPhlKprLBMTTz11FMmLR+A6bEsLCxERkYGmjdvDjc3N5Nj+fPPPyM0NBTDhg2rtN4REREICAgw6btz9OhRHD58GE888USN601UVzCwEMlEWloa8vLy0KpVq3LPtWnTBgaDAZcuXQIAvPnmm8jMzETLli3Rvn17vPTSSzh8+LCxvEajwXvvvYfffvsNvr6+6N27N95//32kpKTcsR7Xr1/Hc889B19fX9jb28Pb2xshISEAgKysrHLlGzdubPLY3d0dAIxB6eLFi1AqlWjWrJlJuYreZ2WWLFmCkJAQaDQanD17FmfPnkWzZs3g4OBg8gV+7tw5BAQEwMPDo9J9nTt3DkqlEm3btq3y61dFyTEq6+bNm5g9ezaCgoKg0Wjg5eUFb29vZGZmmhzLc+fOoV27drfdv1KpxNixY7F27VpjH6GlS5fCzs4Ojz/+uFnfC5EcMbAQ1UG9e/fGuXPnsGjRIrRr1w4LFy5Ep06dsHDhQmOZGTNm4PTp04iLi4OdnR3eeOMNtGnTBgcPHrztvkeMGIGvv/4azzzzDFavXo3Nmzdj06ZNAESH1lvd2qpQQpKku3iHpbRaLX755RckJiaiRYsWxlvbtm2Rl5eHZcuWme21qkKv11e4vmxrSonp06fjnXfewYgRI/Djjz9i8+bN2LJlCzw9PSs8lncyfvx45OTkYO3atcarph5++GG4urpWe19EdQ073RLJhLe3NxwcHHDq1Klyz508eRJKpRJBQUHGdR4eHoiKikJUVBRycnLQu3dvzJkzB5MnTzaWadasGV544QW88MILOHPmDDp27IiPPvoIS5YsqbAON27cQHx8PObOnYvZs2cb11d26qkqmjRpAoPBgHPnzpm0qlT0PiuyevVq5Ofn44svvoCXl5fJc6dOncLrr7+Of/75Bz179kSzZs3w+++/4/r165W2sjRr1gwGgwHHjx9Hx44dK31dd3f3coPT6XQ6JCcnV6negBgrZsKECfjoo4+M6/Lz88vtt1mzZjh69Ogd99euXTvce++9WLp0KQIDA5GUlIRPP/20yvUhqsvYwkIkEyqVCgMGDMC6detMLntNTU3FsmXL0LNnT7i4uABAuUuMnZyc0Lx5cxQUFAAA8vLykJ+fb1KmWbNmcHZ2NpaprA5A+daRefPm1fRtYdCgQQCATz75pEb7XLJkCZo2bYpnnnkGjz32mMntxRdfhJOTk/G00PDhwyFJEubOnVtuPyXvaejQoVAqlXjzzTfLtXKUfd/NmjXDX3/9ZfL8V199VWkLS0VUKlW5Y/npp5+W28fw4cPx77//Ys2aNZXWu8S4ceOwefNmzJs3D56ensbjS1TfsYWFyMIWLVpkPMVS1nPPPYe3334bW7ZsQc+ePTF16lTY2Njgyy+/REFBAd5//31j2bZt26Jv377o3LkzPDw8sG/fPvz000+YNm0aAOD06dPo378/RowYgbZt28LGxgZr1qxBamoqRo0aVWndXFxcjP1dCgsL0ahRI2zevBmJiYk1fr8dO3bE6NGj8fnnnyMrKws9evRAfHw8zp49e8dtr169ij///LNch90SGo0GkZGRWLVqFT755BP069cP48aNwyeffIIzZ85g4MCBMBgM+Pvvv9GvXz9MmzYNzZs3x2uvvYa33noLvXr1wqOPPgqNRoO9e/ciICDAOJ7J5MmT8cwzz2D48OF44IEH8O+//+L3338v18pzOw8//DB++OEHuLq6om3btkhISMAff/wBT09Pk3IvvfQSfvrpJzz++ON48skn0blzZ1y/fh3r16/HggULEBoaaiw7ZswYvPzyy1izZg2mTJkCW1vbKteHqE6z2vVJRA1MyeWvld0uXbokSZIkHThwQIqMjJScnJwkBwcHqV+/ftLOnTtN9vX2229LYWFhkpubm2Rvby+1bt1aeueddySdTidJkiSlp6dL0dHRUuvWrSVHR0fJ1dVVCg8Pl3788cc71vPy5cvSsGHDJDc3N8nV1VV6/PHHpatXr5a7zLfksua0tLQK32diYqJx3c2bN6Vnn31W8vT0lBwdHaXBgwdLly5duuNlzR999JEEQIqPj6+0zOLFiyUA0rp16yRJkqSioiLpgw8+kFq3bi2p1WrJ29tbGjRokLR//36T7RYtWiTde++9kkajkdzd3aU+ffpIW7ZsMT6v1+ulmTNnSl5eXpKDg4MUGRkpnT17ttLLmiu6XP3GjRtSVFSU5OXlJTk5OUmRkZHSyZMny+1DkiQpIyNDmjZtmtSoUSNJrVZLgYGB0oQJE6T09PRy+33wwQclAOV+L4jqM4UkWbC3GhER3bVhw4bhyJEjVWqlIqov2IeFiKgOSU5Oxq+//opx48ZZuypEFsU+LEREdUBiYiL++ecfLFy4ELa2tvjPf/5j7SoRWRRbWIiI6oDt27dj3LhxSExMxHfffQc/Pz9rV4nIotiHhYiIiGSPLSxEREQkewwsREREJHsMLERERCR7DCxEREQkewwsREREJHsMLERERCR7DCxEREQkewwsREREJHsMLERERCR7DCxEREQkewwsREREJHsMLERERCR7DCxEREQkewwsREREJHsMLERERCR7DCxEREQkewwsREREJHsMLERERCR7DCxEREQkewwsREREJHsMLERkcYsXL4ZCocC+ffusXRUiqiMYWIiIiEj2GFiIiIhI9hhYiEiWDh48iEGDBsHFxQVOTk7o378/du3aZVKmsLAQc+fORYsWLWBnZwdPT0/07NkTW7ZsMZZJSUlBVFQUAgMDodFo4O/vjyFDhuDChQsWfkdEdDdsrF0BIqJbHTt2DL169YKLiwtefvll2Nra4ssvv0Tfvn2xfft2hIeHAwDmzJmDuLg4TJ48GWFhYdBqtdi3bx8OHDiABx54AAAwfPhwHDt2DNOnT0dwcDCuXbuGLVu2ICkpCcHBwVZ8l0RUHQpJkiRrV4KIGpbFixcjKioKe/fuRZcuXco9P2zYMGzcuBEnTpxA06ZNAQDJyclo1aoV7r33Xmzfvh0A0LFjRwQGBmLDhg0Vvk5mZibc3d3xwQcf4MUXX6y9N0REtY6nhIhIVvR6PTZv3oyhQ4cawwoA+Pv7Y8yYMdixYwe0Wi0AwM3NDceOHcOZM2cq3Je9vT3UajW2bduGGzduWKT+RFQ7GFiISFbS0tKQl5eHVq1alXuuTZs2MBgMuHTpEgDgzTffRGZmJlq2bIn27dvjpZdewuHDh43lNRoN3nvvPfz222/w9fVF79698f777yMlJcVi74eIzIOBhYjqrN69e+PcuXNYtGgR2rVrh4ULF6JTp05YuHChscyMGTNw+vRpxMXFwc7ODm+88QbatGmDgwcPWrHmRFRdDCxEJCve3t5wcHDAqVOnyj138uRJKJVKBAUFGdd5eHggKioKy5cvx6VLl9ChQwfMmTPHZLtmzZrhhRdewObNm3H06FHodDp89NFHtf1WiMiMGFiISFZUKhUGDBiAdevWmVx6nJqaimXLlqFnz55wcXEBAGRkZJhs6+TkhObNm6OgoAAAkJeXh/z8fJMyzZo1g7Ozs7EMEdUNvKyZiKxm0aJF2LRpU7n1c+bMwZYtW9CzZ09MnToVNjY2+PLLL1FQUID333/fWK5t27bo27cvOnfuDA8PD+zbtw8//fQTpk2bBgA4ffo0+vfvjxEjRqBt27awsbHBmjVrkJqailGjRlnsfRLR3eNlzURkcSWXNVfm0qVLSEtLw6xZs/DPP//AYDAgPDwc77zzDrp3724s984772D9+vU4ffo0CgoK0KRJE4wbNw4vvfQSbG1tkZGRgdjYWMTHx+PSpUuwsbFB69at8cILL+Dxxx+3xFslIjNhYCEiIiLZYx8WIiIikj0GFiIiIpI9BhYiIiKSPQYWIiIikj0GFiIiIpI9BhYiIiKSPQ4cVwGDwYCrV6/C2dkZCoXC2tUhIiKqMyRJQnZ2NgICAqBUmq9dhIGlAlevXjWZq4SIiIiq59KlSwgMDDTb/hhYKuDs7AxAHOySOUuIiIjozrRaLYKCgozfpebCwFKBktNALi4uDCxEREQ1YO4uFex0ayFnr+UgM09n7WoQERHVSWxhsQBJkjDz58M4kazF6LDGmNwrBP6u9tauFhERUZ3BFhYL0N4sQp5OjzydHt/sSETv9//ES6v+xdlrOdauGhERUZ3A2ZoroNVq4erqiqysLLP1YZEkCdtPp+GLbeewO/E6AEChACLb+mFK32YIDXIzy+sQERFZU218hwIMLBWqrYNdYv/FG1iw/Ry2HE81ruvRzBNT+jZDz+ZeHPuFiIjqLAYWC6rtwFLiTGo2Fmw/j3WHrqDIIH4M7Rq5YEqf5hjYzg8qJYMLERHVLQwsFmSpwFLiSuZNLPz7PFbsuYSbhXoAQIiXI/7TuymGdWoEjY2q1utARERkDgwsFmTpwFLieq4O3+28gMU7LyDrZiEAwMdZg8m9QjA6rDGc7WwtVhciIqKaYGCxIGsFlhK5BUVYvicJC/9ORIo2HwDgYmeD8d2DMfG+YHg5aSxeJyIioqpgYLEgaweWEroiA9YeuoIF28/hfFouAEBjo8TIrkF4qldTBHk4WK1uREREFWFgsSC5BJYSBoOEzcdT8cW2s/j3cpZxfVMvR3QIdEWHQDeEBrningBX2NmyvwsREVkPA4sFyS2wlJAkCQnnM/DFtnP4+0x6uedVSgVa+jojtDjEdAh0RSs/Z9iqOD4gERFZBgOLBck1sJR1PVeHfy9n4vClLBy+nIl/L2chPaegXDmNjRJtA1wQWhxgOgS6oamXI5S8ZJqIiGoBA4sF1YXAcitJkpCclW8ML4cvZ+Lw5Sxk5xeVK+ussUH7klNJga5oH+iKAFd7hhgiIrprDCwWVBcDS0UMBgkXMnJx+HKWaI25nIVjV7OQX2goV9ZGqYC3swY+LnbwddbA18UOvi7Fj13s4Ff82NXeliPxEhFRpRhYLKi+BJaKFOkNOJ2aY9IScyol2zjS7p2obZTwddHA11kEGR+X0nDj62wHHxc7eDiq4WJnAxv2nSEianAYWCyoPgeWihTpDUjP0SFVmy9u2QW4Vrycoi1dvpFXWK39Omls4GpvCxd7W7jai+Vbby6VrGNHYSKiuqm2vkNtzLYnqrNsVEr4udrBz9XutuXyC/VIyy7Atex8pGoLigOOCDQpxaHmmrYA2QWi30xOQRFyCopwJfNmtevkqFbB1d4Wzna2sFer4KBWwUFtU3yvgr1aBUe1TZnnSp+3L152LLPsoFZBY6Pk6SwiojqKgYWqzM5WhSAPhzsOWFeoN0B7sxBZt9wqWiduRcbncorDTq5Oj1ydHsjKN1v9lQqYhB7jssYGDrYqOGjE+pIgZLzXqGBvawNHjel2jhobOKptYGfLIEREVNsYWMjsbFVKeDpp4FmDKQSK9AZo84uMYSYnvwh5uiLcLNQjT6dHbkERbur0yCvUi3tdEXJ1pcvivuRWhDydHgVFopOxQSpt9TEnlVJhDDqOGhWcNDaihUdjAyeNCEROxeHGUVMcdDSiBcix+Dk7WyU0Nipoiu/tbJVQqxiEiIhKMLCQrNiolPBwVMPDUW22feoNkgg8BUUmYUYEnSLkFogAVPp8abncAhGWck22LV1fsv/s/KIKLyG/GwqFGEenJMDc7t4k6NgooVGJe7WNErYly8X3mgrW3W49gxMRyQEDC9V7KqUCTsUtGeZkMEjIKw4zOQVFyCvQI6egCLkFRcgtCUK6ojLrRFlx0yO3zHMFRQbkF4rWoJJu8JIE5BcakF9oQFb1uwGZlaY40KhtRF+gkoBUEnRKgpVxuZLn7WyV0NiqYGergp2NUtzbivVinQp26tJlW5WCYYmIADCwENWYskwQ8jXTPiVJgk5vKA0whQYUFOmRXyjWFRSHmvzb3OuKDNDpDeK+yIACvQGFt6wzLlewrlBvQKHe9OLBgiJD8ak187Yi3YlSgdJQUxxwNMUBx1YlWn9sVArYqpSwVSlgo1Qal20reE5to4SNUgEblRJqlbi3VZWGqgoDlDFksbWJyJoYWIhkRKFQFLdUqOBiZ2u1ehgMpcGpJMyUhKWS4CTWFT8uEkGpZLmg0GDcvux2+YX64psB+UWi71FF60tamQwSjKfh5EChgDHElISb0qAj7ktClEopwpFKqSgOSSI0lTwuXV9+nUqlhG3x45LgJcJYccgq3s5GpTC+XmXP2ypLnmdrFdVtDCxEVI5SqYCdUmWV2b9LWpnyC0XYyS804KYx0OiRXxxwivRScWuQAUUGydgyVKg3oEhvgE4voaj4OV2RAUUGAwqLJBQaDGW2FfclrVhlW6oqClCSBNws1Bf3X6reuERyoC5ubSrp21TSZ6ncujKtVCX9mGxVStjaKExatmxKQllxMFKVCWYlIU2EsNLQVBLCygY2G6UCSkVpeZVCYdyXcVmphFIJ03sFGMIaEAYWIpKVsq1MsLdeK1OJWwNUaaARYca4XBxyCvUS9AYRlPQGqdzj0vWG0sf6kvUGFJZ5XFQmXBUZRAAr1Jeu1+nFfZGheL1ebF+kN6Ciwat1egN0ejFsQH2hKtsypSgOTxW0MtnalGmFUt7SOlWmvOnpxfKnFsXj4lOKSiVsbcS2tiYtXqb7F/URQUtZHMAUCkClEEFNqVRAqRDvRVmyrvgxA1kpBhYiotuQW4CqKoOh4takkpuuSDzWlenjVFJGZ3wsnissDkclfZxEi1VJABOvoTdIImwVh6dbQ1lR8bLxOUPxc/qyQU6s0xsk6KXS9bcbj72kvM5yh9biVMWBpiTMlG3JKhuqbg1IFQUt2zKnCUuec9TYYEZES2u/zTtiYCEiqoeUSgU0ShXMfHGcVRjKBJiyrVT6MqHJYIAIT4aS04JlW55uDW8VtEyVKVPSUlVYVPZ0o2mrVqFJmdJTkBWVK1k2SCU3EbSqSm+QINrEamcmHRc7BhYiIqK7plQqoIQCVuhSVauk4hBmkGAMM8bHhuLHkmhh0hc/NhhQHN5EK1lJ6CrUG+7Yr6uwzOnFsiGsrszdxsBCRERkBYrijsZUNXUjVhEREVGDxsBCREREsifrwBIXF4euXbvC2dkZPj4+GDp0KE6dOnXH7VatWoXWrVvDzs4O7du3x8aNGy1QWyIiIqotsg4s27dvR3R0NHbt2oUtW7agsLAQAwYMQG5ubqXb7Ny5E6NHj8akSZNw8OBBDB06FEOHDsXRo0ctWHMiIiIyJ4Uk3e4Kd3lJS0uDj48Ptm/fjt69e1dYZuTIkcjNzcWGDRuM67p164aOHTtiwYIFVXodrVYLV1dXZGVlwcXFxSx1JyIiaghq6ztU1i0st8rKygIAeHh4VFomISEBERERJusiIyORkJBQq3UjIiKi2lNnLms2GAyYMWMG7rvvPrRr167ScikpKfD1NZ0719fXFykpKZVuU1BQgIKCAuNjrVZ79xUmIiIis6kzLSzR0dE4evQoVqxYYfZ9x8XFwdXV1XgLCgoy+2sQERFRzdWJwDJt2jRs2LABf/75JwIDA29b1s/PD6mpqSbrUlNT4efnV+k2s2bNQlZWlvF26dIls9SbiIiIzEPWgUWSJEybNg1r1qzB1q1bERIScsdtunfvjvj4eJN1W7ZsQffu3SvdRqPRwMXFxeRGRERE8iHrPizR0dFYtmwZ1q1bB2dnZ2M/FFdXV9jb2wMAxo8fj0aNGiEuLg4A8Nxzz6FPnz746KOP8NBDD2HFihXYt28fvvrqK6u9DyIiIro7sm5h+eKLL5CVlYW+ffvC39/feFu5cqWxTFJSEpKTk42Pe/TogWXLluGrr75CaGgofvrpJ6xdu/a2HXWJiIhI3urUOCyWwnFYiIiIaobjsBAREVGDxcBCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLIn+8Dy119/YfDgwQgICIBCocDatWtvW37btm1QKBTlbikpKZapMBEREZmd7ANLbm4uQkNDMX/+/Gptd+rUKSQnJxtvPj4+tVRDIiIiqm021q7AnQwaNAiDBg2q9nY+Pj5wc3Mzf4WIiIjI4mTfwlJTHTt2hL+/Px544AH8888/ty1bUFAArVZrciMiIiL5qHeBxd/fHwsWLMDPP/+Mn3/+GUFBQejbty8OHDhQ6TZxcXFwdXU13oKCgixYYyIiIroThSRJkrUrUVUKhQJr1qzB0KFDq7Vdnz590LhxY/zwww8VPl9QUICCggLjY61Wi6CgIGRlZcHFxeVuqkxERNSgaLVauLq6mv07VPZ9WMwhLCwMO3bsqPR5jUYDjUZjwRoRERFRddS7U0IVOXToEPz9/a1dDSIiIqoh2bew5OTk4OzZs8bHiYmJOHToEDw8PNC4cWPMmjULV65cwffffw8AmDdvHkJCQnDPPfcgPz8fCxcuxNatW7F582ZrvQUiIiK6S7IPLPv27UO/fv2Mj2NiYgAAEyZMwOLFi5GcnIykpCTj8zqdDi+88AKuXLkCBwcHdOjQAX/88YfJPoiIiKhuqVOdbi2ltjoMERER1Xe19R3aIPqwEBERUd3GwEJERESyx8BCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLIn+4HjqI6QJCDvOpB5AdAXAkob05vKFlCqyqy75bHKFlAoAYXC2u+EiIhkiIHFUpaPFl/o9u633NwqWOcOaFwApcwawIp0QNYl4MaF4ltimeWLQIH27l/j1kBjowFsHcRN7QDY2gO2jhUsl9zsAbXjLdsU32zUgL4IMBSKUGUovOVxUZn1tzw26E2fAwAnX8AlAHBpJO7tOMggyYwkAbocIDcNKMwHPJoCtnbWrhVRjTCwWMqVA0BOStXLK5SAnZtpiHHwKF22c63gi7yiewfxpV+VlgtJAm7eqCSQXACyLgOS4fb7cPYXr1nyJW/80i9eNhQHhMr2U1IG+VU/VnKhdi4OMGVCjGuj0mWXAPEzrW4rUlEBkJsO5KUX32eI263rSu7VToB7E8A9GPAIEffuxff27mzFqsvKBpDcjOKff1qZ34E009+L3HRAX1C6vUIFeLUE/NoX39oBfh0ARy/rvSeqGoMe0F4p/VuscQHcGgNuQTX7u1IHcWj+CtTKsMIX/hF/RG7eqOCWafq4MM88r2mkKB9m1GVaJJS2QPZV4PoFoCDr9ruysS/+Agy+5QsxWHx4bO2rViWDAZD0pS0ZhqLiUFPmsb5I/LHV5QGFuUDhzVuWc8WxMi7fFI+Ny7nF5YuXi3TFp6ZKTlHZAqri1pxy6+9QTpKAnFRAexXQXgby73DcStg6lA81zv6i3ibBo2Q5A9BlV23fVaFxBTyCy/wMy/z8XIPE+6zPJAnQ68r8jhT/zhTlF98KytwXVLDulnt9BeUkQ/HpTZW4V6rEl4nJ45LnFbc8Lvu8UtT1dgGkqmzsAZW68s+3s39piPEtDjEeTeXXylvf5WeZ/pNY9paZVPzPXAU0LuLzWxJgyi67NQEcPC0aaGpraH4GlgpYfS6hwnwgP7M0wORdLx9y8jNL/9iW/cNr/NLOE1/+NeHkV3kocfJtEEm+2gpygOxk8R+Q9mqZ+zLLeRk137/SRvzRcfASLW2OXmLZ0UusL3ns4AkUZJdvJbueeOcWPoVK/IErG2QcPIu/lHXiy1OvK/2i1hcWL+tK743LhaVf5iXrDUXiS1OlFsHvtst3eF5pI8JB2d9342fgls9B4S3P3amVsC6wdSj9+Rt/FzwBR+8K1nuJ06SSJH4PU48CKYeBlCPidv18Ja/hCPjeU9wK016EGJ+24p8dazIYxN+/vOvFrY1lWh3zMsTPXNKLf4AkQ/F9mceSvvQfppLnjOVuKa9QimOndgI0TuJe7QhonEvXGx9X8lzZv5f6IvEPTmWh5OaN2793lVoEEddAIF8rQkxe+p2Pma1DcYgpDjLGQFN8c/QxazhlYLEgqwcWc9EXVhBmbpa2UJSsK8oX/2G5B4s0bu0/SPVV4c0yIaZMkMlOFn/YKgwjxV9E5mjy1eWJP3BlT/VdL7Nck//c6zKlbZmWRzvAxk6cPi25V2nKPL7lORs70Seqom2UKtMvQEkvwoLJ47LPl7ndWkZpUyaEeJoGEHMpyAZSj5uGmGvHxd+FWymUgEczEWTs3Yvfs7r0eKg0tywXP2eyrL5lO404zZWXUSaEZJie5iy7/ub1OhQ6FaXhRWkjPuuS/vabOHqb/sNY9ubsL36/ytLlilNEmZeAzIuin2HmJfFZz7okXvNObB2AWZfL77uGGFgsqN4EFqKqMhhEC4xJkEkU/8UZv3w0xS0cxV8+KvUty2W+iCpaVqiKOy3riltgdLcsF1ayvoJlQ6H4sit3qtOx+HFxh+zKToPaOoj3QhXTFwHXzxUHmDJBJjfN2jUrpXERAd/Y8ugpHqudSk+rKVXi967kXqEULQll1xmfU5RfJxlEINDliFtBzi3LFT2XK0IgKvlqVWlK+5jdenNrIlpyzKmooDjQJJUPM5lJ4h8nl0bA80fN9pIMLBbEwEJEVIHsVBFc0k6IL+aypwSL8k1PD5b0Ayp7arAov/j0YvFpxqL80vBZ0sLo4HnLzaP01GfJzd5DBGW5kqTivnU5IrzocsX7dwkQp9zl1DdIXyhasJx9zbbL2voOrec97IiIyGycfcWtRYT59mkwyOsL3BwUCtGip3YAnHysXZvbU9maNazUpnr2W0JERHVKfQsrVGv4m0JERESyx1NCFSjp1qPVmmHkViIiogak5LvT3F1kGVgqkJ0tBuoKCgqyck2IiIjqpuzsbLi6upptf7xKqAIGgwFXr16Fs7MzFGYaJE2r1SIoKAiXLl3ilUdWwONvXTz+1sXjb10N7fhLkoTs7GwEBARAacY+SmxhqYBSqURgYGCt7NvFxaVB/MLKFY+/dfH4WxePv3U1pONvzpaVEux0S0RERLLHwEJERESyx8BiIRqNBrGxsdBoNNauSoPE429dPP7WxeNvXTz+5sFOt0RERCR7bGEhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgsZD58+cjODgYdnZ2CA8Px549e6xdpQZhzpw5UCgUJrfWrVtbu1r11l9//YXBgwcjICAACoUCa9euNXlekiTMnj0b/v7+sLe3R0REBM6cOWOdytZDdzr+EydOLPd5GDhwoHUqWw/FxcWha9eucHZ2ho+PD4YOHYpTp06ZlMnPz0d0dDQ8PT3h5OSE4cOHIzU11Uo1rlsYWCxg5cqViImJQWxsLA4cOIDQ0FBERkbi2rVr1q5ag3DPPfcgOTnZeNuxY4e1q1Rv5ebmIjQ0FPPnz6/w+ffffx+ffPIJFixYgN27d8PR0RGRkZHIz8+3cE3rpzsdfwAYOHCgyedh+fLlFqxh/bZ9+3ZER0dj165d2LJlCwoLCzFgwADk5uYayzz//PP45ZdfsGrVKmzfvh1Xr17Fo48+asVa1yES1bqwsDApOjra+Fiv10sBAQFSXFycFWvVMMTGxkqhoaHWrkaDBEBas2aN8bHBYJD8/PykDz74wLguMzNT0mg00vLly61Qw/rt1uMvSZI0YcIEaciQIVapT0N07do1CYC0fft2SZLE77utra20atUqY5kTJ05IAKSEhARrVbPOYAtLLdPpdNi/fz8iIiKM65RKJSIiIpCQkGDFmjUcZ86cQUBAAJo2bYqxY8ciKSnJ2lVqkBITE5GSkmLyWXB1dUV4eDg/Cxa0bds2+Pj4oFWrVpgyZQoyMjKsXaV6KysrCwDg4eEBANi/fz8KCwtNPgOtW7dG48aN+RmoAgaWWpaeng69Xg9fX1+T9b6+vkhJSbFSrRqO8PBwLF68GJs2bcIXX3yBxMRE9OrVC9nZ2dauWoNT8vvOz4L1DBw4EN9//z3i4+Px3nvvYfv27Rg0aBD0er21q1bvGAwGzJgxA/fddx/atWsHQHwG1Go13NzcTMryM1A1nK2Z6rVBgwYZlzt06IDw8HA0adIEP/74IyZNmmTFmhFZ3qhRo4zL7du3R4cOHdCsWTNs27YN/fv3t2LN6p/o6GgcPXqUfebMiC0stczLywsqlapcL/DU1FT4+flZqVYNl5ubG1q2bImzZ89auyoNTsnvOz8L8tG0aVN4eXnx82Bm06ZNw4YNG/Dnn38iMDDQuN7Pzw86nQ6ZmZkm5fkZqBoGllqmVqvRuXNnxMfHG9cZDAbEx8eje/fuVqxZw5STk4Nz587B39/f2lVpcEJCQuDn52fyWdBqtdi9ezc/C1Zy+fJlZGRk8PNgJpIkYdq0aVizZg22bt2KkJAQk+c7d+4MW1tbk8/AqVOnkJSUxM9AFfCUkAXExMRgwoQJ6NKlC8LCwjBv3jzk5uYiKirK2lWr91588UUMHjwYTZo0wdWrVxEbGwuVSoXRo0dbu2r1Uk5Ojsl/64mJiTh06BA8PDzQuHFjzJgxA2+//TZatGiBkJAQvPHGGwgICMDQoUOtV+l65HbH38PDA3PnzsXw4cPh5+eHc+fO4eWXX0bz5s0RGRlpxVrXH9HR0Vi2bBnWrVsHZ2dnY78UV1dX2Nvbw9XVFZMmTUJMTAw8PDzg4uKC6dOno3v37ujWrZuVa18HWPsypYbi008/lRo3biyp1WopLCxM2rVrl7Wr1CCMHDlS8vf3l9RqtdSoUSNp5MiR0tmzZ61drXrrzz//lACUu02YMEGSJHFp8xtvvCH5+vpKGo1G6t+/v3Tq1CnrVroeud3xz8vLkwYMGCB5e3tLtra2UpMmTaSnnnpKSklJsXa1642Kjj0A6dtvvzWWuXnzpjR16lTJ3d1dcnBwkIYNGyYlJydbr9J1iEKSJMnyMYmIiIio6tiHhYiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgYWIiIhkj4GFiIiIZI+BhYiIiGSPgYWIiIhkz6qB5a+//sLgwYMREBAAhUKBtWvX3nGbbdu2oVOnTtBoNGjevDkWL15crsz8+fMRHBwMOzs7hIeHY8+ePeavPBEREVmMVQNLbm4uQkNDMX/+/CqVT0xMxEMPPYR+/frh0KFDmDFjBiZPnozff//dWGblypWIiYlBbGwsDhw4gNDQUERGRuLatWu19TaIiIiolikkSZKsXQkAUCgUWLNmDYYOHVppmZkzZ+LXX3/F0aNHjetGjRqFzMxMbNq0CQAQHh6Orl274rPPPgMAGAwGBAUFYfr06XjllVdq9T0QERFR7bCxdgWqIyEhARERESbrIiMjMWPGDACATqfD/v37MWvWLOPzSqUSERERSEhIqHS/BQUFKCgoMD42GAy4fv06PD09oVAozPsmiIiI6jFJkpCdnY2AgAAoleY7kVOnAktKSgp8fX1N1vn6+kKr1eLmzZu4ceMG9Hp9hWVOnjxZ6X7j4uIwd+7cWqkzERFRQ3Tp0iUEBgaabX91KrDUllmzZiEmJsb4OCsrC40bN8alS5fg4uJixZoRERHVLVqtFkFBQXB2djbrfutUYPHz80NqaqrJutTUVLi4uMDe3h4qlQoqlarCMn5+fpXuV6PRQKPRlFvv4uLCwEJERFQD5u5SUafGYenevTvi4+NN1m3ZsgXdu3cHAKjVanTu3NmkjMFgQHx8vLEMERER1T1WDSw5OTk4dOgQDh06BEBctnzo0CEkJSUBEKdqxo8fbyz/zDPP4Pz583j55Zdx8uRJfP755/jxxx/x/PPPG8vExMTg66+/xnfffYcTJ05gypQpyM3NRVRUlEXfGxEREZmPVU8J7du3D/369TM+LulHMmHCBCxevBjJycnG8AIAISEh+PXXX/H888/jf//7HwIDA7Fw4UJERkYay4wcORJpaWmYPXs2UlJS0LFjR2zatKlcR1wiIiKqO2QzDoucaLVauLq6Iisri31YiIiIqqG2vkPrVB8WIiIiapgYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9mQRWObPn4/g4GDY2dkhPDwce/bsqbRs3759oVAoyt0eeughY5mJEyeWe37gwIGWeCtERERUC2ysXYGVK1ciJiYGCxYsQHh4OObNm4fIyEicOnUKPj4+5cqvXr0aOp3O+DgjIwOhoaF4/PHHTcoNHDgQ3377rfGxRqOpvTdBREREtcrqLSwff/wxnnrqKURFRaFt27ZYsGABHBwcsGjRogrLe3h4wM/Pz3jbsmULHBwcygUWjUZjUs7d3d0Sb4eIiIhqgVUDi06nw/79+xEREWFcp1QqERERgYSEhCrt45tvvsGoUaPg6Ohosn7btm3w8fFBq1atMGXKFGRkZFS6j4KCAmi1WpMbERERyYdVA0t6ejr0ej18fX1N1vv6+iIlJeWO2+/ZswdHjx7F5MmTTdYPHDgQ33//PeLj4/Hee+9h+/btGDRoEPR6fYX7iYuLg6urq/EWFBRU8zdFREREZmf1Pix345tvvkH79u0RFhZmsn7UqFHG5fbt26NDhw5o1qwZtm3bhv79+5fbz6xZsxATE2N8rNVqGVqIiIhkxKotLF5eXlCpVEhNTTVZn5qaCj8/v9tum5ubixUrVmDSpEl3fJ2mTZvCy8sLZ8+erfB5jUYDFxcXkxsRERHJh1UDi1qtRufOnREfH29cZzAYEB8fj+7du99221WrVqGgoABPPPHEHV/n8uXLyMjIgL+//13XmYiIiCzP6lcJxcTE4Ouvv8Z3332HEydOYMqUKcjNzUVUVBQAYPz48Zg1a1a57b755hsMHToUnp6eJutzcnLw0ksvYdeuXbhw4QLi4+MxZMgQNG/eHJGRkRZ5T0RERGReVu/DMnLkSKSlpWH27NlISUlBx44dsWnTJmNH3KSkJCiVprnq1KlT2LFjBzZv3lxufyqVCocPH8Z3332HzMxMBAQEYMCAAXjrrbc4FgsREVEdpZAkSbJ2JeRGq9XC1dUVWVlZ7M9CRERUDbX1HWr1U0JEREREd8LAQkRERLLHwEJERESyx8BCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLIni8Ayf/58BAcHw87ODuHh4dizZ0+lZRcvXgyFQmFys7OzMykjSRJmz54Nf39/2NvbIyIiAmfOnKntt0FERES1xOqBZeXKlYiJiUFsbCwOHDiA0NBQREZG4tq1a5Vu4+LiguTkZOPt4sWLJs+///77+OSTT7BgwQLs3r0bjo6OiIyMRH5+fm2/HSIiIqoFVg8sH3/8MZ566ilERUWhbdu2WLBgARwcHLBo0aJKt1EoFPDz8zPefH19jc9JkoR58+bh9ddfx5AhQ9ChQwd8//33uHr1KtauXWuBd0RERETmZtXAotPpsH//fkRERBjXKZVKREREICEhodLtcnJy0KRJEwQFBWHIkCE4duyY8bnExESkpKSY7NPV1RXh4eG33ScRERHJl1UDS3p6OvR6vUkLCQD4+voiJSWlwm1atWqFRYsWYd26dViyZAkMBgN69OiBy5cvA4Bxu+rss6CgAFqt1uRGRERE8mH1U0LV1b17d4wfPx4dO3ZEnz59sHr1anh7e+PLL7+s8T7j4uLg6upqvAUFBZmxxkRERHS3rBpYvLy8oFKpkJqaarI+NTUVfn5+VdqHra0t7r33Xpw9exYAjNtVZ5+zZs1CVlaW8Xbp0qXqvhUiIiKqRVYNLGq1Gp07d0Z8fLxxncFgQHx8PLp3716lfej1ehw5cgT+/v4AgJCQEPj5+ZnsU6vVYvfu3ZXuU6PRwMXFxeRGRERE8mFj7QrExMRgwoQJ6NKlC8LCwjBv3jzk5uYiKioKADB+/Hg0atQIcXFxAIA333wT3bp1Q/PmzZGZmYkPPvgAFy9exOTJkwGIK4hmzJiBt99+Gy1atEBISAjeeOMNBAQEYOjQodZ6m0RERHQXrB5YRo4cibS0NMyePRspKSno2LEjNm3aZOw0m5SUBKWytCHoxo0beOqpp5CSkgJ3d3d07twZO3fuRNu2bY1lXn75ZeTm5uLpp59GZmYmevbsiU2bNpUbYI6IiIjqBoUkSZK1KyE3Wq0Wrq6uyMrK4ukhIiKiaqit79A6d5UQERERNTwMLERERCR7DCxEREQkewwsREREJHsMLERERCR7DCxEREQkewwsREREJHsMLERERCR7DCxEREQkewwsREREJHsMLERERCR7DCxEREQkewwsREREJHsMLERERCR7DCxEREQkewwsREREJHsMLERERCR7DCxEREQke7IILPPnz0dwcDDs7OwQHh6OPXv2VFr266+/Rq9eveDu7g53d3dERESUKz9x4kQoFAqT28CBA2v7bRAREVEtsXpgWblyJWJiYhAbG4sDBw4gNDQUkZGRuHbtWoXlt23bhtGjR+PPP/9EQkICgoKCMGDAAFy5csWk3MCBA5GcnGy8LV++3BJvh4iIiGqBQpIkyZoVCA8PR9euXfHZZ58BAAwGA4KCgjB9+nS88sord9xer9fD3d0dn332GcaPHw9AtLBkZmZi7dq1NaqTVquFq6srsrKy4OLiUqN9EBERNUS19R1q1RYWnU6H/fv3IyIiwrhOqVQiIiICCQkJVdpHXl4eCgsL4eHhYbJ+27Zt8PHxQatWrTBlyhRkZGSYte5ERERkOTbWfPH09HTo9Xr4+vqarPf19cXJkyertI+ZM2ciICDAJPQMHDgQjz76KEJCQnDu3Dm8+uqrGDRoEBISEqBSqcrto6CgAAUFBcbHWq22hu+IiIiIaoNVA8vdevfdd7FixQps27YNdnZ2xvWjRo0yLrdv3x4dOnRAs2bNsG3bNvTv37/cfuLi4jB37lyL1JmIiIiqz6qnhLy8vKBSqZCammqyPjU1FX5+frfd9sMPP8S7776LzZs3o0OHDrct27RpU3h5eeHs2bMVPj9r1ixkZWUZb5cuXareGyEiIqJaZdXAolar0blzZ8THxxvXGQwGxMfHo3v37pVu9/777+Ott97Cpk2b0KVLlzu+zuXLl5GRkQF/f/8Kn9doNHBxcTG5ERERkXxY/bLmmJgYfP311/juu+9w4sQJTJkyBbm5uYiKigIAjB8/HrNmzTKWf++99/DGG29g0aJFCA4ORkpKClJSUpCTkwMAyMnJwUsvvYRdu3bhwoULiI+Px5AhQ9C8eXNERkZa5T0SERHR3bF6H5aRI0ciLS0Ns2fPRkpKCjp27IhNmzYZO+ImJSVBqSzNVV988QV0Oh0ee+wxk/3ExsZizpw5UKlUOHz4ML777jtkZmYiICAAAwYMwFtvvQWNRmPR90ZERETmYfVxWOSI47AQERHVTL0ch4WIiIioKhhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2ZBFY5s+fj+DgYNjZ2SE8PBx79uy5bflVq1ahdevWsLOzQ/v27bFx40aT5yVJwuzZs+Hv7w97e3tERETgzJkztfkWiIiIqBZZPbCsXLkSMTExiI2NxYEDBxAaGorIyEhcu3atwvI7d+7E6NGjMWnSJBw8eBBDhw7F0KFDcfToUWOZ999/H5988gkWLFiA3bt3w9HREZGRkcjPz7fU2yIiIiIzUkiSJFmzAuHh4ejatSs+++wzAIDBYEBQUBCmT5+OV155pVz5kSNHIjc3Fxs2bDCu69atGzp27IgFCxZAkiQEBATghRdewIsvvggAyMrKgq+vLxYvXoxRo0bdsU5arRaurq7IysqCi4uLmd4pERFR/Vdb36E2ZttTDeh0Ouzfvx+zZs0yrlMqlYiIiEBCQkKF2yQkJCAmJsZkXWRkJNauXQsASExMREpKCiIiIozPu7q6Ijw8HAkJCRUGloKCAhQUFBgfZ2VlARAHnYiIiKqu5LvT3O0hVg0s6enp0Ov18PX1NVnv6+uLkydPVrhNSkpKheVTUlKMz5esq6zMreLi4jB37txy64OCgqr2RoiIiMhERkYGXF1dzbY/qwYWuZg1a5ZJq01mZiaaNGmCpKQksx5sqpxWq0VQUBAuXbrE03AWwmNueTzmlsdjbnlZWVlo3LgxPDw8zLpfqwYWLy8vqFQqpKammqxPTU2Fn59fhdv4+fndtnzJfWpqKvz9/U3KdOzYscJ9ajQaaDSacutdXV35C25hLi4uPOYWxmNueTzmlsdjbnlKpXmv67HqVUJqtRqdO3dGfHy8cZ3BYEB8fDy6d+9e4Tbdu3c3KQ8AW7ZsMZYPCQmBn5+fSRmtVovdu3dXuk8iIiKSN6ufEoqJicGECRPQpUsXhIWFYd68ecjNzUVUVBQAYPz48WjUqBHi4uIAAM899xz69OmDjz76CA899BBWrFiBffv24auvvgIAKBQKzJgxA2+//TZatGiBkJAQvPHGGwgICMDQoUOt9TaJiIjoLlg9sIwcORJpaWmYPXs2UlJS0LFjR2zatMnYaTYpKcmkWalHjx5YtmwZXn/9dbz66qto0aIF1q5di3bt2hnLvPzyy8jNzcXTTz+NzMxM9OzZE5s2bYKdnV2V6qTRaBAbG1vhaSKqHTzmlsdjbnk85pbHY255tXXMrT4OCxEREdGdWH2kWyIiIqI7YWAhIiIi2WNgISIiItljYCEiIiLZa7CBZf78+QgODoadnR3Cw8OxZ8+e25ZftWoVWrduDTs7O7Rv3x4bN260UE3rj+oc86+//hq9evWCu7s73N3dERERccefEZVX3d/zEitWrIBCoeBQADVQ3WOemZmJ6Oho+Pv7Q6PRoGXLlvz7Uk3VPebz5s1Dq1atYG9vj6CgIDz//PPIz8+3UG3rvr/++guDBw9GQEAAFAqFcS6/29m2bRs6deoEjUaD5s2bY/HixdV/YakBWrFihaRWq6VFixZJx44dk5566inJzc1NSk1NrbD8P//8I6lUKun999+Xjh8/Lr3++uuSra2tdOTIEQvXvO6q7jEfM2aMNH/+fOngwYPSiRMnpIkTJ0qurq7S5cuXLVzzuqu6x7xEYmKi1KhRI6lXr17SkCFDLFPZeqK6x7ygoEDq0qWL9OCDD0o7duyQEhMTpW3btkmHDh2ycM3rruoe86VLl0oajUZaunSplJiYKP3++++Sv7+/9Pzzz1u45nXXxo0bpddee01avXq1BEBas2bNbcufP39ecnBwkGJiYqTjx49Ln376qaRSqaRNmzZV63UbZGAJCwuToqOjjY/1er0UEBAgxcXFVVh+xIgR0kMPPWSyLjw8XPrPf/5Tq/WsT6p7zG9VVFQkOTs7S999911tVbHeqckxLyoqknr06CEtXLhQmjBhAgNLNVX3mH/xxRdS06ZNJZ1OZ6kq1jvVPebR0dHS/fffb7IuJiZGuu+++2q1nvVVVQLLyy+/LN1zzz0m60aOHClFRkZW67Ua3CkhnU6H/fv3IyIiwrhOqVQiIiICCQkJFW6TkJBgUh4AIiMjKy1PpmpyzG+Vl5eHwsJCs0+mVV/V9Ji/+eab8PHxwaRJkyxRzXqlJsd8/fr16N69O6Kjo+Hr64t27drhv//9L/R6vaWqXafV5Jj36NED+/fvN542On/+PDZu3IgHH3zQInVuiMz1HWr1kW4tLT09HXq93jiSbglfX1+cPHmywm1SUlIqLJ+SklJr9axPanLMbzVz5kwEBASU+6WnitXkmO/YsQPffPMNDh06ZIEa1j81Oebnz5/H1q1bMXbsWGzcuBFnz57F1KlTUVhYiNjYWEtUu06ryTEfM2YM0tPT0bNnT0iShKKiIjzzzDN49dVXLVHlBqmy71CtVoubN2/C3t6+SvtpcC0sVPe8++67WLFiBdasWVPl6RWoerKzszFu3Dh8/fXX8PLysnZ1GgyDwQAfHx989dVX6Ny5M0aOHInXXnsNCxYssHbV6q1t27bhv//9Lz7//HMcOHAAq1evxq+//oq33nrL2lWjO2hwLSxeXl5QqVRITU01WZ+amgo/P78Kt/Hz86tWeTJVk2Ne4sMPP8S7776LP/74Ax06dKjNatYr1T3m586dw4ULFzB48GDjOoPBAACwsbHBqVOn0KxZs9qtdB1Xk99zf39/2NraQqVSGde1adMGKSkp0Ol0UKvVtVrnuq4mx/yNN97AuHHjMHnyZABA+/btjXPPvfbaayZz15F5VPYd6uLiUuXWFaABtrCo1Wp07twZ8fHxxnUGgwHx8fHo3r17hdt0797dpDwAbNmypdLyZKomxxwA3n//fbz11lvYtGkTunTpYomq1hvVPeatW7fGkSNHcOjQIePtkUceQb9+/XDo0CEEBQVZsvp1Uk1+z++77z6cPXvWGA4B4PTp0/D392dYqYKaHPO8vLxyoaQkMEqcWq9WmO07tHr9geuHFStWSBqNRlq8eLF0/Phx6emnn5bc3NyklJQUSZIkady4cdIrr7xiLP/PP/9INjY20ocffiidOHFCio2N5WXN1VTdY/7uu+9KarVa+umnn6Tk5GTjLTs721pvoc6p7jG/Fa8Sqr7qHvOkpCTJ2dlZmjZtmnTq1Clpw4YNko+Pj/T2229b6y3UOdU95rGxsZKzs7O0fPly6fz589LmzZulZs2aSSNGjLDWW6hzsrOzpYMHD0oHDx6UAEgff/yxdPDgQenixYuSJEnSK6+8Io0bN85YvuSy5pdeekk6ceKENH/+fF7WXB2ffvqp1LhxY0mtVkthYWHSrl27jM/16dNHmjBhgkn5H3/8UWrZsqWkVqule+65R/r1118tXOO6rzrHvEmTJhKAcrfY2FjLV7wOq+7veVkMLDVT3WO+c+dOKTw8XNJoNFLTpk2ld955RyoqKrJwreu26hzzwsJCac6cOVKzZs0kOzs7KSgoSJo6dap048YNy1e8jvrzzz8r/PtccpwnTJgg9enTp9w2HTt2lNRqtdS0aVPp22+/rfbrKiSJbWBEREQkbw2uDwsRERHVPQwsREREJHsMLERERCR7DCxEREQkewwsREREJHsMLERERCR7DCxEREQkewwsREREJHsMLERERCR7DCxEREQkewwsREREJHsMLERERCR7DCxEREQkewwsREREJHsMLERERCR7DCxEZFWff/45FAoFwsPDyz134cIFKBQKfPjhhxVu++GHH0KhUODChQvlnluzZg0GDRoELy8vqNVqBAQEYMSIEdi6dau53wIRWQADCxFZ1dKlSxEcHIw9e/bg7Nmzd70/SZIQFRWFRx99FKmpqYiJicGCBQsQHR2N8+fPo3///ti5c6cZak5ElmRj7QoQUcOVmJiInTt3YvXq1fjPf/6DpUuXIjY29q72+dFHH2Hx4sWYMWMGPv74YygUCuNzr732Gn744QfY2PBPH1FdwxYWIrKapUuXwt3dHQ899BAee+wxLF269K72d/PmTcTFxaF169bG00W3GjduHMLCwu7qdYjI8hhYiMhqli5dikcffRRqtRqjR4/GmTNnsHfv3hrvb8eOHbh+/TrGjBkDlUplxpoSkbUxsBCRVezfvx8nT57EqFGjAAA9e/ZEYGDgXbWynDhxAgDQvn17s9SRiOSDgYWIrGLp0qXw9fVFv379AAAKhQIjR47EihUroNfra7RPrVYLAHB2djZbPYlIHhhYiMji9Ho9VqxYgX79+iExMRFnz57F2bNnER4ejtTUVMTHx1drfyV9VVxcXAAA2dnZZq8zEVkXAwsRWdzWrVuRnJyMFStWoEWLFsbbiBEjAMB4WsjOzg6A6Exbkby8PJNyrVu3BgAcOXKkVutPRJbHa/uIyOKWLl0KHx8fzJ8/v9xzq1evxpo1a7BgwQJ4e3vDwcEBp06dqnA/p06dgoODA7y8vACIfjDu7u5Yvnw5Xn31VXa8JapH2MJCRBZ18+ZNrF69Gg8//DAee+yxcrdp06YhOzsb69evh0qlwoABA/DLL78gKSnJZD9JSUn45ZdfMGDAAGMwcXBwwMyZM3HixAnMnDkTkiSVe/0lS5Zgz549FnmvRGQ+CqmiTzQRUS1ZuXIlRo0ahbVr12LIkCHlnjcYDPDz80O3bt2wfv16nDhxAt26dYOtrS2efvppBAcH48KFC/jqq69QWFiIXbt2oU2bNibbT5w4ET/88AM6deqExx57DH5+fkhJScHatWuxZ88e7Ny5E927d7fk2yaiu8TAQkQW9cgjj2DLli3IyMiAg4NDhWWioqKwdOlSJCcnw9PTEydPnsScOXPw559/4vr16/Dw8MD999+P2NhYY7+VW/3888/46quvsG/fPmi1Wnh7e6N3796YMmUK+vTpU5tvkYhqAQMLERERyR77sBAREZHsMbAQERGR7DGwEBERkewxsBAREZHsMbAQERGR7DGwEBERkexxaP4KGAwGXL16Fc7OzsZJ1YiIiOjOJElCdnY2AgICoFSar12EgaUCV69eRVBQkLWrQUREVGddunQJgYGBZtsfA0sFnJ2dAYiDXTJdPREREd2ZVqtFUFCQ8bvUXKweWObPn48PPvgAKSkpCA0NxaeffoqwsLBKy2dmZuK1117D6tWrcf36dTRp0gTz5s3Dgw8+CACIi4vD6tWrcfLkSdjb26NHjx5477330KpVqyrXqeQ0kIuLCwMLERFRDZi7S4VVO92uXLkSMTExiI2NxYEDBxAaGorIyEhcu3atwvI6nQ4PPPAALly4gJ9++gmnTp3C119/jUaNGhnLbN++HdHR0di1axe2bNmCwsJCDBgwALm5uZZ6W0RERGRmVp1LKDw8HF27dsVnn30GQHR2DQoKwvTp0/HKK6+UK79gwQJ88MEHOHnyJGxtbav0GmlpafDx8cH27dvRu3fvKm2j1Wrh6uqKrKwstrAQEZFsSJKErJuFyC80QFdkgE4v7gv1YrmwyICC4nudXqwvLJJM15VsV7yt2kaJWYPa3PnFq6i2vkOtdkpIp9Nh//79mDVrlnGdUqlEREQEEhISKtxm/fr16N69O6Kjo7Fu3Tp4e3tjzJgxmDlzJlQqVYXbZGVlAQA8PDzM/yaIiIjukiRJ0OYXIT2nAGnZBeXuxbIOadkFyMgtQKHevO0MLnY2Zg0stcVqgSU9PR16vR6+vr4m6319fXHy5MkKtzl//jy2bt2KsWPHYuPGjTh79iymTp2KwsJCxMbGlitvMBgwY8YM3HfffWjXrl2ldSkoKEBBQYHxsVarreG7IiIiAvQGCZl5OlzPLb0ZA0hOAdKydUjLKUB68WNdkaFa+7dRKqC2UcJWpYTaRgl18b2tqnS9rUoJTUkZlRK2xnIK8bh4Gwd1xf/wy43VO91Wh8FggI+PD7766iuoVCp07twZV65cwQcffFBhYImOjsbRo0exY8eO2+43Li4Oc+fOra1qExFRHSZJEvJ0+tLwkafD9RwdbuTpkJGrw40yoeR6nnicebMQ1e1w4ayxgbezBl5OmuJ7tcnjkmVPJzU0NnUjZJiT1QKLl5cXVCoVUlNTTdanpqbCz8+vwm38/f1ha2trcvqnTZs2SElJgU6ng1qtNq6fNm0aNmzYgL/++uuO14HPmjULMTExxscll2QREVH9J0kS0nIKkJiWi8R0cTufnosrN24aQ0h1W0BKuDnYwsNBDXdHNbydNPByVsPbya74XgMvZw28iwOJnW3DCyHVYbXAolar0blzZ8THx2Po0KEARAtKfHw8pk2bVuE29913H5YtWwaDwWAcPe/06dPw9/c3hhVJkjB9+nSsWbMG27ZtQ0hIyB3rotFooNFozPPGiIjozgqygct7gSsHgMbdgOCetf6SWTcLcaFMIBHhJAeJabnI1envuL3GRglPRxE+PMreigPJrc+52dvCRsUZcMzFqqeEYmJiMGHCBHTp0gVhYWGYN28ecnNzERUVBQAYP348GjVqhLi4OADAlClT8Nlnn+G5557D9OnTcebMGfz3v//Fs88+a9xndHQ0li1bhnXr1sHZ2RkpKSkAAFdXV9jb21v+TRIREZB5Cbi0G0jaBVzaBaQeA6TiVgulDTDie6D1Q3f9MvmFelzMyENieo4IJcWtJhcycpGeo6t0O6UCCHR3QIiXI0K8HNHU2xFB7g7wdCoNIA7qOtWLot6x6tEfOXIk0tLSMHv2bKSkpKBjx47YtGmTsSNuUlKSyTwEQUFB+P333/H888+jQ4cOaNSoEZ577jnMnDnTWOaLL74AAPTt29fktb799ltMnDix1t8TEVGDpy8CUo+WCSi7Ae2V8uXcGgMOnsDVg8CPE4pDy4OV7jY7vxCp2nykaguQkpWPFG0+UrX5SMkqvtfm41p2wW37jvg4a4yhxCSceDg0yH4hdYlVx2GRK47DQkRUDflacXqnJKBc2Q/ockzLKFSAfwcgqBvQOFzcu/gD+iIYVj8F5bHVMChtcbDbJzjq1EOEkeJQUrJcldM2gOi82tS7JJA4IcTbEU29HBHs5QgnDVtJalu9G4eFiIjqqKwrwMWd4tRO0m7gWpnTO8UkjQuKArpC690Jya6huGjXBik3VeLS3uMFSN9zCWnZZ5GWXYAbOcPwkU0yHkEC2v8zDfMLn8dWQ6cKX9pZYwNfVzv4udjB18UOvi4a+LmKZT8XOzRyt4eno9rsw8LfFUkCcq4Bzr53LkuVYmAhImoADAYJekmCQZIgSeI71FD82CCJCxYMxeukWx6XrDNIEjSJf8B3wwQoYNo4f93WH6fU9+AgWmGnrjn25fgi/wSAEyUlTt2mdiq8qI+Gg60CEYad+FL9P6xs+l9oG98Pv+IgUhJSHOtaC4kuD/h5EnBqI/Dgh0DYU9auUZ1Vx37yRER0O7kFRTh7LQenU7NxpuQ+NQdXMm+aZf+LbD+Gn0rCOYM/thtCsc/QEvsMrXAt373C8i52t44tUjymSJnHvi4aeDppoJIGAT9Pgu3xtXji4mtAt6VAywFmqbdV5F0Hlo0Qp8sAYOvbQIcRgJ2rdetVRzGwEBHVQTd1emMwOX1NhJLTqdm4fOPug4lCASgVCigVYsZdZfFjD2jRW3EYADDH8XXkuzaFl5MGA4sDiFfZe2cNPB3V1RxbxAYYvhCABBxfB6wcC4xaBrR44K7fk8XduAgsGQ5knAHs3AA7FyAzCUiYD/R71dq1q5MYWIiIZCy/UASTM9eycTo1B2dSxf2lG3mVXg3j5aRGCx9ntPR1QgtfZ7T0dUawlwM0KhUUytIwolQoyoQTBRQQYaXS/h97vgY2GgD/jvjhP0/UzhtW2QLDvxHnrE6sB1aUhJaI2nm92pByBFjyGJCTArgEAk/8DKSdBFZNEIEl7GnA0cvatRS0yYCzn/jByxwDCxGRDOgNEpKu5+FkshYnU7JxMkWLUynZSLqeB0MlwcTDUY0WPk5o6WsaTjwc1RVvcLeO/CTu2z9eO/svobIFHlsE/BQFnPgFWDEGGL0MaF4HQsv57cDKJ4ACLeBzD/DET4BLAODVEvDrAKQcBnb8HxD5jrVrKgbv+6ov4NsWGPYl4ORj7RrdFgMLEZGFXc/V4WSKFieTs3GqJJykZiO/sOLh390cbNHSxxktfEU4Kbn3crLgCN03LoqrgqAA2g2v/ddT2QKPfQusmgic3AAsrwOh5chPwJpnAEMh0KQnMGopYO8mnlMqgf6zgaWPiZaq7tEiyFjT9vdFK5DaoU70q2FgISKqJQVF4nSOCCXFt2QtrmUXVFheY6NES19ntPZzRis/Z7T2c0FLXyd4O2usf5nu0eLWlZBeYvwUSygJLT9FlQkty4Hm/S3z+tWRMB/4vbhvStuhosXC1s60TPMIMf7MpV3AXx8AD/+fxatplH4G2CUGWsXAdwEb+U9Pw8BCRFRNJbP3ZucXIaegENr8ImTnFyE7vxAXM/JwMiUbp1K0OJeWC30l53MaeziglZ8z2vg5o7W/C1r5OSPY0xEqpQz7EkgScHiVWK7t00G3slGXtrSc+rX49NAKoFk/y9ajMgYD8MdsYOen4nH4M0BknGhRuZVCIVpZFj8IHPge6PEs4HHn+e7MTpKA32aKlqAWkUDLSMvXoQYYWIiowdIVGXAyRYv0nAJk5xcVB49C5JQJINklywWlj3MKiioNIrdysbNBa38XtC5uMWlV3HpSp0ZcTT0GpJ0AVGqgzSOWf30bNfD4YtFp9dRGYPkoeYSWIh2wLho48qN4HDEXuO+523dgDb4PaHY/cG4rsO1d4NEvLVPXsk5tBM7Fi5/nwDjLv34N1aFPDBHR3cnTFeFgUiZ2J17H3sTrOHjpRqX9RqpCqQCc7WzhbGcj7jU28HO1Q2t/Z7Txc0Frf2f4udhZ/3TO3Sr5Qm4xoLRPhqXZqIHHvwN+HA+c/k2EljErgaZ9rVOfgmzRufb8NjF545D5QOioqm17/xsisBxeCfScAfi0qc2amiq8CWyaJZa7TwM8m1nute8SAwsR1Vs3cnXYe+E69l64jj0XbuDYlSwU3dIy4uZgiyB3h+LQYQMnjQggLiUhxM4GTmWWS9Y7aWzgoFbV/TByJwYDcORnsdxhhHXrYqMGRpSElk3AspLQ0sey9chOFZ1nUw4Dto7AyO+r1xm4USeg9cOiX86f7wAjl9ReXW+181Mg8yLgHAD0esFyr2sGDCxEVLdIEpCdUuHYEVczb4pwkihCyunUnHKbB7jaISzEA11DPBAW7IFm3k5QyrHfiFwkJQDay4DGRfR3sDYbjZjVeeU44MzvwLKRwNgfgZDelnn99LPAkkfFl76jNzDmRxFAquv+14GTv4rLtq8eBALuNX9db5WZBPz9sVge8Bagcar91zQjBhYiqju0V4ENzwOnN0EKuBfJnV7Adn0H7L1wA3suXK9wlNfmPk7oGuyBsBB3dA32QKC7gxUqXocdKe5s2+aR8le9WIuNBhj5gzglc2YzsHQEMHaVuIKpNl3eJ4baz8sA3EOAcasBj6Y125dPG9FidXilGLL/iZ/NW9eKbH4dKLopLrm2xKXpZsbAQkQVy0kTg19Z6Rx3od6A67k6pGUXIE2bD6cTy9Hh2PvQ6HMBAIqrBxFw9Qk0N7TEuqIRuGxoC5VSgXsCXNA12KP45g5PS45VUt8U6YDja8Vy+8esWpVybDTAiOLQcnaLCBJjVwHBPWvn9U7/Lq5UKswTrSFjVgFO3ne3z76vAEd/Bs7+IWa/btLDLFWt0PltYroDhRIY9F6dGNn2VgpJqmxw54ZLq9XC1dUVWVlZcHFxsXZ1iCxPlwt8FiZOBdwzDIiYA7gH3/Vu9QYJ13N1SM8pQFp2gfHeuGxcr8P1XB0AoBHSEGe7EL1VRwAAhwzN8E7hWDyg2o/xqs2wUxQCADJ9u0M9YDYcmtXiH/2G5lRx51YnXyDmBKCszrxAFlKYL+YcOvsHYOsghvVv3A2wdzffl/LBJcD6ZwFJL/qqPP6d+U6n/DID2P8t0Lg7EPVb7QQJfSGwoKeYHiDsaeDBD8z/GmXU1ncoW1iIqLy934iwAgDH1ohz7eH/AXq9WOWrRCRJwplrOdhxJh3/nE3H4StZyMgpqHSY+VspYMA4m3jMtFkOR+RDp1Djd5/JOBH8BB50cUCHwCgoXPKAhP8B+76FW2oC8MMgcSVLv1ct0yegvjtcfHVQu+HyDCuAOE01cqkYn+VcPLBidPF6R8C1EeAaWHwLAlzKPHZpdOdTXJIE/PUh8Ofb4nHoGOCRT8SAdubS+yXg0DLRV+hsfO3MmbTnaxFWHDzr9MSLbGGpAFtYqEEryAH+10Gcp+/1InBln2hOBgB7D6DvLKBLVIV/tFOy8rHjrAgoO86mI62CEV0VCsDDQQ3v4hl9vZxK7ovXOdnB33AFjf9+BbaXd4qNGvcAhnxW+empzCQxcujBpeK/YEBchdHvNTFPClVfQTbwQQvR5+GpP2vWsdSSCvOBX2NEn5bctKpt4+hdJsAE3hJuAsTv1L5vRNmeMWLQt9poAdn0KrBrPuDfEXh6m3lfI+ca8GlncXp38P+AzhPNt+9K1NZ3KANLBRhYqEH7+2Mgfq7oTBi9V/xnfWaL6LCXfkqU8WwBPPAmtE0isDvxBv45m46/z6ThXFquya7sbJXoGuyBXi280DXYA43c7OHhqIaNqoJRQAHAoAd2LwDi3xJflLaO4nRU18kVjxx6q4xzwPb3ilsGJBjnvek7C/BqfjdHpeH5dwWw5j+ARzNg+v661eeh8KbooJ11Cci6AmRdLl6+DGiLHxfmVXFnCmDQ+0D407VX39x04H+hgC5H9Mtpa8bB+dZGA4eWiDD01FaLtJQxsFgQAws1WPla0bpy8wYw7CsgdGTpc/oiFO1bDOnP/8I2PwMAkGBoi7cLn8AxKRiAGEitfaAbejb3xH3NvdCpsTvsbKv4BzLtlBg19PJe8Tikj2h+r0nfmWsngW1xpR1GFUogdDTQ52Wz9MVpEJYMF/1C+s4SnUPrE0kSv+O3CzTZyYDaWfwO3jO09uu09W3RouPVCpiaYJ5gcXkfsLB43qVJfwBBXe9+n1XAwGJBDCzUYG3/QJyv92wBRO+GpFDiZEq28RTP7vPXoSrMxlSb9Zik+g0aRSEMUOCQ+0Bk9XgFne65B64O1Ty/ry8Cdv5PDFOu14kvici3gU4T7v6/+uTDwJ//FSOjAmJE0k7jxaku10Z3t+/6LCcN+KiVOL02/UCdGg3VbPSiM7dZ+6vczs1M0cqSnykmTqzqqLmVMRhEWLl6QPS9GfaFOWpZJQwsFsTAQg1BfqEeGbk6ZOQUICNHh6wbaRgUHwlNUTZ+CHgDm1W9cCJZi/Qcncl2no5q3NfcCwMCdLj/yhdwOL1GPGFjD/SYLuZSqeoVFClHRatK8iHxuPkDwOB5oh+BOV3eJ/6DPf+neKzSAF2eBHrFAE4+5n2t+mD3V8BvLwEBnYCn/7R2bRqOktOxbk2AafvEyL41deB7YP108Q/A9P2As6/56nkHDCwWxMBCdVV2fiGSs/KRXhxCMnIKkFF8GXF6mccZOTrkFBSZbDvD5ifMsFmN04ZGGKh7DwaIPiN2tkqEh3iiZ3Mv3NfcC639nE1Hhr28D/j9NeDSLvHYyVeM4tlxbOXN2kU64O+PgL8/BAxFgJ0rMPA98V9lbfaVuPCPCC5JxZ15bR3EZZ49Z4jLYElYGCFOzUXGAd2nWrs2DYcuF/hfRyD3GvDQx0DXSTXbz81M0dE2Lx0Y8A7QY5o5a3lHDCwWxMBCdYHBIC4bPph0AweSbuBgUibOpuWgOp9oW5UCno4aNHHQ4dusJ+Eg5eHnZm8jrfGD8HRUo7GHAzo2doPG5g7n0yUJOLEe2DIbuHFBrPNtJ4b/bna/admrB0VHwGvHxOPWDwMPfSSG2rcESRItLVvfBq7sF+vcGouBwHxaW6YOcnY9Efiko+j3E3PCcj8XEnZ/Cfz2MuDsDzx7ELC1r/4+fpspOq97tQKm/GO501rFGFgsiIGF5OhGrg6HLmUaw8m/lzKRfUsrCSAm8/Ny0sDTUS3undTwdBT3Xk5qeBY/5+mkgYudjZi8r6TDn889wDM7qnZFTkWKCsSYD3+9D+RniXUtBgAPvCU6u25/F/jnE9E3wsFTDGB1z6PWuQJFksTopZtmipClcRWT2Flr9l+5+OsD8fvQtC8wfp21a9PwFBWI1pGsS8CAt8Vp1upIPS4GiZP0wLi1QLN+tVLN22FgsSAGFrK2Ir0Bp1KzcTBJBJRDSZk4n55brpyDWoXQQDfc29gN9zZ2x72N3eBV3aHo864D89qLSypHLgHaDL77N5B3XVxevHehOOWjUIn/GEsGo7vnURFWHL3u/rXuVm6GGHTs0i7RKffheUCncdaulXVIEjA/XFy+PuRz4N6x1q5Rw3TgB2D9NDHu0XP/AnZV/B6SJOC7wcCFv8Xn2JKzQJfBkW6J6rH0nAIcTMo0nt45fDkLeTp9uXJNvRyNwaRTY3e09HWqfEyTqtr5iQgrfh3E6RlzcPAQ85V0fQr4IxY4uUGEFUcf4OGPzROKzMXRU7QkrIsGjv4kviiunwfuf6PmLU11VcoREVZUGqCNmX4XqPpCRwP/zAMyzgK7vgD6zqzadsfWiLBiYyf6rtQzDCxEVnIhPRc/H7iMDYeTkVhB64mTxgYdg9zQqbj1pGOQG9wd7+KqgYrkpIkrQgAxZLe5T814NQdGLRWdXS/vFZcUO3iY9zXMwdYOGL5QDJb31/vAjo+BG4nA0C9q1oegrjpSPBR/y0jREZqsQ2Ujxr/5eRKQ8BkQ9tSdPze6XDG4IwD0fB5wb1L79bQwBhYiC8q6WYhfDyfj5wOXsf/iDZPnWvg4oVNx68m9jd3R3McJKmUt9+3Y+T+gMFfMu9NyYO29TvB94iZnCgVw/2uAR4iY6O7YGjGo2Ojl8jh1VdsMBuDIz2K5wwjr1oXEadMd/wekHgX++R/wwNzbl//7YzHonVtjMbRAPcTAQlTLivQG/H02HT/vv4zNx1OhKzIAEKPC9mrhjeGdA9GnpTdc7S3bkx/ZqcCehWK5by20rtRVHceIcWBWPgFc3iMG3xqzCvBuae2a1a6L/wDZV0Xn4+YPWLs2pFSK4QGWjxJXDnWbUvkVW9fPi1O7ABD533rbKsjAQlRLTqVk4+cDl7Hm4BWTSQBb+jpheKdADL23EXxd7jBbbG36Z56Yr6dRF6AFv6BMhPQWQ5kvfUxcQfRNhJgROKSXtWtWe46sEvdtH7nzLMZkGS0HAoFdxenUvz8SHdUrsulVMUp0037m64cmQwwsRGZ0PVeHdYeu4OcDl3H0ita43t3BFkM6NsLwToFo18hFXEpsTdpkYN8isVwbfVfqA++WYrK45aNFS8sPw8S8Mh3HWLtm5ldUUDrvUvvHrVoVKkOhEJ2/v38E2PetuMTZrbFpmTNbxNQTShsxSWM9/ixbvQv8/PnzERwcDDs7O4SHh2PPnj23LZ+ZmYno6Gj4+/tDo9GgZcuW2Lhx413tk+hu6IoM2HQ0BU99vw9h7/yBub8cx9ErWtgoFRjQ1hdfjuuM3a9GYM4j96B9oKv1wwogzo0X5QNB3coP7EalHL2ACeuBe4YBhkJg7RRg6zuo1uh8dcHZP8S4Oc7+QHBPa9eGymraR0wEaigEtr1n+lxRgRgkDgDCn6n3py2t2sKycuVKxMTEYMGCBQgPD8e8efMQGRmJU6dOwcen/PweOp0ODzzwAHx8fPDTTz+hUaNGuHjxItzc3Gq8T6KakCQJR65k4ef9l7H+36u4kVdofK59I1cM79QIj3RsBI+qXNWTnwVoXCz3n1HWFWD/t2KZrSt3ZmsPDF8EuIeIq4f+el/0GRgyv/6cOjlcfHVQu+HmmSWYzKv/bNGX6t9lYhoJrxZi/a7PgevnxHQYfap46XMdZtWB48LDw9G1a1d89tlnAACDwYCgoCBMnz4dr7xSfjrzBQsW4IMPPsDJkydha1txB8Xq7rMiHDiOKqLNL8S+C9ex+/x1bD15DWeu5Rif83bWYNi94pRPKz/nqu/05K/AjxNEn4mRSwC1Qy3U/BYbYoB93wBNegITNzCwVMeBH4ANM8RgeEHdgFHLxDgudVm+FviwhWhxe3o7ENDR2jWiiiwbJU793DMMeHwxoL0KfNpFXOU3dAHQcbS1a2hU7waO0+l02L9/P2bNmmVcp1QqERERgYSEhAq3Wb9+Pbp3747o6GisW7cO3t7eGDNmDGbOnAmVSlWjfQJAQUEBCgpKO0VqtdpKy1LDkXWzEHsTr2N3YgZ2nb+OY1ezYCgT79U2Sgxo64vhnQPRq7lX9Qdwy7sO/DJDNPWeiweWPg6MWVn1mY5rIjNJzOIKAP1mMaxUV6dxgFsQsHK8GBl3YX9g7E9ivBlzytcCqcdEv4Sgrubd961ObhBhxbMF4B9au69FNXf/6yKwHFsD9IwRVwUV5gKBYUCHkdaunUVYLbCkp6dDr9fD19d0ymtfX1+cPHmywm3Onz+PrVu3YuzYsdi4cSPOnj2LqVOnorCwELGxsTXaJwDExcVh7tw7XONO9V5mng57Eq9j13kRUo4na8t1VQj2dEB4iCe6NfPA/a197+5S5C1viFlZ3ZoAN28AF3cAS4YDY1dVfSju6vrrQxGQQnqzr0JNNe0LTNoMLHtcDC63sL9oaanJODOSJOaMSTkqRplNPSLuSyaQBICB74pLWmtLydVBHUYwwMqZXztxyu7oz8Dqp4G0EwAU4sqhBjIic526SshgMMDHxwdfffUVVCoVOnfujCtXruCDDz5AbGxsjfc7a9YsxMTEGB9rtVoEBQWZo8okYzdyddideB27zmdgd+J1nEwpH1CaejkivKkHujX1RHiIJ/xczdRn4fw24OASAArg0a/EbKo/DBP/tf8wFHhiNWDvZp7XKnE9ETi0VCz3fdW8+25ofFoDk+PFFURX9gHfDwGGfAaEjqp8m6IC4NoJMRBYyhERUlKPlE4SeStHbyA3Ddj0imhpCXvK/O8jO1X8LgLiy5Dkre+rwLG1xWEFQOcJDeoUntUCi5eXF1QqFVJTU03Wp6amws+v4sFx/P39YWtrC5WqtFNYmzZtkJKSAp1OV6N9AoBGo4FGU80J46jOycgpwO7E69htDCjZ5co083YU4aSpJ7qFeMCnNsZJ0eUBvxSPRNl1MtC4m1ie8Iv44ruyX1zGOG6teYex/+tD0fei2f1Ak+7m229D5eQj+gCt+Q9wfJ24v54I9H0FyMsoDiVHSgNK+mlx/G+ltAG8WwO+7QC/9uI/ad/24mf/xxwxXs7GF0W5LlHmfQ/H1gCSQYzF49nMvPsm8/NqLi6rP/gDYOcG3D/b2jWyKKsFFrVajc6dOyM+Ph5Dhw4FIFpQ4uPjMW3atAq3ue+++7Bs2TIYDAYoi5vATp8+DX9/f6jV4mqM6u6T6rfLN/Kw8Ugyfj2cjH8vl/9PtqWvE8JDPBHe1ANhIR7wcbbAVR9/viOa/F0aARFlWgb9Q4GJvwLfPQIk/ytmXR2/zjzDwmecA/5dLpbZumI+tvbAY4uB+LkiWGx/F9jzpTjFVxE7t+JQUnzzbQd4twJsKvmHKWKOCDkJn4nOvkob884kXTJ3EMdeqTv6x4o+R+1H1P0O39VU5cBy9epVBAQEmPXFY2JiMGHCBHTp0gVhYWGYN28ecnNzERUl/osYP348GjVqhLi4OADAlClT8Nlnn+G5557D9OnTcebMGfz3v//Fs88+W+V9Uv13NfMmNh5JxobDyTh0KdPkudZ+zggPEad4wkI84Olk4Za1KwfEpYgA8PD/AZpbrijyvUeElu8fEf+ZL34IGL8ecPYtv6/q+OsDQNIDLQbUfifOhkapFPO8eISIK7BKwopH0+JWkw6i1cSvvQip1eknolAAA94WoWX3AmD9dBFazHFFSMY50ZqnUALtHr37/ZFlOHmLiToboCoHlnvuuQfz58/HmDHmG+Vx5MiRSEtLw+zZs5GSkoKOHTti06ZNxk6zSUlJxpYUAAgKCsLvv/+O559/Hh06dECjRo3w3HPPYebMmVXeJ9VPKVn5oiXlSLLJpIIKBRAe4oGHOwQg8h4/eDtb8dSfvlB84UgGoN1jYkbcivi0BiZuFC0saSdFaJmwHnCp4T8M6WeAwyvFct9Zty9LNdd5ItCsP5CdDPi0KR9Ga0qhEB1v9YXicvR1U0Vo6XCXrSJHfhL3TfuK01tEMlflcVg+//xzzJw5EwMHDsSXX34JDw8ZThFvJhyHpW64ps3Hb0dT8OvhZOy9eN3YYVahALoGe+DhDv4Y2M7PMqd5quKvD4GtbwH2HsC0vXc+1XP9vDg9lHVJDFo24RdxSW11/TxZXAnSchAwZkXN6k7WZzAAG54Tl6UrlMBji8SYHDUhScBnXYGMM7Ibw4Pqvtr6Dq3WwHGJiYmYNGkSjh8/jq+//hqDBw82W0XkhIFFvtKyC7DpqDjds+fCdZOrero0ccdDHfwxqJ2/+a7mMZf0M8AX9wH6AmDYV0BoFcdNuHFRtLRkXhRziEzYALg3qfrrXjsJfN4NgAT85y+Os1HXGQzA+mniai+FChjxHdCmBn+Hrx4EvuoL2NgBL56pvcvoqUGSxcBxISEh2Lp1Kz777DM8+uijaNOmDWxsTHdx4MABs1WOCBBX92w6JlpSdp3PMBm87d7Gbni4QwAebO8Hf1eZTqluMADrnxVhpVl/Md5FVbk3AaI2ipaW6+eAbx8Up4eqekXH9ncBSGIGV4aVuk+pBB75VPRpObwSWBUFjPwBaDWoevspOR3UahDDCtUZ1b5K6OLFi1i9ejXc3d0xZMiQcoGF6qDr5wHXxoBKPj9LXZEBGw5fxeoDV5BwPgP6MiklNMgND7f3x6D2fgh0t8BQ9ndr/7dA0k7A1hEYPK/6g3O5BpZ2xE0/Xdyn5ZfS+UQqk3pMXLYKsO9KfaJUAUM+F6Hl6M/Aj+PFwHUtHqja9gZ9aWDh1UFUh1TrG+rrr7/GCy+8gIiICBw7dgze3t61VS+ylD/mignduk8DIt+xdm2QnV+I5XuSsGjHBaRo843r2zdyxUMd/PFQe38EedSBkFJCexXYUnzpcv/Z5aeGryoX/9JLntNOFLe0/CI66FZm27vivu1QcZUK1R8qG3Fq0VAkxoBZMRYYvRxo3v/O217YAeSkAHauQPOI2q8rkZlUObAMHDgQe/bswWeffYbx48fXZp3IUhI+F2EFAPYvFgNemevKhmq6ps3Hon8uYOmui8guEINr+ThrMK5bEwwODUCwl6NV6nVXJAn49QVAlw0Edr37kUpLBir7fqgYIXXxQ2KclorCSPJh4MR6AArxc6X6R2UDDP9GtJic3ACsGAOM+RFo2uf225WMvdJ2aOXjvxDJUJUDi16vx+HDhxEYGFib9SFLOfIT8HvxaQJbB0CXI6aY7zrJotU4ey0HX/91HmsOXoFObwAANPdxwtO9mmLIvQHQ2NThqe6PrQFObQSUtqLfgdIM78XRS/Rh+WEYkHwI+O5hMSLurcNzl7SutBsuLrGl+kllCzz2LfDjOOD0JmD5KDEZY2XzGhXmA8d/Ecs8HUR1TLWuEmoo6v1VQue2AktHiEnwwp8Rpyl+f1UMB/7M3xaZAG3fhetYsP08/jhROo1C12B3/Kd3M9zf2gdKZR2fhC3vOjA/TMwF0+cVMTOyOd3MBJY+BlzeK5r2n1gDBHYWz5VcAaJQAlN3A94tzfvaJD9FBaKF5ewfoq/UuNWlUz6UdXy9CDfOAcDzxxrMpHlkWbX1Hcrf1obm6kFg5TgRVu55FIiMA0JHi8sbU4+IL8BaYjBI+P1YCoZ/sROPLUjAHydSxUCebX3x85TuWPVMD0S09a37YQUANr8uwop3a6BXzJ3LV5e9m5ggsXF3MXne90OApN3iuT/FyNBo/zjDSkNhowFGLhGDwBXmAkseAy5V8FkumZm5/XCGFapz+BvbkGScE3/IdDlASB9g2ALxR8vBo3Sm1r3fmP1l8wv1WLEnCRH/tx3/+WE/9l+8AbVKidFhQfgjpg++Gt8FnZvUo4EIz20tnhVZIU4F1VY/ATuX4ub/XqKfzA/DgJ2fAmd+F2N09Jl5531Q/WFrD4xaXvr7sORRMRVEifws4PTvYrl9NS6tJ5IJBpaGIjtVfKHlpYu5TUYuMf0i7fKkuD+2RpzOMIOsm4X4fNtZ9Hr/T7yy+gjOp+XC2c4GU/s2w46Z/RD3aAc083Yyy2vJhi4X+GWGWA57GggKq93X0zgVd7TsJ/6z3vy6WB86irPvNkRqB2DMSqBxD6BAC/wwVEykCQAnfhFjAXm1EvMaEdUx8hl4g2pPvhZYOlyMluoeAjzxc/nBohp1FkEm5bBoHegxvcYvdzXzJhbtSMTyPUnI1ekBAP6udpjUMwSjwhrDSVOPf+3+/K84zq5B4jJmS1A7AKNXiL4JZzaL1pXeL1nmtUl+1I7A2B+BJcOBS7vF6cIJG0SnekDMQWSBfmpE5laPvzkIQGlnvJQjgKO36IxX0URnCoVoZdkwA9j3LdAtutrnuI9czsK3/yRi/b9XUVQ80FsrX2f8p09TPNwhAGqbet6gd2X/LTMxW7D1yNZOtJr99QHg2ULMHEwNl8ZZnC78Yaj4vfxucOks0u0es2rViGqKgaU+M+iB1U8DF/4G1E7iD5hH08rLt38c2PyGGAI+cTvQrN8dX6JQb8Dvx1Kw+J8L2FdmluRuTT3wnz7N0LelNxQN4b85fSGwrngm5vYjqj7qqDnZaID7X7f865I82bmIjtnfDxGXwANAYBjDLNVZDCz1lSQBv80Ejq8V44CMWlp+rI5baZzEpHx7FwL7Ft02sGTkFGD5niQs2ZVkHJHWRqnAQx38EXVfCDoGuZntrdQJ/8wDrh0DHDyBge9auzZEgr0bMG6NmNYh5Qhw71hr14ioxhhY6qu/PwT2fg1AATz6pbjcsSq6PCkCy8lfAW2yGBK+jKNXsrB45wWs//cqdEVioDcvJzXGhjfB2PDG8HGR2SzJlpB2Gtj+vlge+B7g6Gnd+hCV5eABPPm7ODXUpKe1a0NUYwws9dH+74Ctb4vlQe+VXrJcFb73AEHdgEu7gIM/AH1eNp72+W7nBey9UHrap0OgK6LuC8aD7f3r9oi0d8NgAH55FtDrgBYDgPbsH0AypHYEQnpbuxZEd4WBpb45uVF0nAWAnjFA+H+qv4+uk4BLu6Df9y2+1D+C73ddMTnt82B7f0y8Lxj3Brk1jP4pt7N/EZCUIPoIPfQxr74gIqolDCz1SdIu4Kco0fHz3idqfFntMdc+CFa5wjH7Kg788SNSDJ3h5aTGmOLTPr4N8bRPRbKuAFvmiOX+sYBbkFWrQ0RUnzGw1BfXTgDLRgBF+UDLgcDD/6vWf/tFegM2H0/F4n8uYM+F63jFpheesdmAKY7b8OCgJ/FQhzp82ifvOrDvGzGAlksjMXeSW2PArYm4t3er/j4lCfg1pngm5jCg62SzV5uIiEoxsNQHmZeAHx4VQ28HhonZW1VV+9Hm6YqweOcF/JBwEclZpad9UluMAhI3oFPhAXQOLgTqYljJOCfGRTm4FCi6WXk5jSvgXibAuN2yfOsgewBwbLWYHVelLp6JuZ6PMUNEZGUMLHVd3nUxomX2VTHk9piVYuTTKth/8QZe+PEQLmTkAQA8HdUYG94YY7s1Ead9frgfinNbgf2LgQfm1uKbMLNLe4CdnwAnNgAonozcP1QMmJWXAWQmidFoM5PEBIUFWeKSz5QjFe/Pzg1wb1IaZFyDxABtANDrRcCntSXeFRFRg8bAUpfp8sRpoPRT4lTHuNXiEsY7bVZkwLw/TmPB9nMwSICfix1ejGyFhzv4w862TEtKlyfFRH4HlwD9Xq29SfzMwaAXl2InfCaGIy/RIlJMMxDcs+JTZLpc0UJVNsQY75NEwMnPBJIzS+dkKeHTFuj5fG2+KyIiKsbAUlfpC4FVE4HLe0ULwBM/A66Bd9zsRLIWMT/+ixPJWgDAsHsbYc7ge+DqYFu+cMtBgLM/kJ0sJk6T4yW7ujwx91HCfOBGolinUgMdRgLdp9259UPtKMpUVq4gu0ygKQkzF8X8TJH/BWzU5n0/RERUIQaWukiSgF+eA878DtjYidl6fdrcdhO9QcKXf53D/205jUK9BHcHW/x3WHsMau9f+UYqG6DTBGD7u2LkWzkFlpxrwJ6vxSB3N4tnl7Z3B7pMErMkO/ua53U0zoBvW3EjIiKrYWCpS3R5wMkNwIHvxfxAChXw+GKgcfhtN7uQnosXVv2L/cVz/US08UXco+3h7VyFUzydJ4j+Ghf/EVci3SEY1bq0U+K0z78rAX2BWOceLFpTOo4RLSZERFTvMLDInSSJ8VUOLQWOrRWX0QKAQgk88gnQatBtNpWwZNdF/HfjSdws1MNJY4PYwW3xWOfAqg/45hIgXuPkBjGL84Pv3/17qi5JEoFp56fiypwSjboA9z0LtH4YUNbBq5iIiKjKGFjkKvMS8O8K4N9lwPXzpevdmoiWhNBRomWhEslZN/HyT4fx95l0AED3pp744PEOCHSv2hVEJro8KQLLv8uBiFjLtWLoi8TkjTs/LZ1tFgqg9UOiI21QOEeWJSJqIBhY5ESXKzq3HloGJP4F4yW5to7APcNEUGnc/bZjfkiShHWHruKNdUeRnV8EjY0SMwe2xsQewVAqa/jl3rQf4B4iOrUe/RnoNL5m+6mOmzeAxYOB1OJLjW3sgI5jge7RgGez2n99IiKSFQYWa5MkMReN8ZRPTulzIb2B0DFAm8GAxumOu8rIKcDra4/it6MpAIDQQFd8NKIjmvvcedvbUiqBLlHAltnA3m9qP7CUXAGVekR0pA2fIuY3cvSq3dclIiLZYmCxlhsXS0/53LhQut49WLQkdBgpBiuroi3HUzFr9WGk5+hgo1Tg2f4tMLVvM9iozDQCa8cnxAzQyYeAKweARp3Ms99bSRLw20zg/DbRsjR+PeDfoXZei4iI6gwGFkvS5QLH14vWlAt/l65XOwP3DC095VONfhnZ+YV485fjWLX/MgCghY8T/m9kR7Rr5Greujt6Am2HAkd+FPPy1FZg2fOV2D8UwPCvGVaIiAgAA4tlFOSIVoPja8uc8lEATfsUn/J5uEYdWXeeS8dLqw7jSuZNKBTAU72aIuaBlqaj1ZpT10kisBz5GRjwTs0mDbydM38Am14Ryw/MFZ1riYiIAFh9xrb58+cjODgYdnZ2CA8Px549eyotu3jxYigUCpObnZ2dSZmcnBxMmzYNgYGBsLe3R9u2bbFgwYLafhu3p3YEknaKsOLRFLj/dWDGEWD8OiB0ZLXDSn6hHnN/OYYxX+/GlcybCPKwx4qnuuHVB9vUXlgBxFU5Pm3FRIL/rjDvvq+dEP1WJIM4/dTjWfPun4iI6jSrtrCsXLkSMTExWLBgAcLDwzFv3jxERkbi1KlT8PHxqXAbFxcXnDp1yvj41vFEYmJisHXrVixZsgTBwcHYvHkzpk6dioCAADzyyCO1+n4qpVAAA98Ts/7e5aW4+YV6RH27FwnnMwAAo8Ma47WH2sBJY4EfpUIhLnHe+KIY+Tb8P+a5rDg3HVg2Uowx0+Q+4OH/4+XKRERkwqotLB9//DGeeuopREVFGVtCHBwcsGjRokq3USgU8PPzM958fU2HYN+5cycmTJiAvn37Ijg4GE8//TRCQ0Nv23JjES0HAI273dUXcZHegGnLDiLhfAacNDb4dmJXxD3a3jJhpUSHkaIzbPopMZjb3SoqAFaMFfPzuAcDI37g/DxERFSO1QKLTqfD/v37ERERUVoZpRIRERFISEiodLucnBw0adIEQUFBGDJkCI4dO2byfI8ePbB+/XpcuXIFkiThzz//xOnTpzFgwIBK91lQUACtVmtykxuDQcLLPx/GHydSobZRYuGELujXuuJWqFpl5wJ0eFws7/3m7vZVMifSpV2AxlXMieToefd1JCKiesdqgSU9PR16vb5cC4mvry9SUlIq3KZVq1ZYtGgR1q1bhyVLlsBgMKBHjx64fPmyscynn36Ktm3bIjAwEGq1GgMHDsT8+fPRu3fvSusSFxcHV1dX4y0oKMg8b9JMJEnCW78ex+oDV6BSKvD5mE7o1tSKX+xdnhT3J34RkxDW1I7/E6PnKlTAiMWAdyuzVI+IiOofq3e6rY7u3btj/Pjx6NixI/r06YPVq1fD29sbX375pbHMp59+il27dmH9+vXYv38/PvroI0RHR+OPP/6odL+zZs1CVlaW8Xbp0iVLvJ0q+3TrWXz7zwUAwIePd0BEWzPNRFxT/qFiHh9DIXDwh5rt4/h6IH6uWB70HtDsfvPVj4iI6h2rdbr18vKCSqVCamqqyfrU1FT4+flVaR+2tra49957cfbsWQDAzZs38eqrr2LNmjV46CFxSWyHDh1w6NAhfPjhhyann8rSaDTQaKowc7EVfLfzAj7echoAEDu4LYbdG2jlGhXr8iRwZR+wfzFw34zqTT549RCw5j9iOexpIOypWqggERHVJ1ZrYVGr1ejcuTPi4+ON6wwGA+Lj49G9e/cq7UOv1+PIkSPw9/cHABQWFqKwsBDKW+baUalUMBgM5qu8haw9eAWx60UfnRkRLRB1X4iVa1RGu0cBO1cgMwk4G3/n8iW0ycDyUUBhHtCsPxAZV3t1JCKiesOqp4RiYmLw9ddf47vvvsOJEycwZcoU5ObmIioqCgAwfvx4zJo1y1j+zTffxObNm3H+/HkcOHAATzzxBC5evIjJkycDEJc89+nTBy+99BK2bduGxMRELF68GN9//z2GDRtmlfdYU38cT8ULq/4FAEzsEYzn+rewco1uYWsvphAAxCXOVaHLE2ElOxnwbg08/i2g4tiFRER0Z1b9thg5ciTS0tIwe/ZspKSkoGPHjti0aZOxI25SUpJJa8mNGzfw1FNPISUlBe7u7ujcuTN27tyJtm3bGsusWLECs2bNwtixY3H9+nU0adIE77zzDp555hmLv7+a2nU+A9HLDkBvkPDovY0w++G25cabkYUuTwK7PgfO/A5kXgLcbtNZ2WAA1j4j5iJy8ARGrxAtNERERFWgkCRJsnYl5Ear1cLV1RVZWVlwcXGx6GsfvZKFUV/tQk5BESLa+OKLJzrB1lwTGNaG7wYDiX8BvV8SI/hWJv4t4O8PAaUtMGE90KSH5epIREQWU1vfoTL+Jmx4zqXlYPyiPcgpKEJ4iAc+G3OvvMMKUHqJ84HvAX1hxWX+XSnCCgA88gnDChERVZvMvw0bjquZNzFu4W5cz9WhXSMXLJzQpXbnBTKX1g8DTr5ATipw8tfyzyftBtZPE8s9nxczUhMREVUTA4sMZOQU4IlvduNqVj6aejviu6gwONvZWrtaVaOyBe4dJ5b33TLy7Y2LwIoxgF4ngs39sy1fPyIiqhcYWKwsO78QE77dg/NpuQhwtcOSSeHwdJLnmDCV6jwRUChFX5b0M2JdvlZcEZSXDvh1AB79ClDy142IiGqG3yBWlF+ox+Tv9uHoFS08HdX4YXI4AtzsrV2t6nMLAloUz9W071vAoAd+ngRcOw44+YkrgtSO1q0jERHVaRwEw0oK9QZMW3YAuxOvw1ljg++eDEMzbydrV6vmukwCTm8CDi0F9AXAmc2AjR0wehng2sjatSMiojqOLSxWYDBIePmnw/jjxDVoimdebteojo9J0rw/4NoYyM8E9i4U64YtABp1tmq1iIiofmBgsTBJkvDmhuNYc7B45uWxnRBuzZmXzUWpArpMLH3c73Xgnro1ujAREckXTwlZ2P/iz2DxzgsAgI8eD0X/NlaeedmcOkcBpzaJVpXeL1q7NkREVI8wsFjQt/8kYt4f4iqauY/cg6H31rO+HQ4ewOQt1q4FERHVQzwlZCGrD1zG3F+OAwBiHmiJCT2CrVshIiKiOoSBxQKuafMxa/URAMCT94Vg+v3NrVwjIiKiuoWnhCzAx8UO88d0wtZT1/D6Q23kOfMyERGRjDGwWEhEW19EtK1HHWyJiIgsiKeEiIiISPYYWIiIiEj2eEqoApIkAQC0Wq2Va0JERFS3lHx3lnyXmgsDSwWys7MBAEFBQVauCRERUd2UnZ0NV1fzTTujkMwdgeoBg8GAq1evwtnZ2WxX9Gi1WgQFBeHSpUtwcXExyz6p6nj8rYvH37p4/K2roR1/SZKQnZ2NgIAAKJXm63nCFpYKKJVKBAYG1sq+XVxcGsQvrFzx+FsXj7918fhbV0M6/uZsWSnBTrdEREQkewwsREREJHsMLBai0WgQGxsLjUZj7ao0SDz+1sXjb108/tbF428e7HRLREREsscWFiIiIpI9BhYiIiKSPQYWIiIikj0GFiIiIpI9BhYLmT9/PoKDg2FnZ4fw8HDs2bPH2lVqEObMmQOFQmFya926tbWrVW/99ddfGDx4MAICAqBQKLB27VqT5yVJwuzZs+Hv7w97e3tERETgzJkz1qlsPXSn4z9x4sRyn4eBAwdap7L1UFxcHLp27QpnZ2f4+Phg6NChOHXqlEmZ/Px8REdHw9PTE05OThg+fDhSU1OtVOO6hYHFAlauXImYmBjExsbiwIEDCA0NRWRkJK5du2btqjUI99xzD5KTk423HTt2WLtK9VZubi5CQ0Mxf/78Cp9///338cknn2DBggXYvXs3HB0dERkZifz8fAvXtH660/EHgIEDB5p8HpYvX27BGtZv27dvR3R0NHbt2oUtW7agsLAQAwYMQG5urrHM888/j19++QWrVq3C9u3bcfXqVTz66KNWrHUdIlGtCwsLk6Kjo42P9Xq9FBAQIMXFxVmxVg1DbGysFBoaau1qNEgApDVr1hgfGwwGyc/PT/rggw+M6zIzMyWNRiMtX77cCjWs3249/pIkSRMmTJCGDBlilfo0RNeuXZMASNu3b5ckSfy+29raSqtWrTKWOXHihARASkhIsFY16wy2sNQynU6H/fv3IyIiwrhOqVQiIiICCQkJVqxZw3HmzBkEBASgadOmGDt2LJKSkqxdpQYpMTERKSkpJp8FV1dXhIeH87NgQdu2bYOPjw9atWqFKVOmICMjw9pVqreysrIAAB4eHgCA/fv3o7Cw0OQz0Lp1azRu3JifgSpgYKll6enp0Ov18PX1NVnv6+uLlJQUK9Wq4QgPD8fixYuxadMmfPHFF0hMTESvXr2QnZ1t7ao1OCW/7/wsWM/AgQPx/fffIz4+Hu+99x62b9+OQYMGQa/XW7tq9Y7BYMCMGTNw3333oV27dgDEZ0CtVsPNzc2kLD8DVcPZmqleGzRokHG5Q4cOCA8PR5MmTfDjjz9i0qRJVqwZkeWNGjXKuNy+fXt06NABzZo1w7Zt29C/f38r1qz+iY6OxtGjR9lnzozYwlLLvLy8oFKpyvUCT01NhZ+fn5Vq1XC5ubmhZcuWOHv2rLWr0uCU/L7zsyAfTZs2hZeXFz8PZjZt2jRs2LABf/75JwIDA43r/fz8oNPpkJmZaVKen4GqYWCpZWq1Gp07d0Z8fLxxncFgQHx8PLp3727FmjVMOTk5OHfuHPz9/a1dlQYnJCQEfn5+Jp8FrVaL3bt387NgJZcvX0ZGRgY/D2YiSRKmTZuGNWvWYOvWrQgJCTF5vnPnzrC1tTX5DJw6dQpJSUn8DFQBTwlZQExMDCZMmIAuXbogLCwM8+bNQ25uLqKioqxdtXrvxRdfxODBg9GkSRNcvXoVsbGxUKlUGD16tLWrVi/l5OSY/LeemJiIQ4cOwcPDA40bN8aMGTPw9ttvo0WLFggJCcEbb7yBgIAADB061HqVrkdud/w9PDwwd+5cDB8+HH5+fjh37hxefvllNG/eHJGRkVasdf0RHR2NZcuWYd26dXB2djb2S3F1dYW9vT1cXV0xadIkxMTEwMPDAy4uLpg+fTq6d++Obt26Wbn2dYC1L1NqKD799FOpcePGklqtlsLCwqRdu3ZZu0oNwsiRIyV/f39JrVZLjRo1kkaOHCmdPXvW2tWqt/78808JQLnbhAkTJEkSlza/8cYbkq+vr6TRaKT+/ftLp06dsm6l65HbHf+8vDxpwIABkre3t2Rrays1adJEeuqpp6SUlBRrV7veqOjYA5C+/fZbY5mbN29KU6dOldzd3SUHBwdp2LBhUnJysvUqXYcoJEmSLB+TiIiIiKqOfViIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIhI9vR6PXr06IFHH33UZH1WVhaCgoLw2muvWalmRGQpHOmWiOqE06dPo2PHjvj6668xduxYAMD48ePx77//Yu/evVCr1VauIRHVJgYWIqozPvnkE8yZMwfHjh3Dnj178Pjjj2Pv3r0IDQ21dtWIqJYxsBBRnSFJEu6//36oVCocOXIE06dPx+uvv27tahGRBTCwEFGdcvLkSbRp0wbt27fHgQMHYGNjY+0qEZEFsNMtEdUpixYtgoODAxITE3H58mVrV4eILIQtLERUZ+zcuRN9+vTB5s2b8fbbbwMA/vjjDygUCivXjIhqG1tYiKhOyMvLw8SJEzFlyhT069cP33zzDfbs2YMFCxZYu2pEZAFsYSGiOuG5557Dxo0b8e+//8LBwQEA8OWXX+LFF1/EkSNHEBwcbN0KElGtYmAhItnbvn07+vfvj23btqFnz54mz0VGRqKoqIinhojqOQYWIiIikj32YSEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItn7f+rAiX6OhbcsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 600x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2928005456924438, 'auc': 0.6218370795249939}\n",
      "============val_data===================\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "     Android_Adware     0.5996    0.1766    0.2729     13842\n",
      "Android_SMS_Malware     0.2155    0.3785    0.2746      6109\n",
      "  Android_Scareware     0.3733    0.3445    0.3583     10790\n",
      "             Benign     0.1336    0.4998    0.2109      2181\n",
      "\n",
      "           accuracy                         0.2905     32922\n",
      "          macro avg     0.3305    0.3498    0.2792     32922\n",
      "       weighted avg     0.4233    0.2905    0.2971     32922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = dt.evaluate(x_test,y_test)\n",
    "print(result)\n",
    "\n",
    "#scoring\n",
    "preds = dt.predict(x_test)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(6, 6))\n",
    "axes[0].plot(history.history['loss'], label='train_loss')\n",
    "axes[0].plot(history.history['val_loss'], label='val_loss')\n",
    "axes[0].set_title('Loss')\n",
    "# axes[1].plot(history.history['accuracy'], label='train_accuracy')\n",
    "# axes[1].plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "# axes[1].set_title('Accuracy')\n",
    "axes[2].plot(history.history['AUC'], label='train_AUC')\n",
    "axes[2].plot(history.history['val_AUC'], label='val_AUC')\n",
    "axes[2].set_title('AUC')\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=0.5)\n",
    "fig.suptitle('Loss and Accuracy')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()\n",
    "\n",
    "print(result)\n",
    "print('============val_data===================')\n",
    "print(classification_report(y_test, preds, target_names=names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1df8247",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(\n",
    "    nets=['dnn_nets','autoint_nets','fm_nets'],\n",
    "    categorical_columns=categorical_columns,\n",
    "    auto_discrete=True,\n",
    "    auto_categorize=True,\n",
    "    metrics=['AUC'],\n",
    "    dnn_params={\n",
    "        'hidden_units': ((300, 0.3, True), (300, 0.3, True)),\n",
    "        'use_bn': True,\n",
    "        'learning_rate': 0.01,\n",
    "        'activation': 'relu',\n",
    "    },\n",
    "    earlystopping_patience=5,\n",
    "    apply_gbm_features=True,\n",
    "    embeddings_output_dim=100,\n",
    "    autoint_params={\n",
    "        'num_attention': 3,\n",
    "        'num_heads': 1,\n",
    "        'dropout_rate': 0.2,\n",
    "        'use_residual': True\n",
    "    },\n",
    "    stacking_op='add',\n",
    ")\n",
    "\n",
    "dt = deeptable.DeepTable(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3bb3be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-20 12:54:28 I deeptables.m.deeptable.py 338 - X.Shape=(264440, 80), y.Shape=(264440,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fm_nets', 'dnn_nets', 'autoint_nets'], categorical_columns=['Source Port', 'Destination Port', 'Protocol', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC'], auto_categorize=True, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=True, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((300, 0.3, True), (300, 0.3, True)), 'use_bn': True, 'learning_rate': 0.01, 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0.2, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-20 12:54:28 I deeptables.m.deeptable.py 339 - metrics:['AUC']\n",
      "04-20 12:54:29 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-20 12:54:29 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-20 12:54:29 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.3381049633026123s\n",
      "04-20 12:54:29 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-20 12:54:30 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5744738578796387s\n",
      "04-20 12:54:30 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-20 12:54:30 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.17353558540344238s\n",
      "04-20 12:54:30 I deeptables.m.preprocessor.py 398 - Data discretization...\n",
      "04-20 12:54:30 I hypernets.t.sklearn_ex.py 716 - 66 variables to discrete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-20 12:54:32 I deeptables.m.preprocessor.py 404 - Discretization taken 2.0026743412017822s\n",
      "04-20 12:54:32 I deeptables.m.preprocessor.py 423 - Extracting GBM features...\n",
      "04-20 12:54:32 I hypernets.t.sklearn_ex.py 640 - LightGBM task:multiclass\n",
      "04-20 12:54:36 I deeptables.m.preprocessor.py 434 - Extracting gbm features taken 3.859729290008545s\n",
      "04-20 12:54:40 I deeptables.m.preprocessor.py 196 - fit_transform taken 11.620600700378418s\n",
      "04-20 12:54:40 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-20 12:54:41 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5854346752166748s\n",
      "04-20 12:54:41 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-20 12:54:41 I deeptables.m.preprocessor.py 236 - transform_y taken 0.002992391586303711s\n",
      "04-20 12:54:42 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-20 12:54:42 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-20 12:54:42 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-20 12:54:42 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-20 12:54:42 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-20 12:54:43 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (174)', 'input_continuous_all: (66)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [48234, 16794, 5, 4, 9, 10, 23, 5, 5, 11, 13, 8, 5, 14, 15, 8, 6, 14, 15, 21, 23, 23, 20, 23, 20, 22, 22, 20, 22, 19, 18, 19, 18, 18, 13, 7, 6, 23, 22, 5, 8, 16, 16, 16, 3, 3, 3, 3, 3, 4, 15, 13, 14, 7, 5, 11, 5, 13, 10, 9, 4, 4, 13, 7, 13, 13, 14, 8, 14, 14, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 17466)\n",
      "---------------------------------------------------------\n",
      "nets: ['fm_nets', 'dnn_nets', 'autoint_nets']\n",
      "---------------------------------------------------------\n",
      "fm: input_shape (None, 174, 100), output_shape (None, 1)\n",
      "dnn: input_shape (None, 17466), output_shape (None, 300)\n",
      "autoint: input_shape (None, 174, 100), output_shape (None, 17400)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-20 12:54:43 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "258/258 [==============================] - 77s 213ms/step - loss: 2.8422 - auc: 0.6195 - val_loss: 1.9557 - val_auc: 0.6703\n",
      "Epoch 2/100\n",
      "258/258 [==============================] - 54s 209ms/step - loss: 2.1465 - auc: 0.7154 - val_loss: 1.1429 - val_auc: 0.7732\n",
      "Epoch 3/100\n",
      "258/258 [==============================] - 54s 208ms/step - loss: 1.8927 - auc: 0.7731 - val_loss: 1.1899 - val_auc: 0.7425\n",
      "Epoch 4/100\n",
      "258/258 [==============================] - 54s 209ms/step - loss: 1.7626 - auc: 0.7991 - val_loss: 1.3150 - val_auc: 0.7120\n",
      "Epoch 5/100\n",
      "258/258 [==============================] - 54s 209ms/step - loss: 1.6637 - auc: 0.8177 - val_loss: 1.3461 - val_auc: 0.7220\n",
      "Epoch 6/100\n",
      "258/258 [==============================] - 54s 209ms/step - loss: 1.5895 - auc: 0.8318 - val_loss: 1.4588 - val_auc: 0.7144\n",
      "Epoch 7/100\n",
      "258/258 [==============================] - 54s 209ms/step - loss: 1.5277 - auc: 0.8448 - val_loss: 1.4461 - val_auc: 0.7495\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "04-20 13:01:24 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-20 13:01:24 I deeptables.m.deeptable.py 370 - Training finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-20 13:01:25 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20230420125428_fm_nets_dnn_nets_autoint_nets/fm_nets+dnn_nets+autoint_nets.h5\n"
     ]
    }
   ],
   "source": [
    "model, history = dt.fit(x_train, y_train, epochs=epochs, validation_data=(x_val, y_val),class_weight=class_weight,\n",
    "                        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0688fad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-20 13:01:25 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-20 13:01:26 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5659923553466797s\n",
      "04-20 13:01:26 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-20 13:01:26 I deeptables.m.preprocessor.py 236 - transform_y taken 0.001995086669921875s\n",
      "04-20 13:01:26 I deeptables.m.deepmodel.py 158 - Performing evaluation...\n",
      "04-20 13:01:26 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=256, shuffle=False, drop_remainder=False\n",
      "{'loss': 1.1358505487442017, 'auc': 0.7748055458068848}\n",
      "04-20 13:01:31 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "04-20 13:01:31 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-20 13:01:31 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5734765529632568s\n",
      "04-20 13:01:31 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "04-20 13:01:31 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "04-20 13:01:38 I deeptables.m.deeptable.py 559 - predict_proba taken 7.019326448440552s\n",
      "04-20 13:01:38 I deeptables.m.deeptable.py 594 - Reverse indicators to labels.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAJJCAYAAAB4RTeNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB7kElEQVR4nO3deVhUZf8G8HtmYIZ93wXc11RcEHLXJNHM1MolS82y3lxKpdVK0TZssZ8tpqmZ9aZplpqZGy+lZmooSO4LioKyKAgM6wAz5/fHgYGRQUFnOAPcn+uaizlnnjnzPcdlbp7znOfIBEEQQERERCQRudQFEBERUdPGMEJERESSYhghIiIiSTGMEBERkaQYRoiIiEhSDCNEREQkKYYRIiIikhTDCBEREUmKYYSIiIgkxTBCRGYhk8mwcOFCqcsgogaAYYTIBNauXQuZTIajR49KXUqDdebMGchkMtjY2CAnJ0fqcoioHjGMEJFF+OGHH+Dj4wMA+PnnnyWuhojqE8MIEUlOEASsX78eEydOxEMPPYR169ZJXVKNCgoKpC6BqNFhGCGqR8eOHcPw4cPh5OQEBwcHDBkyBIcPHzZoU1paikWLFqFt27awsbGBu7s7+vXrh+joaH2b9PR0TJ06Ff7+/lCpVPD19cWoUaNw+fLl237+8ePH8fTTT6NVq1awsbGBj48PnnnmGWRlZRm0W7hwIWQyGRITE/H000/DxcUFzs7OmDp1KgoLCw3aajQazJ07F56ennB0dMQjjzyCq1ev1um4/P3337h8+TImTJiACRMmYP/+/Ua3odPp8Nlnn6FLly6wsbGBp6cnhg0bVu302A8//ICQkBDY2dnB1dUVAwYMwJ49e/Sv1zSepUWLFnj66af1yxWn3/bt24cZM2bAy8sL/v7+AIArV65gxowZaN++PWxtbeHu7o6xY8ca/TPIycnB3Llz0aJFC6hUKvj7+2Py5MnIzMxEfn4+7O3tMXv27Grvu3r1KhQKBaKiomp5JIkaJiupCyBqKk6dOoX+/fvDyckJr732GqytrfH1119j0KBB2LdvH0JDQwGIQSAqKgrTpk1DSEgI1Go1jh49ivj4eDz44IMAgMceewynTp3Ciy++iBYtWuD69euIjo5GcnIyWrRoUWMN0dHRuHTpEqZOnQofHx+cOnUKK1euxKlTp3D48GHIZDKD9uPGjUPLli0RFRWF+Ph4rF69Gl5eXvjwww/1baZNm4YffvgBEydORJ8+ffDHH39gxIgRdTo269atQ+vWrdGrVy907twZdnZ2+PHHH/Hqq68atHv22Wexdu1aDB8+HNOmTUNZWRn++usvHD58GMHBwQCARYsWYeHChejTpw/eeecdKJVK/PPPP/jjjz8wdOjQOtVVYcaMGfD09MSCBQv0PSNHjhzBwYMHMWHCBPj7++Py5ctYvnw5Bg0ahNOnT8POzg4AkJ+fj/79++PMmTN45pln0KNHD2RmZmLbtm24evUqunXrhjFjxmDjxo349NNPoVAo9J/7448/QhAEPPnkk3dVN1GDIRDRPfv2228FAMKRI0dqbDN69GhBqVQKFy9e1K9LTU0VHB0dhQEDBujXBQUFCSNGjKhxO9nZ2QIA4eOPP65znYWFhdXW/fjjjwIAYf/+/fp1kZGRAgDhmWeeMWg7ZswYwd3dXb+ckJAgABBmzJhh0G7ixIkCACEyMvKONZWUlAju7u7CW2+9ZfD+oKAgg3Z//PGHAEB46aWXqm1Dp9MJgiAIFy5cEORyuTBmzBhBq9UabSMIQo21NW/eXJgyZYp+ueLPtV+/fkJZWZlBW2PH8tChQwIA4fvvv9evW7BggQBA2Lx5c4117969WwAg7Ny50+D1rl27CgMHDqz2PqLGhqdpiOqBVqvFnj17MHr0aLRq1Uq/3tfXFxMnTsSBAwegVqsBAC4uLjh16hQuXLhgdFu2trZQKpXYu3cvsrOz61SHra2t/nlxcTEyMzNx//33AwDi4+OrtX/hhRcMlvv374+srCx9rTt27AAAvPTSSwbt5syZU+uadu7ciaysLDzxxBP6dU888QT+/fdfnDp1Sr/ul19+gUwmQ2RkZLVtVPTobN26FTqdDgsWLIBcLjfa5m4899xzBj0WgOGxLC0tRVZWFtq0aQMXFxeDY/nLL78gKCgIY8aMqbHusLAw+Pn5GYyVOXnyJI4fP46nnnrqrusmaigYRojqwY0bN1BYWIj27dtXe61jx47Q6XRISUkBALzzzjvIyclBu3bt0KVLF7z66qs4fvy4vr1KpcKHH36InTt3wtvbGwMGDMBHH32E9PT0O9Zx8+ZNzJ49G97e3rC1tYWnpydatmwJAMjNza3WPjAw0GDZ1dUVAPQh6MqVK5DL5WjdurVBO2P7WZMffvgBLVu2hEqlQmJiIhITE9G6dWvY2dkZfDlfvHgRfn5+cHNzq3FbFy9ehFwuR6dOnWr9+bVRcYyqKioqwoIFCxAQEACVSgUPDw94enoiJyfH4FhevHgRnTt3vu325XI5nnzySWzdulU/JmfdunWwsbHB2LFjTbovRJaIYYTIwgwYMAAXL17EmjVr0LlzZ6xevRo9evTA6tWr9W3mzJmD8+fPIyoqCjY2Npg/fz46duyIY8eO3Xbb48aNw6pVq/DCCy9g8+bN2LNnD3bt2gVAHBx6q1t7AyoIgnAPe1hJrVbjt99+Q1JSEtq2bat/dOrUCYWFhVi/fr3JPqs2tFqt0fVVe0EqvPjii3j//fcxbtw4/PTTT9izZw+io6Ph7u5u9FjeyeTJk5Gfn4+tW7fqry56+OGH4ezsXOdtETU0HMBKVA88PT1hZ2eHc+fOVXvt7NmzkMvlCAgI0K9zc3PD1KlTMXXqVOTn52PAgAFYuHAhpk2bpm/TunVrvPzyy3j55Zdx4cIFdOvWDUuWLMEPP/xgtIbs7GzExMRg0aJFWLBggX59TaeDaqN58+bQ6XS4ePGiQW+Isf00ZvPmzSguLsby5cvh4eFh8Nq5c+fw9ttv4++//0a/fv3QunVr7N69Gzdv3qyxd6R169bQ6XQ4ffo0unXrVuPnurq6VptYraSkBGlpabWqGxDnQpkyZQqWLFmiX1dcXFxtu61bt8bJkyfvuL3OnTuje/fuWLduHfz9/ZGcnIwvvvii1vUQNWTsGSGqBwqFAkOHDsWvv/5qcOlnRkYG1q9fj379+sHJyQkAql1m6+DggDZt2kCj0QAACgsLUVxcbNCmdevWcHR01LepqQageq/G0qVL73a3MHz4cADA559/flfb/OGHH9CqVSu88MILePzxxw0er7zyChwcHPSnah577DEIgoBFixZV207FPo0ePRpyuRzvvPNOtd6JqvvdunVr7N+/3+D1lStX1tgzYoxCoah2LL/44otq23jsscfw77//YsuWLTXWXWHSpEnYs2cPli5dCnd3d/3xJWrs2DNCZEJr1qzRn/aoavbs2XjvvfcQHR2Nfv36YcaMGbCyssLXX38NjUaDjz76SN+2U6dOGDRoEHr27Ak3NzccPXoUP//8M2bNmgUAOH/+PIYMGYJx48ahU6dOsLKywpYtW5CRkYEJEybUWJuTk5N+fElpaSmaNWuGPXv2ICkp6a73t1u3bnjiiSfw1VdfITc3F3369EFMTAwSExPv+N7U1FT8+eef1Qa/VlCpVAgPD8emTZvw+eefY/DgwZg0aRI+//xzXLhwAcOGDYNOp8Nff/2FwYMHY9asWWjTpg3eeustvPvuu+jfvz8effRRqFQqHDlyBH5+fvr5OqZNm4YXXngBjz32GB588EH8+++/2L17d7Xemdt5+OGH8d///hfOzs7o1KkTDh06hP/9739wd3c3aPfqq6/i559/xtixY/HMM8+gZ8+euHnzJrZt24YVK1YgKChI33bixIl47bXXsGXLFkyfPh3W1ta1roeoQZPsOh6iRqTiEtCaHikpKYIgCEJ8fLwQHh4uODg4CHZ2dsLgwYOFgwcPGmzrvffeE0JCQgQXFxfB1tZW6NChg/D+++8LJSUlgiAIQmZmpjBz5kyhQ4cOgr29veDs7CyEhoYKP/300x3rvHr1qjBmzBjBxcVFcHZ2FsaOHSukpqZWu9S14tLeGzduGN3PpKQk/bqioiLhpZdeEtzd3QV7e3th5MiRQkpKyh0v7V2yZIkAQIiJiamxzdq1awUAwq+//ioIgiCUlZUJH3/8sdChQwdBqVQKnp6ewvDhw4W4uDiD961Zs0bo3r27oFKpBFdXV2HgwIFCdHS0/nWtViu8/vrrgoeHh2BnZyeEh4cLiYmJNV7aa+yS7ezsbGHq1KmCh4eH4ODgIISHhwtnz56ttg1BEISsrCxh1qxZQrNmzQSlUin4+/sLU6ZMETIzM6tt96GHHhIAVPt7QdSYyQShHkeHERHRbY0ZMwYnTpyoVe8SUWPBMSNERBYiLS0Nv//+OyZNmiR1KUT1imNGiIgklpSUhL///hurV6+GtbU1/vOf/0hdElG9Ys8IEZHE9u3bh0mTJiEpKQnfffcdfHx8pC6JqF5xzAgRERFJij0jREREJCmGESIiIpIUwwgRERFJimGEiIiIJMUwQkRERJJiGCEiIiJJMYwQERGRpBhGiIiISFIMI0RERCQphhEiIiKSFMMIERERSYphhIiIiCTFMEJERESSYhghIiIiSTGMEBERkaQYRoiIiEhSDCNEREQkKYYRIiIikhTDCBEREUmKYYSIiIgkxTBCRCa1du1ayGQyHD16VOpSiKiBYBghIiIiSTGMEBERkaQYRoio3h07dgzDhw+Hk5MTHBwcMGTIEBw+fNigTWlpKRYtWoS2bdvCxsYG7u7u6NevH6Kjo/Vt0tPTMXXqVPj7+0OlUsHX1xejRo3C5cuX63mPiOheWEldABE1LadOnUL//v3h5OSE1157DdbW1vj6668xaNAg7Nu3D6GhoQCAhQsXIioqCtOmTUNISAjUajWOHj2K+Ph4PPjggwCAxx57DKdOncKLL76IFi1a4Pr164iOjkZycjJatGgh4V4SUV3IBEEQpC6CiBqPtWvXYurUqThy5AiCg4OrvT5mzBjs2LEDZ86cQatWrQAAaWlpaN++Pbp37459+/YBALp16wZ/f39s377d6Ofk5OTA1dUVH3/8MV555RXz7RARmR1P0xBRvdFqtdizZw9Gjx6tDyIA4Ovri4kTJ+LAgQNQq9UAABcXF5w6dQoXLlwwui1bW1solUrs3bsX2dnZ9VI/EZkHwwgR1ZsbN26gsLAQ7du3r/Zax44dodPpkJKSAgB45513kJOTg3bt2qFLly549dVXcfz4cX17lUqFDz/8EDt37oS3tzcGDBiAjz76COnp6fW2P0RkGgwjRGSRBgwYgIsXL2LNmjXo3LkzVq9ejR49emD16tX6NnPmzMH58+cRFRUFGxsbzJ8/Hx07dsSxY8ckrJyI6ophhIjqjaenJ+zs7HDu3Llqr509exZyuRwBAQH6dW5ubpg6dSp+/PFHpKSkoGvXrli4cKHB+1q3bo2XX34Ze/bswcmTJ1FSUoIlS5aYe1eIyIQYRoio3igUCgwdOhS//vqrweW3GRkZWL9+Pfr16wcnJycAQFZWlsF7HRwc0KZNG2g0GgBAYWEhiouLDdq0bt0ajo6O+jZE1DDw0l4iMos1a9Zg165d1dYvXLgQ0dHR6NevH2bMmAErKyt8/fXX0Gg0+Oijj/TtOnXqhEGDBqFnz55wc3PD0aNH8fPPP2PWrFkAgPPnz2PIkCEYN24cOnXqBCsrK2zZsgUZGRmYMGFCve0nEd07XtpLRCZVcWlvTVJSUnDjxg3MmzcPf//9N3Q6HUJDQ/H++++jd+/e+nbvv/8+tm3bhvPnz0Oj0aB58+aYNGkSXn31VVhbWyMrKwuRkZGIiYlBSkoKrKys0KFDB7z88ssYO3ZsfewqEZkIwwgRERFJimNGiIiISFIMI0RERCQphhEiIiKSFMMIERERSYphhIiIiCTFMEJERESSanKTnul0OqSmpsLR0REymUzqcoiIiBoMQRCQl5cHPz8/yOWm689ocmEkNTXV4N4XREREVDcpKSnw9/c32faaXBhxdHQEIB7IintgEBER0Z2p1WoEBATov0tNpcmFkYpTM05OTgwjREREd8HUwxw4gNUEikq0OH41R+oyiIiIGiSGERP44o8LGLXsb7y99QRyC0ulLoeIiKhBYRi5R4IgIDNfA0EAfjicjAeW7MUvcVfB+w8SERHVDsPIPZLJZPjo8SD8+Nz9aOPlgKyCEry86V+MX3kY5zPypC6PiIjI4jGMmEjv1u7Y8VJ/vDasPWys5YhNuomHPvsLUTvPoEBTJnV5REREFothxISUVnLMGNQG/4sYiAc7eaNMJ+DrfZfw4Kf7sOtkOk/dEBERGcEwYgb+rnZYNTkYqycHw9/VFqm5xXjhhzg8s/YIkrMKpS6PiIjIojCMmFFYJ29Ezx2ImYNbw1ohw5/nbuDB/9uHL2IuQFOmlbo8IiIii8AwYma2SgVeDe+AnbMHoE9rd2jKdFgSfR7Dlv6FAxcypS6PiIhIcgwj9aSNlwPWTQvFZxO6wdNRhaTMAjz1zT+YtT4eGepiqcsjIiKSDMNIPZLJZBjVrRliXh6Ip/u0gFwGbD+ehiFL9mHNgSSUaXVSl0hERFTvZEITu8RDrVbD2dkZubm5kt+b5uS1XLy19ST+TckBAHT0dcJ7ozujZ3NXSesiIiIyxlzfoewZkVDnZs7YMr0PPhjTBc621jiTpsZjyw/ijV+OI7ugROryiIiI6gXDiMTkchkmhgYi5uWBeLynPwBgw5EUPLBkL346kgKdrkl1XBERURPE0zQWJjbpJuZvPYlz5VPJ92zuivdGd0ZHX8urlYiImhaepmkiQlq6YftL/fDWQx1hp1Qg7ko2Hv7iAN7bfhr5nFaeiIgaIYYRC2StkOO5Aa0Q8/JAPNTFB1qdgNUHkjBkyV78fjyN08oTEVGjwjBiwXydbfHVkz3x7dReCHSzQ4Zag5nr4zF5TSySMgukLo+IiMgkGEYagMHtvbBn7gDMHtIWSoUcf13IRPj/7cen0edRXMpp5YmIqGGTNIxERUWhV69ecHR0hJeXF0aPHo1z587d9j1r166FTCYzeNjY2NRTxdKxsVZg7oPtsHvuAPRv64ESrQ6fx1xA+NL92HvuutTlERER3TVJw8i+ffswc+ZMHD58GNHR0SgtLcXQoUNRUHD7UxBOTk5IS0vTP65cuVJPFUuvpYc9vn8mBMsm9oC3kwpXsgrx9LdHMP2HOKTmFEldHhERUZ1ZSfnhu3btMlheu3YtvLy8EBcXhwEDBtT4PplMBh8fH3OXZ7FkMhlGdPXFwPaeWBp9Ht8evIydJ9Ox7/wNzAlri6l9W8JawTNwRETUMFjUN1Zubi4AwM3N7bbt8vPz0bx5cwQEBGDUqFE4depUjW01Gg3UarXBo7FwUFnh7Yc74bdZ/dCzuSsKS7T4YMdZPPz5ARy5fFPq8oiIiGrFYiY90+l0eOSRR5CTk4MDBw7U2O7QoUO4cOECunbtitzcXHzyySfYv38/Tp06BX9//2rtFy5ciEWLFlVbb6mTnt0tnU7Az3FXEbXzDLILSwEAj/f0x7zhHeDuoJK4OiIiagzMNemZxYSR6dOnY+fOnThw4IDRUFGT0tJSdOzYEU888QTefffdaq9rNBpoNBr9slqtRkBAQKMLIxWyC0rw0e6z+DE2BQDgbGuN14d1wIReAZDLZRJXR0REDVmjnoF11qxZ2L59O/788886BREAsLa2Rvfu3ZGYmGj0dZVKBScnJ4NHY+Zqr0TUo13xy/Q+6OjrhNyiUry55QTGLD+Ik9dypS6PiIioGknDiCAImDVrFrZs2YI//vgDLVu2rPM2tFotTpw4AV9fXzNU2HD1bO6K32b1xYKHO8FBZYV/U3LwyJcHsHDbKaiLS6Uuj4iISE/SMDJz5kz88MMPWL9+PRwdHZGeno709HQUFVVeojp58mTMmzdPv/zOO+9gz549uHTpEuLj4/HUU0/hypUrmDZtmhS7YNGsFHI8068lYl4eiJFBftAJwNqDlzFkyT78mnCN08oTEZFFkDSMLF++HLm5uRg0aBB8fX31j40bN+rbJCcnIy0tTb+cnZ2N5557Dh07dsRDDz0EtVqNgwcPolOnTlLsQoPg7WSDL57ojv8+G4KWHva4kafB7A0JeHL1P0i8ni91eURE1MRZzADW+mKuwTcNhaZMi5X7LuHLPxOhKdPBWiHD8wNaYdbgtrBVKqQuj4iILFijHsBK9UdlpcCLQ9oieu5ADG7viVKtgGV/XsSD/7cPMWcypC6PiIiaIIaRJirQ3Q5rnu6FFU/1hJ+zDa5mF+HZ747iue+P4mp2odTlERFRE8Iw0oTJZDIM6+yD6IiB+M/AVrCSyxB9OgNhn+7DV3sTUVKmk7pEIiJqAhhGCPYqK8wb3hE7ZvdHSEs3FJfq8NGuc3jo879w6GKW1OUREVEjxzBCeu28HbHx+fvx6bgguNsrkXg9H0+sOoy5GxNwI09z5w0QERHdBYYRMiCTyfBoD3/88fIgPHV/IGQyYMuxa3hgyV58f+gytLomdfEVERHVA17aS7f1b0oO3t56EifKp5Lv0swZ743ujKAAF2kLIyKiesdLe0kSQQEu2DqzL94ZdR8cbaxw4louRn/1N97eegK5hZxWnoiI7h3DCN2RQi7D5N4tEPPyQIzp3gyCAPxwOBkPLNmLX+Kuclp5IiK6JzxNQ3V26GIW5v96Uj+VfEhLN7w3ujPaeTtKXBkREZkTT9OQxejd2h07XuqP14a1h421HLFJN/HQZ38haucZFJaUSV0eERE1MAwjdFeUVnLMGNQG/4sYiAc7eaNMJ+DrfZcQtmQfdp9K56kbIiKqNYYRuif+rnZYNTkYqycHw9/VFqm5xfjPf+Pw7HdHkZzFaeWJiOjOGEbIJMI6eSN67kDMGtwG1goZ/jh7HQ/+3z58EXMBmjKt1OUREZEF4wBWMrnE6/lY8OtJHCyfSt5OqUCQvwu6B7qgR6ArejR3hZu9UuIqiYiorsz1HcowQmYhCAK2/ZuKD3acQYa6+lTyLdzt0CPQFd2bu6JHoAvaezvCSsGOOiIiS8YwYiIMI/VLqxOQeD0f8cnZiL+SjfjkbFy8UVCtXUXvSY/mYu9J90D2nhARWRqGERNhGJFeTmEJjqXk4NiVbMQn5yAhJQf5muqXBLf0sK88tRPoivY+jlDIZRJUTEREAMOIyTCMWB6tTsCF63mIv5Ij9qAkZ+OSkd4Te6UCQQEV405c0D3AFa7sPSEiqjcMIybCMNIw5BSW4FhyZThJSM5BQUn1q3Jaedije3k46RHoinbe7D0hIjKXRhlGoqKisHnzZpw9exa2trbo06cPPvzwQ7Rv3/6279u0aRPmz5+Py5cvo23btvjwww/x0EMP1eozGUYaJq1OwPmMPMQnZ+tDirHeEweVFYICnPWndroHusDFjr0nRESm0CjDyLBhwzBhwgT06tULZWVlePPNN3Hy5EmcPn0a9vb2Rt9z8OBBDBgwAFFRUXj44Yexfv16fPjhh4iPj0fnzp3v+JkMI41HdkEJElJq0Xviaa8PJuw9ISK6e40yjNzqxo0b8PLywr59+zBgwACjbcaPH4+CggJs375dv+7+++9Ht27dsGLFijt+BsNI41W19yT+Sg6OJWfjUiZ7T4iITMVc36FWJtuSCeTm5gIA3Nzcamxz6NAhREREGKwLDw/H1q1bjbbXaDTQaCrnuVCr1fdeKFkkhVyGjr5O6OjrhCdDmwMQe0+OpWTrB8f+W37lzt+JWfg7MUv/3orek4rBsW292HtCRFRfLCaM6HQ6zJkzB3379r3t6Zb09HR4e3sbrPP29kZ6errR9lFRUVi0aJFJa6WGw9VeiQc6eOOBDuLfGa1OwLn0PP2pnWPJOUjKLMClG+Lj57irAMTek24BLugR6CJOzBbgCmc7ayl3hYio0bKYMDJz5kycPHkSBw4cMOl2582bZ9CTolarERAQYNLPQEEWcHwDEPoCIFeYdttkUgq5DJ38nNDJzwlP3S/2ntwsKMGx8nASfyUH/14Ve08OJGbiQGKm/r2tK3pPmos9KG29HCBn7wkR0T2ziDAya9YsbN++Hfv374e/v/9t2/r4+CAjI8NgXUZGBnx8fIy2V6lUUKlUJqu1GkEAts0Czu0Azu4AHv0acL79PpBlcbNXYkhHbwzpKPaelGl1OJeRh/jkionZsnE5qxAXbxTg4o0CbCrvPXFUWaFboIt4aXGgOO8Je0+IiOpO0gGsgiDgxRdfxJYtW7B37160bdv2ju8ZP348CgsL8dtvv+nX9enTB127dpVmAKsgAAnrgB2vAaUFgI0zMPIz4L4x975tshhZ+RqDeU/+TclFUWn1K3faeDmgR5UbArbxZO8JETUejfJqmhkzZmD9+vX49ddfDeYWcXZ2hq2tLQBg8uTJaNasGaKiogCIl/YOHDgQixcvxogRI7BhwwZ88MEH0l/am3UR2PwccC1OXO72FDB8MaByNN1nkMUo0+pwNj0Px6rMe3I5q7Bau2q9J4GucLZl7wkRNUyNMozIZMZ/Y/z222/x9NNPAwAGDRqEFi1aYO3atfrXN23ahLfffls/6dlHH31kGZOeaUuBvYuBv5YAEADXlsBj3wD+PU37OWSR6tp7EhTgAn9XOzRzsYGvsy3sVRZx1pSIqEaNMoxIoV7mGbn8N7DlP0BuCiBTAIPnAf0iOLi1ianaexJfHlKuGOk9qeBkYwU/F1v4udjC19lG/9PX2RbNXGzh7ayCyop/h4hIOgwjJlJvk54V5QDb5wKnNovLgX3Ewa0ugeb7TLJ4mVV6T86kqZGWU4zU3CLkFVe/a7ExHg4q+LnY6MOKn7MtfF0qA4uno4rzoxCR2TCMmEi9zsAqCMC/G4AdrwAl+YDKGXj4U6DL4+b9XGpw8jVlSMspwrWcIqTlFiMtpwipucVILV9OzSmCpkx3x+1YyWXwdhLDiq+LLfxcbMTAUqWnxc1eWeMpUiKi22EYMRFJpoO/eQnY/Dxw9Yi4HPQEMPwjwIbT0VPtCIKA7MJSpOYUVQaU3CKxZ6V8OV1dDK3uzv+cVVZyg1NAfi421U4NOdpwkC0RVccwYiKS3ZtGWwrs+wj46xNA0AGuLYBHVwMBveqvBmrUtDoBN/I0SM0tDyw5lYElLbcI13KKkZmvufOGIF4F5OdSeQrIryKolPe0+DjbwMaa41eImhqGEROR/EZ5Vw6JvSS5yeLg1oGvA/1fBhS8koLMT1OmRUZulcByy6mg1JwiqGs5fsXdXll9sK2Lrf7qIC9HFawUcjPvERHVJ4YRE5E8jABAcS7w+8vAiU3icsD9wKMrAdfm0tRDVEWBpgxpuUVILT8FlFo+hqXi1FBqThGKS+88fkUug378StWrhCoG2/q62MCd41eIGhSGEROxiDBS4fhPwPYIoCQPUDkBIz4Fuo6VtiaiOxAEATmFpdVOAaXlVp4aSs8tRlktxq8oreTlAcXw6iAvRxu4Oyjh6aCCu4MSdkr2HBJZAoYRE7GoMAIA2ZfF0zYp/4jLXccDD30sTitP1EBpdQIy8zW3nAISA0tFT8uNfA1q+7+PrbUC7g5KuDuo4OmghLu9Sr/s4aCER3locbdXwc1eycubicyEYcRELC6MAIC2TBzYuu9DcXCrSyDw6Cog8H6pKyMym5IyHTLUxQZXB1UMvM3M1yAzvwSZ+ZpaXdJclUwGuNkp9eHEvTyseJSHF3f7ilBT0eui4KkiolpiGDERiwwjFZL/Ee9vk3MFkMmBAa8BA17l4FZqsgRBQEGJFlnl4aTqz6wCMaxklYeWrIISZBeW1Lq3pYKNtRzu9ip4OKrgYa/U97i42yvh6aiq0gujhJudkoNyqUljGDERiw4jAFCsBna8ChzfIC77h4iDW91aSlsXUQNQptUhu7BUH1KyCqqGmPLgUlC5XJuBuFXJZICrnbK8d6VKD0t5b8utvTD27HWhRoZhxEQsPoxUOPGzOJ28Rg0oHYERn4jjSfgfG5FJCIKAwhJteUCp0sNS0ftSYBhibt5Fr4vKSm70FNGt41w8HJRws2evC1k+hhETaTBhBACyr4g33Es+JC53fhwYsQSwdZG0LKKmSKsTkF14y6mhit6XvCq9MOXLxu7YfCeudtb60FJ52khlMP7F2dYazrbWcLKxho21nD0vVK8YRkykQYURANBpgb8+BfZGAYIWcA4UT9s07y11ZUR0G4UlZQahJdPYOJfy8HKzoAS1uBK6GqVCDidbazjZWhmEFOca11X+dFRZQc6rjqiOGEZMpMGFkQpXjwK/PCteCiyTi7O2DnwdUPAeIkQNnVYnIKewpHJ8S/kpoorQklkltKiLSqEuLqvVfYhuRyYTp/13tjMMLLeGFicbq+rrbayhtOIppaaIYcREGmwYAQBNHrDjNeDf9eJys2DgsVWAWytp6yKieiUIAvI1ZVAXlyG3sBS5RaVQF5f/LH/kFlWsL6t8Xv6zrpdLG2NrragSUqz0IcXpluDiXBFoqoQeXk7dcDGMmEiDDiMVTv4C/DYX0OQCSgfxDsDdJnJwKxHVSnGpFuriinBSpg8p6uLSauFGDDFl+jCTp6ndvYtux0ouq+x5qRpYjAYZw8DjaGPNSe0kxDBiIo0ijABAToo4uPXK3+LyfWOAh/8PsHWVti4iatS0OgF5xYYhpXp4MeyZUVdZX5vbBNyJo8qqSg9MZUhxUFnB0cYK9ior/XOH8ucONlZwVFnDwcYK9ioFVFa86/TdYBgxkUYTRgBxcOuB/xMHt+rKACd/4NGvgRb9pK6MiKiaisupq/fCGJ5KUhfdeupJfP1urlCqiVIhh0O1sFIeZMqfV6w3HnLEYGNnrWhSA4EZRkykUYWRClfjgM3TgJuXAMiA/hHAoHkc3EpEjYqmTIs8I2NgKk4f5ReXoUBTpn+eryl/FIvrCjRlKCwxXaABxLPjDsrKEGOsR+bW5/owc0vIaQiDghtlGNm/fz8+/vhjxMXFIS0tDVu2bMHo0aNrbL93714MHjy42vq0tDT4+PjU6jMbZRgBAE0+sOt14NgP4rJfD+Cx1YB7a2nrIiKyIGVaHQpKtPqQkq8pRV5xGQo0Wv3z/CphpmrIydeUVb6uufcrmm6ltJKLPTJVA0zVZYMeG2s4qBT6HpqqIcicA4TN9R0q6U1PCgoKEBQUhGeeeQaPPvpord937tw5g4Pg5eVljvIaFpUDMGoZ0OZB4LfZQGo8sKI/MPxDoPtTHNxKRATASiGHs60czrb31nMsCAKKS3XI05SKQaa4DHmaUoMembwqIaaid8agx6Z8ueL0U0mZDlll4uy/96Kit6YipHwxsTs6+Fj2L9+ShpHhw4dj+PDhdX6fl5cXXFxcTF9QY3DfaMA/GNjyAnD5L2DbLCAxGnh4KWDnJnV1RESNgkwmg61SAVulAnC8t22VaXUo0GjFMGPkFJNBj0xxGfJLqrTRvy6+VycAggDklffqAIC8Afwy2iBvB9utWzdoNBp07twZCxcuRN++fWtsq9FooNFo9Mtqtbo+SpSWsz8w+Vfg4OfAH+8Bp38VJ00b8zXQsr/U1RERURVWCjmc7eRwtrv33pqiUq1hoCkuQ4CrnYkqNR/LHy1Tha+vL1asWIFffvkFv/zyCwICAjBo0CDEx8fX+J6oqCg4OzvrHwEBAfVYsYTkCqDfXGDa/wD3NoD6GvDdSCA6Eii7ty5AIiKyPDKZDHZKK3g52qCVpwO6+rugTxsPsffGwlnM1TQymeyOA1iNGThwIAIDA/Hf//7X6OvGekYCAgIa3wDW2ykpAHa9AcR/Ly77dhMHt3q0lbQsIqI60emAoptAXhqQlwHkpwN56UD+dfHeXTKF+IuYTF7+85bncnmVNlVfq9qmtq/dsi39a7Lq6+Tl6421l8vF7VV7TW6RY/0a5QBWUwgJCcGBAwdqfF2lUkGlUtVjRRZIaQ888kX54NaXgLQE4OsBwLAooMcUi/wLT0RNiE4LFGRWhou8dCA/w0joyBDnVGoqZMaCivyWwGIkgBm8Jgce+8bif/ls8GEkISEBvr6+UpfRMHR6pHJwa9I+8aqbC9FiUOHgViIyNW0ZUHC9SsBIF8NFXlp52ChfX3BD7NmoLTsPwNFHfDj4AA5e4rxKOq24HZ0WEHTiw2CdVuxdEbRGXquybPCazvD9+teMvN9Ye51WHFFabV0t9rdiH1AG3Mv0KGWaO7eRmKRhJD8/H4mJifrlpKQkJCQkwM3NDYGBgZg3bx6uXbuG778XTy8sXboULVu2xH333Yfi4mKsXr0af/zxB/bs2SPVLjQ8Tn7ApK3AoS+BmHeAs9uBa3HAmBVAq0FSV0dEDUFZiRgm9L0XxnoyMsSQgVqOBJDJAXtPwMEbcPQFHL3FoOFYvlzx3N4LsFKadffqjU5XJcQYC0Z1eU1nJBiVP3dtLvWe3pGkYeTo0aMGk5hFREQAAKZMmYK1a9ciLS0NycnJ+tdLSkrw8ssv49q1a7Czs0PXrl3xv//9z+hEaHQbcjnQ9yWg5QDgl2lA1gXg+9FAnxeBB+Y3nn/oRFQ3pcXGey9uDRqFWbXfpkxRHjAqwkVFj8YtocPeE1A0+M76upHLIV5H0sT22wiLGcBaXxrtDKx3q6QQ2P0mEPetuOzTVTy/6NlO2rqIyHRKCqqEitucMinOqf025dZVQkWVUya3hg47d3EMAzUKjXI6eCkwjNTg7O/Ar7PEkepWtsCwD4CeUzm4lciSafKMDPi8NXRkAJo6zK+kUFU5NWIkaFScMrF1Lf/NnpoSXk1D5tVhhHg/m63TgUt/AtvnAhf+Jw5utXeXujqihkOnA7QacdCgtuSWnxpxvIXBTw2gLb3Ne6q8V5NnOCajtKD2dVnbGRmPYeSUiY0LfwmheseeETKk0wGHvwJiFon/CTp4i4NbWz8gdWVE1QlClS/ykrsIAXfb7jbt6/vSU6Vj9Z4MY6FD5ciQQfeMp2lMhGGkltKOi4NbM8+Jy71nAUMWAFZNfM4WMp0yjXhaQZ0qPnKvij/zUsW7UGtLbhMCqoQBS6dQiqc+rGr6qRLbGPy8tV35c6W9YcBw8BZvkklUTxhGTIRhpA5KCoHo+cCR1eKydxdx5lavDtLWRZavtKgyZKivlT+qLqeWX/ZpYnKrOnzh1/TFb6T9vbyXvRHUiDCMmAjDyF04twv4dYZ4OZ+VDTD0PaDXNP4n21Rp8sUejYqejKoBo+J50c3abcvKRpz7xqlZ+cNPfKicKr/cFcra9yZwQCWRWTGMmAjDyF3KyxAHt16MEZfbDQdGfQnYe0hbF5lWsbrmnoyK58W5tduWtV1lwHD2rwwa+tDRTLwig6GWqMFgGDERhpF7oNMBsV8D0QvEc/b2XsCY5UCbMKkrozsRBKAou+aAUfG8JK9221M53RIumt0SNPwAG2cGDaJGhmHERBhGTCD9pDi49cYZcfn+GcCQSMDaRtq6mipBEE+h6UNFDUGjtLB227NxEUOFs5GA4dRMvErDhv92iJoihhETYRgxkdIisYckdqW47N25fHBrR2nramx0OnGg5+1Om6hTa39ViZ17eaio4bSJk694xQYRkREMIybCMGJi5/eIg1sLboiDER98Fwh5jt3ztaHTirNjGgSMKkEj95o4UFRXWrvt2XvdZnyGH+Dox94rIronDCMmwjBiBvnXga0zgMRocbntUGDUMvG23g2ZIIhzWZQWio+Swsrn+uUicRbMqs9Li8R7gZQWGW9bWiQul+SV3x78TmTinBLGxmVUPHf05Q0OicjsGEZMhGHETAQBiF0F7HlbPGVg7wmM+gpoN9R8n6nTAWVFhiHBaGCoeNQUEmpoW1pYy7BwD2QKMUjUND7DyU+c2Ephbd46iIhqgWHERBhGzCzjtDi49fopcTnkP0Df2UBZcc0h4NaQoH9epRfBWNuyovrbL4VSvFTV2g5Q2gHWtoC1vfhTWf7T4HUjzw3a2oszZ9p78o6mRNRgMIyYCMNIPSgtBv63EPhnef19pvWtAaGmEHC7kFBD2LC2AxS8pyQREe/aSw2HtQ0wfLE4/8j2uYD6qvjFXq1HoWogMNbjcKfXywOElQ1n3iQiasAYRsh82oYBc46Lz3l1DRER1YBhhMyLIYSIiO6AfdtEREQkqSbXM1IxXletVktcCRERUcNS8d1p6mtfmlwYycsTbwQWEBAgcSVEREQNU15eHpydnU22vSZ3aa9Op0NqaiocHR0hM9F4BrVajYCAAKSkpPBy4Vvw2BjH41IzHhvjeFxqxmNjnDmOiyAIyMvLg5+fH+QmvIqxyfWMyOVy+Pv7m2XbTk5O/IdQAx4b43hcasZjYxyPS814bIwz9XExZY9IBQ5gJSIiIkkxjBAREZGkGEZMQKVSITIyEiqVSupSLA6PjXE8LjXjsTGOx6VmPDbGNaTj0uQGsBIREZFlYc8IERERSYphhIiIiCTFMEJERESSYhghIiIiSTGMmMCyZcvQokUL2NjYIDQ0FLGxsVKXJLn9+/dj5MiR8PPzg0wmw9atW6UuySJERUWhV69ecHR0hJeXF0aPHo1z585JXZbkli9fjq5du+onZ+rduzd27twpdVkWafHixZDJZJgzZ47UpUhq4cKFkMlkBo8OHTpIXZbFuHbtGp566im4u7vD1tYWXbp0wdGjR6Uuq0YMI/do48aNiIiIQGRkJOLj4xEUFITw8HBcv35d6tIkVVBQgKCgICxbtkzqUizKvn37MHPmTBw+fBjR0dEoLS3F0KFDUVBQIHVpkvL398fixYsRFxeHo0eP4oEHHsCoUaNw6tQpqUuzKEeOHMHXX3+Nrl27Sl2KRbjvvvuQlpamfxw4cEDqkixCdnY2+vbtC2tra+zcuROnT5/GkiVL4OrqKnVpNRPonoSEhAgzZ87UL2u1WsHPz0+IioqSsCrLAkDYsmWL1GVYpOvXrwsAhH379kldisVxdXUVVq9eLXUZFiMvL09o27atEB0dLQwcOFCYPXu21CVJKjIyUggKCpK6DIv0+uuvC/369ZO6jDphz8g9KCkpQVxcHMLCwvTr5HI5wsLCcOjQIQkro4YiNzcXAODm5iZxJZZDq9Viw4YNKCgoQO/evaUux2LMnDkTI0aMMPj/pqm7cOEC/Pz80KpVKzz55JNITk6WuiSLsG3bNgQHB2Ps2LHw8vJC9+7dsWrVKqnLui2GkXuQmZkJrVYLb29vg/Xe3t5IT0+XqCpqKHQ6HebMmYO+ffuic+fOUpcjuRMnTsDBwQEqlQovvPACtmzZgk6dOkldlkXYsGED4uPjERUVJXUpFiM0NBRr167Frl27sHz5ciQlJaF///7Iy8uTujTJXbp0CcuXL0fbtm2xe/duTJ8+HS+99BK+++47qUurUZO7ay+RpZg5cyZOnjzJ89zl2rdvj4SEBOTm5uLnn3/GlClTsG/fviYfSFJSUjB79mxER0fDxsZG6nIsxvDhw/XPu3btitDQUDRv3hw//fQTnn32WQkrk55Op0NwcDA++OADAED37t1x8uRJrFixAlOmTJG4OuPYM3IPPDw8oFAokJGRYbA+IyMDPj4+ElVFDcGsWbOwfft2/Pnnn/D395e6HIugVCrRpk0b9OzZE1FRUQgKCsJnn30mdVmSi4uLw/Xr19GjRw9YWVnBysoK+/btw+effw4rKytotVqpS7QILi4uaNeuHRITE6UuRXK+vr7VQnzHjh0t+jQWw8g9UCqV6NmzJ2JiYvTrdDodYmJieK6bjBIEAbNmzcKWLVvwxx9/oGXLllKXZLF0Oh00Go3UZUhuyJAhOHHiBBISEvSP4OBgPPnkk0hISIBCoZC6RIuQn5+PixcvwtfXV+pSJNe3b99qUwacP38ezZs3l6iiO+NpmnsUERGBKVOmIDg4GCEhIVi6dCkKCgowdepUqUuTVH5+vsFvKElJSUhISICbmxsCAwMlrExaM2fOxPr16/Hrr7/C0dFRP7bI2dkZtra2ElcnnXnz5mH48OEIDAxEXl4e1q9fj71792L37t1SlyY5R0fHamOK7O3t4e7u3qTHGr3yyisYOXIkmjdvjtTUVERGRkKhUOCJJ56QujTJzZ07F3369MEHH3yAcePGITY2FitXrsTKlSulLq1mUl/O0xh88cUXQmBgoKBUKoWQkBDh8OHDUpckuT///FMAUO0xZcoUqUuTlLFjAkD49ttvpS5NUs8884zQvHlzQalUCp6ensKQIUOEPXv2SF2WxeKlvYIwfvx4wdfXV1AqlUKzZs2E8ePHC4mJiVKXZTF+++03oXPnzoJKpRI6dOggrFy5UuqSbksmCIIgUQ4iIiIi4pgRIiIikhbDCBEREUmKYYSIiIgkxTBCREREkmIYISIiIkkxjBAREZGkGEaIiIhIUgwjREREJCmGESIiIpIUwwgRERFJimGEiIiIJMUwQkRERJJiGCEiIiJJMYwQERGRpBhGiIiISFIMI0RERCQphhEiIiKSFMMIERERSYphhIiIiCTFMEJERESSYhghIiIiSTGMEBERkaQYRoiIiEhSDCNEREQkKYYRIiIikhTDCBEREUmKYYSIiIgkxTBCREREkmIYISIiIkkxjBAREZGkGEaIiIhIUgwjREREJCmzhZH9+/dj5MiR8PPzg0wmw9atW+/4nr1796JHjx5QqVRo06YN1q5dW63NsmXL0KJFC9jY2CA0NBSxsbGmL56IiIjqjdnCSEFBAYKCgrBs2bJatU9KSsKIESMwePBgJCQkYM6cOZg2bRp2796tb7Nx40ZEREQgMjIS8fHxCAoKQnh4OK5fv26u3SAiIiIzkwmCIJj9Q2QybNmyBaNHj66xzeuvv47ff/8dJ0+e1K+bMGECcnJysGvXLgBAaGgoevXqhS+//BIAoNPpEBAQgBdffBFvvPGGWfeBiIiIzMNK6gIqHDp0CGFhYQbrwsPDMWfOHABASUkJ4uLiMG/ePP3rcrkcYWFhOHToUI3b1Wg00Gg0+mWdToebN2/C3d0dMpnMtDtBRETUiAmCgLy8PPj5+UEuN93JFYsJI+np6fD29jZY5+3tDbVajaKiImRnZ0Or1Rptc/bs2Rq3GxUVhUWLFpmlZiIioqYoJSUF/v7+JtuexYQRc5k3bx4iIiL0y7m5uQgMDERKSgqcnJwkrIyIiKhhUavVCAgIgKOjo0m3azFhxMfHBxkZGQbrMjIy4OTkBFtbWygUCigUCqNtfHx8atyuSqWCSqWqtt7JyYlhhIiI6C6YepiDxcwz0rt3b8TExBisi46ORu/evQEASqUSPXv2NGij0+kQExOjb0NEREQNj9nCSH5+PhISEpCQkABAvHQ3ISEBycnJAMTTJ5MnT9a3f+GFF3Dp0iW89tprOHv2LL766iv89NNPmDt3rr5NREQEVq1ahe+++w5nzpzB9OnTUVBQgKlTp5prN4iIiMjMzHaa5ujRoxg8eLB+uWLcxpQpU7B27VqkpaXpgwkAtGzZEr///jvmzp2Lzz77DP7+/li9ejXCw8P1bcaPH48bN25gwYIFSE9PR7du3bBr165qg1qJiIio4aiXeUYsiVqthrOzM3JzczlmhIiIqA7M9R1qMWNGiIiIqGliGCEiIiJJMYwQERGRpBhGiIiISFIMI0RERCQphhEiIiKSFMMIERERSYphhIiIiCTFMEJERESSYhghIiIiSTGMEBERkaQYRoiIiEhSDCNEREQkKYYRIiIikhTDCBEREUmKYYSIiIgkxTBCREREkmIYISIiIkmZPYwsW7YMLVq0gI2NDUJDQxEbG1tj20GDBkEmk1V7jBgxQt/m6aefrvb6sGHDzL0bREREZCZW5tz4xo0bERERgRUrViA0NBRLly5FeHg4zp07By8vr2rtN2/ejJKSEv1yVlYWgoKCMHbsWIN2w4YNw7fffqtfVqlU5tsJIiIiMiuz9ox8+umneO655zB16lR06tQJK1asgJ2dHdasWWO0vZubG3x8fPSP6Oho2NnZVQsjKpXKoJ2rq6s5d4OIiIjMyGxhpKSkBHFxcQgLC6v8MLkcYWFhOHToUK228c0332DChAmwt7c3WL937154eXmhffv2mD59OrKysmrchkajgVqtNngQERGR5TBbGMnMzIRWq4W3t7fBem9vb6Snp9/x/bGxsTh58iSmTZtmsH7YsGH4/vvvERMTgw8//BD79u3D8OHDodVqjW4nKioKzs7O+kdAQMDd7xQRERGZnFnHjNyLb775Bl26dEFISIjB+gkTJuifd+nSBV27dkXr1q2xd+9eDBkypNp25s2bh4iICP2yWq1mICEiIrIgZusZ8fDwgEKhQEZGhsH6jIwM+Pj43Pa9BQUF2LBhA5599tk7fk6rVq3g4eGBxMREo6+rVCo4OTkZPIiIiMhymC2MKJVK9OzZEzExMfp1Op0OMTEx6N27923fu2nTJmg0Gjz11FN3/JyrV68iKysLvr6+91wzERER1T+zXk0TERGBVatW4bvvvsOZM2cwffp0FBQUYOrUqQCAyZMnY968edXe980332D06NFwd3c3WJ+fn49XX30Vhw8fxuXLlxETE4NRo0ahTZs2CA8PN+euEBERkZmYdczI+PHjcePGDSxYsADp6eno1q0bdu3apR/UmpycDLncMA+dO3cOBw4cwJ49e6ptT6FQ4Pjx4/juu++Qk5MDPz8/DB06FO+++y7nGiEiImqgZIIgCFIXUZ/UajWcnZ2Rm5vL8SNERER1YK7vUN6bhoiIiCTFMEJERESSYhghIiIiSTGMEBERkaQYRoiIiEhSDCNEREQkKYYRIiIikhTDCBEREUmKYYSIiIgkxTBCREREkmIYISIiIkkxjBAREZGkGEaIiIhIUgwjREREJCmGESIiIpIUwwgRERFJimGEiIiIJGX2MLJs2TK0aNECNjY2CA0NRWxsbI1t165dC5lMZvCwsbExaCMIAhYsWABfX1/Y2toiLCwMFy5cMPduEBERkZmYNYxs3LgRERERiIyMRHx8PIKCghAeHo7r16/X+B4nJyekpaXpH1euXDF4/aOPPsLnn3+OFStW4J9//oG9vT3Cw8NRXFxszl0hIiIiMzFrGPn000/x3HPPYerUqejUqRNWrFgBOzs7rFmzpsb3yGQy+Pj46B/e3t761wRBwNKlS/H2229j1KhR6Nq1K77//nukpqZi69at5twVIiIiMhOzhZGSkhLExcUhLCys8sPkcoSFheHQoUM1vi8/Px/NmzdHQEAARo0ahVOnTulfS0pKQnp6usE2nZ2dERoaetttEhERkeUyWxjJzMyEVqs16NkAAG9vb6Snpxt9T/v27bFmzRr8+uuv+OGHH6DT6dCnTx9cvXoVAPTvq8s2NRoN1Gq1wYOIiIgsh0VdTdO7d29MnjwZ3bp1w8CBA7F582Z4enri66+/vuttRkVFwdnZWf8ICAgwYcVERER0r8wWRjw8PKBQKJCRkWGwPiMjAz4+PrXahrW1Nbp3747ExEQA0L+vLtucN28ecnNz9Y+UlJS67goRERGZkdnCiFKpRM+ePRETE6Nfp9PpEBMTg969e9dqG1qtFidOnICvry8AoGXLlvDx8THYplqtxj///FPjNlUqFZycnAweREREZDmszLnxiIgITJkyBcHBwQgJCcHSpUtRUFCAqVOnAgAmT56MZs2aISoqCgDwzjvv4P7770ebNm2Qk5ODjz/+GFeuXMG0adMAiFfazJkzB++99x7atm2Lli1bYv78+fDz88Po0aPNuStERERkJmYNI+PHj8eNGzewYMECpKeno1u3bti1a5d+AGpycjLk8srOmezsbDz33HNIT0+Hq6srevbsiYMHD6JTp076Nq+99hoKCgrw/PPPIycnB/369cOuXbuqTY5GREREDYNMEARB6iLqk1qthrOzM3Jzc3nKhoiIqA7M9R1qUVfTEBERUdPDMEJERESSYhghIiIiSTGMEBERkaQYRoiIiEhSDCNEREQkKYYRIiIikhTDCBEREUmKYYSIiIgkxTBCREREkmIYISIiIkkxjBAREZGkGEaIiIhIUgwjREREJCmGESIiIpIUwwgRERFJimGEiIiIJMUwQkRERJIyexhZtmwZWrRoARsbG4SGhiI2NrbGtqtWrUL//v3h6uoKV1dXhIWFVWv/9NNPQyaTGTyGDRtm7t0gIiIiMzFrGNm4cSMiIiIQGRmJ+Ph4BAUFITw8HNevXzfafu/evXjiiSfw559/4tChQwgICMDQoUNx7do1g3bDhg1DWlqa/vHjjz+aczeIiIjIjGSCIAjm2nhoaCh69eqFL7/8EgCg0+kQEBCAF198EW+88cYd36/VauHq6oovv/wSkydPBiD2jOTk5GDr1q13VZNarYazszNyc3Ph5OR0V9sgIiJqisz1HWq2npGSkhLExcUhLCys8sPkcoSFheHQoUO12kZhYSFKS0vh5uZmsH7v3r3w8vJC+/btMX36dGRlZZm0diIiIqo/VubacGZmJrRaLby9vQ3We3t74+zZs7Xaxuuvvw4/Pz+DQDNs2DA8+uijaNmyJS5evIg333wTw4cPx6FDh6BQKKptQ6PRQKPR6JfVavVd7hERERGZg9nCyL1avHgxNmzYgL1798LGxka/fsKECfrnXbp0QdeuXdG6dWvs3bsXQ4YMqbadqKgoLFq0qF5qJiIioroz22kaDw8PKBQKZGRkGKzPyMiAj4/Pbd/7ySefYPHixdizZw+6du1627atWrWCh4cHEhMTjb4+b9485Obm6h8pKSl12xEiIiIyK7OFEaVSiZ49eyImJka/TqfTISYmBr17967xfR999BHeffdd7Nq1C8HBwXf8nKtXryIrKwu+vr5GX1epVHBycjJ4EBERkeUw66W9ERERWLVqFb777jucOXMG06dPR0FBAaZOnQoAmDx5MubNm6dv/+GHH2L+/PlYs2YNWrRogfT0dKSnpyM/Px8AkJ+fj1dffRWHDx/G5cuXERMTg1GjRqFNmzYIDw83564QERGRmZh1zMj48eNx48YNLFiwAOnp6ejWrRt27dqlH9SanJwMubwyDy1fvhwlJSV4/PHHDbYTGRmJhQsXQqFQ4Pjx4/juu++Qk5MDPz8/DB06FO+++y5UKpU5d4WIiIjMxKzzjFgizjNCRER0dxrcPCNEREREtcEwQkRERJJiGCEiIiJJMYwQERGRpBhGiIiISFIMI0RERCQphhEiIiKSFMMIERERSYphhIiIiCTFMEJERESSYhghIiIiSTGMEBERkaQYRoiIiEhSDCNEREQkKYYRIiIikhTDCBEREUmKYYSIiIgkxTBCREREkjJ7GFm2bBlatGgBGxsbhIaGIjY29rbtN23ahA4dOsDGxgZdunTBjh07DF4XBAELFiyAr68vbG1tERYWhgsXLphzF4iIiMiMzBpGNm7ciIiICERGRiI+Ph5BQUEIDw/H9evXjbY/ePAgnnjiCTz77LM4duwYRo8ejdGjR+PkyZP6Nh999BE+//xzrFixAv/88w/s7e0RHh6O4uJic+4KERERmYlMEATBXBsPDQ1Fr1698OWXXwIAdDodAgIC8OKLL+KNN96o1n78+PEoKCjA9u3b9evuv/9+dOvWDStWrIAgCPDz88PLL7+MV155BQCQm5sLb29vrF27FhMmTLhjTWq1Gs7OzsjNzYWTk5OJ9pSIiKjxM9d3qJXJtnSLkpISxMXFYd68efp1crkcYWFhOHTokNH3HDp0CBEREQbrwsPDsXXrVgBAUlIS0tPTERYWpn/d2dkZoaGhOHTokNEwotFooNFo9Mu5ubkAxANKREREtVfx3WnqfgyzhZHMzExotVp4e3sbrPf29sbZs2eNvic9Pd1o+/T0dP3rFetqanOrqKgoLFq0qNr6gICA2u0IERERGcjKyoKzs7PJtme2MGIp5s2bZ9DbkpOTg+bNmyM5OdmkB5JqplarERAQgJSUFJ4aqyc85vWPx7z+8ZjXv9zcXAQGBsLNzc2k2zVbGPHw8IBCoUBGRobB+oyMDPj4+Bh9j4+Pz23bV/zMyMiAr6+vQZtu3boZ3aZKpYJKpaq23tnZmX9565mTkxOPeT3jMa9/POb1j8e8/snlpr3+xWxX0yiVSvTs2RMxMTH6dTqdDjExMejdu7fR9/Tu3dugPQBER0fr27ds2RI+Pj4GbdRqNf75558at0lERESWzaynaSIiIjBlyhQEBwcjJCQES5cuRUFBAaZOnQoAmDx5Mpo1a4aoqCgAwOzZszFw4EAsWbIEI0aMwIYNG3D06FGsXLkSACCTyTBnzhy89957aNu2LVq2bIn58+fDz88Po0ePNueuEBERkZmYNYyMHz8eN27cwIIFC5Ceno5u3bph165d+gGoycnJBl09ffr0wfr16/H222/jzTffRNu2bbF161Z07txZ3+a1115DQUEBnn/+eeTk5KBfv37YtWsXbGxsalWTSqVCZGSk0VM3ZB485vWPx7z+8ZjXPx7z+meuY27WeUaIiIiI7oT3piEiIiJJMYwQERGRpBhGiIiISFIMI0RERCSpRhlGli1bhhYtWsDGxgahoaGIjY29bftNmzahQ4cOsLGxQZcuXbBjx456qrTxqMsxX7VqFfr37w9XV1e4uroiLCzsjn9GVF1d/55X2LBhA2QyGS+Hvwt1PeY5OTmYOXMmfH19oVKp0K5dO/7/Ukd1PeZLly5F+/btYWtri4CAAMydO5d3da+D/fv3Y+TIkfDz84NMJtPfG+529u7dix49ekClUqFNmzZYu3Zt3T9YaGQ2bNggKJVKYc2aNcKpU6eE5557TnBxcREyMjKMtv/7778FhUIhfPTRR8Lp06eFt99+W7C2thZOnDhRz5U3XHU95hMnThSWLVsmHDt2TDhz5ozw9NNPC87OzsLVq1frufKGq67HvEJSUpLQrFkzoX///sKoUaPqp9hGoq7HXKPRCMHBwcJDDz0kHDhwQEhKShL27t0rJCQk1HPlDVddj/m6desElUolrFu3TkhKShJ2794t+Pr6CnPnzq3nyhuuHTt2CG+99ZawefNmAYCwZcuW27a/dOmSYGdnJ0RERAinT58WvvjiC0GhUAi7du2q0+c2ujASEhIizJw5U7+s1WoFPz8/ISoqymj7cePGCSNGjDBYFxoaKvznP/8xa52NSV2P+a3KysoER0dH4bvvvjNXiY3O3RzzsrIyoU+fPsLq1auFKVOmMIzUUV2P+fLly4VWrVoJJSUl9VVio1PXYz5z5kzhgQceMFgXEREh9O3b16x1Nla1CSOvvfaacN999xmsGz9+vBAeHl6nz2pUp2lKSkoQFxeHsLAw/Tq5XI6wsDAcOnTI6HsOHTpk0B4AwsPDa2xPhu7mmN+qsLAQpaWlJr/xUmN1t8f8nXfegZeXF5599tn6KLNRuZtjvm3bNvTu3RszZ86Et7c3OnfujA8++ABarba+ym7Q7uaY9+nTB3FxcfpTOZcuXcKOHTvw0EMP1UvNTZGpvkMb1V17MzMzodVq9TO8VvD29sbZs2eNvic9Pd1o+/T0dLPV2ZjczTG/1euvvw4/P79qf6HJuLs55gcOHMA333yDhISEeqiw8bmbY37p0iX88ccfePLJJ7Fjxw4kJiZixowZKC0tRWRkZH2U3aDdzTGfOHEiMjMz0a9fPwiCgLKyMrzwwgt4880366PkJqmm71C1Wo2ioiLY2trWajuNqmeEGp7Fixdjw4YN2LJlS62n9Ke6ycvLw6RJk7Bq1Sp4eHhIXU6TodPp4OXlhZUrV6Jnz54YP3483nrrLaxYsULq0hqtvXv34oMPPsBXX32F+Ph4bN68Gb///jveffddqUujO2hUPSMeHh5QKBTIyMgwWJ+RkQEfHx+j7/Hx8alTezJ0N8e8wieffILFixfjf//7H7p27WrOMhuVuh7zixcv4vLlyxg5cqR+nU6nAwBYWVnh3LlzaN26tXmLbuDu5u+5r68vrK2toVAo9Os6duyI9PR0lJSUQKlUmrXmhu5ujvn8+fMxadIkTJs2DQDQpUsX/b3M3nrrLZPf9p5q/g51cnKqda8I0Mh6RpRKJXr27ImYmBj9Op1Oh5iYGPTu3dvoe3r37m3QHgCio6NrbE+G7uaYA8BHH32Ed999F7t27UJwcHB9lNpo1PWYd+jQASdOnEBCQoL+8cgjj2Dw4MFISEhAQEBAfZbfIN3N3/O+ffsiMTFRH/wA4Pz58/D19WUQqYW7OeaFhYXVAkdFGBR4GzazMNl3aN3G1lq+DRs2CCqVSli7dq1w+vRp4fnnnxdcXFyE9PR0QRAEYdKkScIbb7yhb//3338LVlZWwieffCKcOXNGiIyM5KW9dVTXY7548WJBqVQKP//8s5CWlqZ/5OXlSbULDU5dj/mteDVN3dX1mCcnJwuOjo7CrFmzhHPnzgnbt28XvLy8hPfee0+qXWhw6nrMIyMjBUdHR+HHH38ULl26JOzZs0do3bq1MG7cOKl2ocHJy8sTjh07Jhw7dkwAIHz66afCsWPHhCtXrgiCIAhvvPGGMGnSJH37ikt7X331VeHMmTPCsmXLeGlvhS+++EIIDAwUlEqlEBISIhw+fFj/2sCBA4UpU6YYtP/pp5+Edu3aCUqlUrjvvvuE33//vZ4rbvjqcsybN28uAKj2iIyMrP/CG7C6/j2vimHk7tT1mB88eFAIDQ0VVCqV0KpVK+H9998XysrK6rnqhq0ux7y0tFRYuHCh0Lp1a8HGxkYICAgQZsyYIWRnZ9d/4Q3Un3/+afT/54rjPGXKFGHgwIHV3tOtWzdBqVQKrVq1Er799ts6f65MENh3RURERNJpVGNGiIiIqOFhGCEiIiJJMYwQERGRpBhGiIiISFIMI0RERCQphhEiIiKSFMMIERERSYphhIiIiCTFMEJERESSYhghIiIiSTGMEBERkaQYRoiIiEhSDCNEREQkKYYRIiIikhTDCBEREUmKYYSIzOarr76CTCZDaGhotdcuX74MmUyGTz75xOh7P/nkE8hkMly+fLnaa1u2bMHw4cPh4eEBpVIJPz8/jBs3Dn/88Yepd4GI6gHDCBGZzbp169CiRQvExsYiMTHxnrcnCAKmTp2KRx99FBkZGYiIiMCKFSswc+ZMXLp0CUOGDMHBgwdNUDkR1ScrqQsgosYpKSkJBw8exObNm/Gf//wH69atQ2Rk5D1tc8mSJVi7di3mzJmDTz/9FDKZTP/aW2+9hf/+97+wsuJ/a0QNDXtGiMgs1q1bB1dXV4wYMQKPP/441q1bd0/bKyoqQlRUFDp06KA/hXOrSZMmISQk5J4+h4jqH8MIEZnFunXr8Oijj0KpVOKJJ57AhQsXcOTIkbve3oEDB3Dz5k1MnDgRCoXChJUSkdQYRojI5OLi4nD27FlMmDABANCvXz/4+/vfU+/ImTNnAABdunQxSY1EZDkYRojI5NatWwdvb28MHjwYACCTyTB+/Hhs2LABWq32rrapVqsBAI6Ojiark4gsA8MIEZmUVqvFhg0bMHjwYCQlJSExMRGJiYkIDQ1FRkYGYmJi6rS9irEhTk5OAIC8vDyT10xE0mIYISKT+uOPP5CWloYNGzagbdu2+se4ceMAQH+qxsbGBoA4MNWYwsJCg3YdOnQAAJw4ccKs9RNR/eM1cERkUuvWrYOXlxeWLVtW7bXNmzdjy5YtWLFiBTw9PWFnZ4dz584Z3c65c+dgZ2cHDw8PAOK4E1dXV/z444948803OYiVqBFhzwgRmUxRURE2b96Mhx9+GI8//ni1x6xZs5CXl4dt27ZBoVBg6NCh+O2335CcnGywneTkZPz2228YOnSoPnTY2dnh9ddfx5kzZ/D6669DEIRqn//DDz8gNja2XvaViExHJhj7F01EdBc2btyICRMmYOvWrRg1alS113U6HXx8fHD//fdj27ZtOHPmDO6//35YW1vj+eefR4sWLXD58mWsXLkSpaWlOHz4MDp27Gjw/qeffhr//e9/0aNHDzz++OPw8fFBeno6tm7ditjYWBw8eBC9e/euz90monvEMEJEJvPII48gOjoaWVlZsLOzM9pm6tSpWLduHdLS0uDu7o6zZ89i4cKF+PPPP3Hz5k24ubnhgQceQGRkpH6cyK1++eUXrFy5EkePHoVarYanpycGDBiA6dOnY+DAgebcRSIyA4YRIiIikhTHjBAREZGkGEaIiIhIUgwjREREJCmGESIiIpIUwwgRERFJimGEiIiIJNXkpoPX6XRITU2Fo6Oj/gZcREREdGeCICAvLw9+fn6Qy03Xn9HkwkhqaioCAgKkLoOIiKjBSklJgb+/v8m21+TCiKOjIwDxQFbckpyIiIjuTK1WIyAgQP9daipNLoxUnJpxcnJiGCEiIroLph7mwAGsREREJKkm1zNCRETUmJRpdcguLEVWgQZZ+SXIzBd/ZhVokJlXgteHd4CbvVLqMm+LYYSIiMiCCIKAfE1ZZaCoGjDyNcgsEH+Kr5cgu7AEt7vl7eQ+zRlGiIiImrqSMp3RngtxufJ5RdgoKdPVafsyGeBqp4S7vRLuDkq4O6jgYa+Eh4PK4oMIwDBCRERUZzqdgNyiUn3PRdVejKxqYUMDdXFZnT/DXqmAu4NKDBf2Kng4KPXP3R3EoFGx7GpnDStFwx0GyjBCREQEoKhEK/ZaVDkNklmlxyKroEQfNm4WlKBMd5tzI0Yo5LLynovyYFH+3N1BCY/ygOHuoNL3btgpm85XdNPZUyIialJuN7Cz4vSIGD7E5cISbZ0/w8nGyqCHoiJQeBoECzF8ONlYQy7nzN/GMIwQEVGDoSnTIiu/BDfyNOIjX4PMPA0y72JgpzFKK7k41sJRdduei4qxGEqrhntqxJIwjBARkaS0OgFZBZrKgJEnjr2oCBs38or1y7lFpXXatkwGuNkZGWtRNWhUec1BZcX7lkmAYYSIiExOEATkFJaWhwmx56Jq2Ki6Pqugbj0Y1goZPBxU8HRUwdNBBQ8HFTwcKwOFp4NKHzRc7ZRQ8NSIxWMYISKiWqmY/8Kw96K4/FRJiT5g3MgTx2GUamufMOQywM2+PGA4imMsKsKGwU9HFZxtrdl70cgwjBARNXHFpVqD3orKsFG9F6O4tG7zX7jYWet7LyrChBg2DEOGmz17MJoyhhEiokaoYpItsceiuPpYjCphI09TtzkwHFRWlT0XVU6V3Bo23B2UUFkpzLSH1JgwjBARNRBanYDswpI79l7cyNMgu7BuAz2VVnKDUyEG4cJBBU9HJTwdbODh2LTmv6D6wb9RREQWQlOmRWpOMa5lF+FqdiGu5RThasXz7CJk5GmgrcNEWwq5TN+D4eGgqjlsOKrgyKtISEIMI0RE9aSoRFseMKoGjSJcyy7E1ewiXM/T1Go7FfNcGJ4WqRjwaaNfdrVTcpItahAYRoiITCRfU4Zr2UW4llNYJWhUho/M/JI7bsPGWg5/Vzs0c7GFv6ut+NxVfO7nbAt3ByWsG/A9SIiMYRghIqql3KJS46dQyp/n1GKchoPKCv6utvqw0awicJQvu9krebqEmhyGESIiVE7SdfWWno2qgSOvFndedba1rjFo+Lvaco4MIiMkDyPLli3Dxx9/jPT0dAQFBeGLL75ASEhIje2XLl2K5cuXIzk5GR4eHnj88ccRFRUFGxubeqyaiBoaQRCQmV9SOWYju3rPRm1ulOZmrzTs2XCpPJXSzNUWTjbW9bA3RI2LpGFk48aNiIiIwIoVKxAaGoqlS5ciPDwc586dg5eXV7X269evxxtvvIE1a9agT58+OH/+PJ5++mnIZDJ8+umnEuwBEVkKnU7AjXwNrmYb9mpUhI/UnKJaTdjl6aiq1rPhX2WZl7USmZ5MEOp6T0PTCQ0NRa9evfDll18CAHQ6HQICAvDiiy/ijTfeqNZ+1qxZOHPmDGJiYvTrXn75Zfzzzz84cOBArT5TrVbD2dkZubm5cHJyMs2OEJHZaXUC0tVVLnutCBw54vPUnGKUaG8fNmQywNvRpkrQsEUzFzv9cjMXW9hYc5IuopqY6ztUsohfUlKCuLg4zJs3T79OLpcjLCwMhw4dMvqePn364IcffkBsbCxCQkJw6dIl7NixA5MmTarxczQaDTSaysvl1Gq16XaCiEwqu6AE5zPykHyzytUo5eM30nOLUXaHOTbkMsDXuTJo+LsYXo3i62zLW74TWSDJwkhmZia0Wi28vb0N1nt7e+Ps2bNG3zNx4kRkZmaiX79+EAQBZWVleOGFF/Dmm2/W+DlRUVFYtGiRSWsnonuTrynDhYw8nM/Iw7n0fPFnRh5u3GGeDWuFDL7O1cdqVAwO9XGygRUveyVqcBrUyc+9e/figw8+wFdffYXQ0FAkJiZi9uzZePfddzF//nyj75k3bx4iIiL0y2q1GgEBAfVVMlGTpinT4uL1An3YOJ8u/ryaXVTje/xdbdHSw94gcFScRvFytOHN1IgaIcnCiIeHBxQKBTIyMgzWZ2RkwMfHx+h75s+fj0mTJmHatGkAgC5duqCgoADPP/883nrrLcjl1X8jUqlUUKlUpt8BItIr0+pw5WahPmyIPR55uJxVWOP05V6OKrT3cUQ7b0e093ZEOx9HtPVygL2qQf2OREQmINm/eqVSiZ49eyImJgajR48GIA5gjYmJwaxZs4y+p7CwsFrgUCjEwWYSjsMlajIEQcC1nCLD0yvpeUi8kY+SMuODR51srNDBxwntfBzE0FH+cLVX1nP1RGSpJP0VJCIiAlOmTEFwcDBCQkKwdOlSFBQUYOrUqQCAyZMno1mzZoiKigIAjBw5Ep9++im6d++uP00zf/58jBw5Uh9KiOjeVczJURE2Kk6zXMjIR34Nt5u3tVagnbeD2NNR0ePh4wgvRxUn+SKi25I0jIwfPx43btzAggULkJ6ejm7dumHXrl36Qa3JyckGPSFvv/02ZDIZ3n77bVy7dg2enp4YOXIk3n//fal2gajByy0qxYVbxnScz8jHzQLj91GxVsjQ2vOW0OHtCH9XW96UjYjuiqTzjEiB84xQU1VUokXi9XyDMR3nM/KQlltstL1MBjR3szMIHR18HNHCw543aiNqohrdPCNEZB6lWh2SMgsqT6+U/7xysxA1/erh62xTraejjZcDbJU8/UlE5scwQtRA6XQCUrILq4zpyMf59DxcysxHqdZ46nC1s0Z7n8qrV9p7O6KttyOcbXk/FSKSDsMIkYUTBAEZas0tYzrEwaRFpcZv7GavVOjDRtUeDw8H3p6eiCwPwwiRBckuKKk2puNceh7UNdy6XmklRxtPhypXr4gDS5u52DJ0EFGDwTBC5lWQCZRpACc/cUQkARDHdZy8llvr6dAVchlauNtVmySsuZsdpz8nogaPYYTMo6wE2P8x8NcSQNAC9l6AX3fDh6P3nbfTyFzIyMNPR1Ow5dg1ZOYbv3TW39XWYExHO29HtPK0591kiajRYhgh08s4BWx5AUg/Li7L5EDBdeDCbvFRwdHvloDSDbD3kKRkc1IXl2L7v2n46WgKElJy9Otd7KzR2c/Z4PRKW29HOHA6dCJqYvi/HpmOTgsc/Bz48wNAWwLYugIjlgDthgMZJ4HUY5WPG+eAvFTgXCpw7vfKbTgHiqGkakCxdZVqj+6aTifgcFIWNh29ip0n01BcKk6VrpDL8EAHL4wLDsCg9p6cr4OICJz0TOpyGo+si2JvyNVYcbltOPDI54Cj8ZseQpMv9pxUDShZicbburY07EHxDQJsLPPP7lpOEX6Ju4pNcSlIuVl5Z9o2Xg4YF+yP0d2bwcvRRsIKiYjunrm+QxlG6N7odMCR1UD0AqCsCFA6AsOigO5P1X3AanEukPavYUDJvmy8rXvbWwJKV0Bpf8+7czeKS7XYczoDm46m4EBipn5iMQeVFUYG+WFcsD+6Bbjw6hYiavAYRkyEYcSEclKAX2cCSfvE5Rb9gdFfAS6BpvuMwptAWkKVgJIA5KZUbyeTAx7tDQOKT2fA2tZ0tVQhCAJOparx09EUbD12zeDS296t3DGulz+G3efLGUyJqFFhGDERhhETEAQgYR2wax6gUQNWtsCDi4BezwHyehgDkX/jloByDMhLq95OpgC8OhmOQfG+D7BS3fVH3ywowdZj17Ap7irOpKn16/2cbfB4T3883jMAge52d719IiJLxjBiIgwj9ygvA/htNnB+p7js3wsYvQLwaCNtXeq06gGl4Eb1dnJrMZBU7UHx6ggoap4OvUyrw18XMrEpLgXRpzP0U60rreQIv88HY3v6o28bDyh4x1oiauQYRkyEYeQenNwM/B4BFGUDCiUw+E2gz0uA3AJPRQgCoL5mGE5Sj4m130qhAny6GAYUz/ZIulmMTUdT8Ev8VWSoKycj69zMCeOCA/BIkB9c7JT1uFNERNLiXXtJOoU3gR2vACd/EZd9ugBjvhZ7GCyVTAY4+4uPjiPFdYIA5Fy5JaD8C2hygWtHxUe5YpkKmdrmcNe1xv26lrhi2w7duwdjbHBzdPJjiCUiMiX2jNDtnd8NbHsRyM8Qx2D0fxkY8Cpg1Uh6BHQ6CDcvIen4AVw99TdsM0+gI5LgICuu3lbpWD7+pFtlD4prS05zT0SWqSgHuBYHtH7AZP9P8TSNiTCM1FKxGtg9Dzj2g7js0U4cG+LfU9q6TChDXYzN8dew6WgKLmUW6Ne3clNhWicBD7mnwyWnfLK2tOPipcu3snGuPs29cwADChHVL0EAcpKB5MNAymHx5/UzAARg9nHAtblJPoanaaj+JO0Hts4EcpMByIDeM4EH3jbbZbL1qaRMhz/OZuCno1ex99x16MqjuK21AiO6+mJccAB6tXCtPieItgzIPGd4iif9pDg3yqW94qOCnbuR+/D4MqAQkeloy8SZrauGD2NXFbq1Enu2TRRGzIU9I1SppBCIWQT8s0JcdmkuzhvSop+0dZnA2XQ1Nh29ii3HruFmQeUN6oKbu2JssD9GdPWr+z1hykqAG2cMA0rGaUBXWr2tg3f1gOLgdY97RURNhiYPuHq0MnxcPQqU5Bu2kVuJM1QH9gYCQoHA+03+/wxP05gIw0gNUo4AW1+onJK951Rg6LuAylHauu5BblEptv2bik1HU3D8aq5+vaejCo/18MfYYH+09nQw7YeWFgPXTxlO0nb9jHjn4ls5NQN8uzX6GwUS0V1Qp4rBoyJ8pJ8ABJ1hG5UTEBAiho6A+4FmPQGleec5kjyMpKamws/Pz2QfLBWGkVuUaYC9i4G/l4p/0R19gUe+BNqGSV3ZXdHpBBy6lIWfjqZg18l0aMrEf7xWchnCOnpjbLA/BrbzhFV93qCupPCWGwUmADfOAjDyT885QAwlVUOKnVv91UpE9U+nE/9PSD4EpPwj/sxJrt7OORAIDK0MH14d631qBcnDiKurK5YtW4aJEyea7MOlwDBSRfoJ8eZ2GSfF5a7jgeEfNsi75KbcLMTPcVfxc9xVXMupHGjaztsB44IDMLp7M3g43P3Mqyanv1FgQvkA2QQg8wKMBhSXwCr34OnWYO9kTETlSouAa/GV4SPlH3H8WVUyOeDdWQweFeHDuZk09VYheRj56quv8Prrr2PYsGH4+uuv4ebWMH9bYxiBOPDp7/8D9n4ojm+w8wAe/j+g0yNSV1YnxaVa7D6Vjp+OpuDvxCz9ekcbKzwS5IdxwQHo6u/ccG5QV6yucifjBPHnzYvG27q2NLzE2DdIvLKHiCxPQWb5KZfy8JGaUH1smbU94B9cGT6aBVvk3cklDyMAkJSUhGeffRanT5/GqlWrMHLkSJMVUl+afBi5cR7Y8h8gNV5c7vAw8PBSwMFT0rJqSxAEHL+ai5+OpmDbv6nIq3KDur5t3DEuOADh9/nAxtoCZ4W9G0U5VQJKeUjJTjLe1q21YUDx6WqR/5kRNWqCII69qzreo2IsXlUOPpXBI/B+wLsLoLD8C1wtIoxU+PLLLzF37lx07NgRVlaGBy8+Pt5kxZlDkw0jOp14lUzMIqCsGFA5Aw99JJ6aaQA9B1n5Gmw5dg2bjl7FuYw8/fpmLrYYG+yPx3r4I8CtidygrigbSPvX8CoeY+eXIQPc21QOjq0IKCoTD9olasrKSsTTrFXDR2FW9XaeHQ3Dh0vzBvF/760sZp6RK1euYPPmzXB1dcWoUaOqhRGyQNmXxXlDrhwQl1s/IA5StYDzj7dTptVh3/kb+OloCmLOXEdZ+aQgKis5hnX2wbjgAPRu5Q55U7tBna0r0GqQ+KhQeLMymKQliD0ouSlA1gXxceKn8oYycQK7qlfw+HQBlPb1vRdEDVNRNpASWxk+UuPFX/CqUqjEK1sqgod/Lw5Ev4M6JYlVq1bh5ZdfRlhYGE6dOgVPz4bRtd9kCQIQ/x2w+y3xenRre/Fy3eBnLDqRX7yRj01Hr+KX+Ku4kVd5g7ogf2eMDQ7AyCA/ONvWfJfdJsnODWgzRHxUKMisHHtSEVLU18TJ2zLPAcc3iO1kcsCzg+EVPD6dG8Ukd0T3pOJ+VhXBI/mwOLfQrWzdxLk9KsKHbxBgZUED5huAWp+mGTZsGGJjY7F06VJMnjzZ3HWZTZM5TaNOE+8pkxgtLgf2Ficwc2slbV01yNeU4ffjqfjp6FXEXam8s66bvRJjujfD2GB/dPBpxH9e9SUvo7LnpCKgGJu1UaYQLxvUX2bcQ7wxorVNvZZLVK+0ZUDGCSC5/PLa5MNAfnr1du5txKtbKsKHexuL/gXPlCQ/TaPVanH8+HH4+/ub7MPJDAQBOPGzeJfd4hyxu3DIfOD+GfV+PfqdCIKA2KSb+OnoVew4kYaiUnFiMLkMGNzeC2ODA/BABy8orepxTpDGztEbcAwH2oVXrlOnGQaU1GNAwXXxku+Mk5X3J5JblQeU7pW9KN738TdAcxAEoLRQHMBcnFP5s0wDKB3EcT9KB/H0mspRfG5t22S+EE1GkwdcPVLZ63H1KFBaYNhGbi2G8oDQyplNG8iA/4aEM7A2JgWZwPa5wJlt4rJvN2DM14BXB0nLulVabpH+BnWXswr161t52GNscAAe7dEM3k78DVwygiDO/piWYHiZcWFm9bZya8C7k+E8KF6dGs9dne+FIIinR28NFMW51dcZ+2nstgK3I1MYBhX9T8cqy/bi3acN2tSw3BhDZu61yvu4JB8Ww3a1WU2dK2c1Dbxf7BU086ymDYlFXU3TkDXaMHL2d+C32UDBDfE32AGvAf0jAIVljK3QlGnxv9PXsSkuBfvP39DfoM5eqcDDXf0wrpc/egQauUEdWQZBAHKvVgko5SGl6Gb1tgqlOFlT1cuMPTtYzN/FOhEE8bfnOwUHYz+LcwFdmdHN1ppMAdi6ADYu4jwy1rZiwNHkV/689Td5U5FbG/a81CbAKO2NBKDy5fr+89fpxPEdyYfKT7scLr/55y1cAg1PuXh2BOTsja0Jw4iJNLowUpQD7HoD+PdHcdmzIzBmhfhFYAHOpefhx9hkbE24hpzCyt/0Qlq4YWywPx7q4gv7ut6gjixDxS3LDa7iOVZ9JklAPF3o08XwMmOP9vUzr4JOB2jUdx8obv3Nua7kVmKYqAgVdfmpdLjzqRedTgwm+pCSB5QUVAksebcEmFuWbw03ZUW3/7y7pVDd0ltjf5twc2ugKW9f9bVbTzuXFIpXtlSEj5RYQGNkVlOfLobhw6nh3+akPjGMmEijCiMX/wB+nSVeIQEZ0PclYPBbFtG9mldcik92n8P3h6+g4m+Yt5MKj/f0x+M9A9DSg5eSNkqCIE7KZnAVz/HqXwoAYGVbJaCUhxSPdsbHNum0YjCoMTjc5jWN+t4DhUJ594HC2q5hjeXQlhkJKXcIMCV5NSwXAFrNnT/zbljZVoYTK5U4sditPVHW9kBAr8rw4R/coG/+aQkYRkykUYQRTT4QvQA4+o247NYKGL1CvIGSBdhzKh0Lfj2FdLV47f3QTt54IjQQA9p6QtHU5gQh8Tf37CTD0ztpCdVvfw6IX9w+XcUxJ/pAkSsGCmP37akLK5s6BAlnw3UcHHr3ykruHG4qlvU9OreGmyptbnfqy9G38j4ugfeLpwsbwKymDQnDiIk0+DBy5RCwdXrllOAhzwNhCy1i0qoMdTEWbjuFnSfFS+EC3ezwwZgu6NfWQ+LKyOLodOJ9dwwCyr93Hv9gbVf7AHHrT16W3PAJgnhFkT6cFFSGGPc24vgPhkazkvzSXpJYaTHw53vAwS8BCICTPzDqS6D1YKkrg04n4McjyVi88yzyisugkMvw/IBWeOmBtrBVWtblxGQh5HLAo6346DpOXKfTil3taccBCMbDhgWcgiQJyWRiqLS2Aez5S05jYhFDhpctW4YWLVrAxsYGoaGhiI2NrbHtoEGDIJPJqj1GjBhRjxXXs9RjwMqBwMEvAAhAt6eAGQctIohcyMjDuK8P4a0tJ5FXXIYgf2f8NqsfXh/WgUGE6kauADzbA13HigGl3VDxEkvPdoCDF4MIUSMmec/Ixo0bERERgRUrViA0NBRLly5FeHg4zp07By8vr2rtN2/ejJKSEv1yVlYWgoKCMHbs2Posu35oS4G/lgD7PxbPk9p7ASM/Azo8JHVl0JRpsezPi1i+NxGlWgF2SgVeGdoeU/q04LgQIiKqE8nHjISGhqJXr1748ssvAQA6nQ4BAQF48cUX8cYbb9zx/UuXLsWCBQuQlpYGe/s7j5toMGNGrp8BtvxHPI8OAJ1GAyM+BezdJS0LAGKTbuKNzcdx6YZ4fv+BDl54d3RnNHPhvUyIiBqzRjlmpKSkBHFxcZg3b55+nVwuR1hYGA4dOlSrbXzzzTeYMGFCjUFEo9FAo6m8tEytVt9b0eam0wKHlgF/vCdeEmfrCoxYAnR+TOrKkFtUisU7z+LHWHHiIA8HFRY9ch8e6uLDycqIiOiuSRpGMjMzodVq4e3tbbDe29sbZ8+eveP7Y2NjcfLkSXzzzTc1tomKisKiRYvuudZ6kXUR2DpDnK4YANqGA498Djj6SFqWIAjYcSIdC387pb+L7hMhAXhjWEc42zXAWTWJiMiiSD5m5F5888036NKlC0JCQmpsM2/ePEREROiX1Wo1AgIC6qO82hMEcc6QPfPFm2MpHYFhUUD3pyS/TC01pwjzt55EzNnrAIBWnvaIGtMFoa2kP11ERESNg6RhxMPDAwqFAhkZGQbrMzIy4ONz+96AgoICbNiwAe+8885t26lUKqhUFjwKP/eqOIvqpT/F5Rb9gVHLANfmkpal1Qn4/tBlfLL7HApKtLBWyDB9UBvMGNQaNta8SoaIiExH0jCiVCrRs2dPxMTEYPTo0QDEAawxMTGYNWvWbd+7adMmaDQaPPXUU/VQqRkIgng/mZ2vi7NLWtkCDy4Cej0n+U2azqSp8cbmE/g3JQcA0LO5KxY/2gVtvTmNMhERmZ7kp2kiIiIwZcoUBAcHIyQkBEuXLkVBQQGmTp0KAJg8eTKaNWuGqKgog/d98803GD16NNzdG+DpgvzrwG9zgHO/i8v+vcTp3D3aSFpWcakWn8VcwKr9l1CmE+CossLrwztgYkgg5Lxcl4iIzETyMDJ+/HjcuHEDCxYsQHp6Orp164Zdu3bpB7UmJydDfktPwblz53DgwAHs2bNHipLvzamtwPa54q3X5dbA4DeBPi9Jfv+EvxMz8eaWE7iSVQgAGHafDxaNug/eTpxCm4iIzEvyeUbqm2TzjBTeBHa+BpzYJC57dwHGrAB8OtdfDUZkF5Tgvd/P4Jf4qwAAHycbLBp1H8Lvk/YKHiIisjyNcp6RJuNCNLDtRSAvDZApgP4RwIDXxDuTSkQQBPyakIp3tp/GzYISyGTApPub49Xw9nC04eW6RERUfxhGzEmTB+x+C4j/Tlz2aCeODfHvKWlZKTcL8dbWk9h//gYAoJ23A6Ie7YqezV0lrYuIiJomhhFzuXwA2DodyEkGIAPunwEMmQ9YSzdleplWhzV/J+HT6PMoLtVBaSXHSw+0wfMDWkNpZRH3TCQioiaIYcTUSouAmHeAw1+Jyy6BwOjlQIt+kpZ14mou3th8HKdSxenw72/lhg/GdEErTwdJ6yIiImIYMaWrceLN7bIuiMs9nwaGvgeopJufo0BThv+LPo81fydBJwDOttZ466GOGBvsz/vJEBGRRWAYMYWyEmDfh8CBTwFBBzj6Ao98CbQNk7SsP89dx9tbTuJaThEA4JEgP8x/uBM8HS14RloiImpyGEbulSAA/x0DXDkgLncZBzz0kXi3XYncyNPg3e2nse3fVABAMxdbvDe6MwZ38JKsJiIiopowjNwrmQzoNhG4cQZ4+P+ATqMkK0UQBGw6ehXv7ziD3KJSyGXA1L4tEfFgO9ir+EdNRESWid9QptBtItDhIUl7Q5IyCzBv83EcvnQTANDJ1wmLH+uCrv4uktVERERUGwwjpiCTSRZESsp0WPXXJXwWcwElZTrYWMsxN6wdnu3XElYKXq5LRESWj2GkAYtPzsa8X07gXEYeAKB/Ww+8P7oLAt3tJK6MiIio9hhGGqC84lJ8vPsc/nv4CgQBcLNXYsHDnTCqmx8v1yUiogaHYaSB2XMqHQt+PYV0dTEA4NEezfD2iE5ws5fuPjdERET3gmGkgchQF2PhtlPYeTIdABDoZocPxnRBv7YeEldGRER0bxhGLJxOJ2B9bDI+3HkWeZoyKOQyPD+gFV56oC1slQqpyyMiIrpnDCMW7EJGHuZtPoGjV7IBAEH+zoh6tCs6+TlJXBkREZHpMIxYIE2ZFsv+vIjlexNRqhVgp1Tg1fD2mNy7BRRyDlAlIqLGhWHEwvxzKQvztpzApRsFAIAHOnjh3dGd0czFVuLKiIiIzINhxELkFpZi8a4z+DE2BQDg4aDCokfuw0NdfHi5LhERNWoMIxITBAE7TqQjctspZOZrAABPhATgjWEd4WxnLXF1RERE5scwIqHUnCLM33oSMWevAwBaedojakwXhLZyl7gyIiKi+sMwIgGtTsD3hy7jk93nUFCihbVChumD2mDGoNawseblukRE1LQwjNSz06lqzNt8HP9ezQUA9GzuisWPdkFbb0eJKyMiIpIGw0g9KS7V4rOYC1i5/xK0OgGOKiu8PrwDJoYEQs7LdYmIqAljGKkHBy5k4q2tJ3AlqxAAMOw+HywadR+8nWwkroyIiEh6DCNmdLOgBO//fga/xF8FAPg42WDRqPsQfp+PxJURERFZDoYRMxAEAVsTruHd7Wdws6AEMhkw6f7meDW8PRxteLkuERFRVQwjJpacVYi3tp7AXxcyAQDtvB0Q9WhX9GzuKnFlRERElolhxETKtDp8cyAJ//e/8ygu1UFpJcdLD7TB8wNaQ2kll7o8IiIii8UwYgLHr+bgjV9O4HSaGgBwfys3fDCmC1p5OkhcGRERkeVjGLlHgiAgctspnE5Tw9nWGm891BFjg/15PxkiIqJaYhi5RzKZDO+N7oyV+y/h7RGd4OmokrokIiKiBoVhxATu83PGZxO6S10GERFRg8SRlURERCQphhEiIiKSVJM7TSMIAgBArVZLXAkREVHDUvHdWfFdaipNLozk5eUBAAICAiSuhIiIqGHKy8uDs7OzybYnE0wdbyycTqdDamoqHB0dTXb5rVqtRkBAAFJSUuDk5GSSbTYWPDbG8bjUjMfGOB6XmvHYGGeO4yIIAvLy8uDn5we53HQjPZpcz4hcLoe/v79Ztu3k5MR/CDXgsTGOx6VmPDbG8bjUjMfGOFMfF1P2iFTgAFYiIiKSFMMIERERSYphxARUKhUiIyOhUnH21Vvx2BjH41IzHhvjeFxqxmNjXEM6Lk1uACsRERFZFvaMEBERkaQYRoiIiEhSDCNEREQkKYYRIiIikhTDiAksW7YMLVq0gI2NDUJDQxEbGyt1SZLbv38/Ro4cCT8/P8hkMmzdulXqkixCVFQUevXqBUdHR3h5eWH06NE4d+6c1GVJbvny5ejatat+cqbevXtj586dUpdlkRYvXgyZTIY5c+ZIXYqkFi5cCJlMZvDo0KGD1GVZjGvXruGpp56Cu7s7bG1t0aVLFxw9elTqsmrEMHKPNm7ciIiICERGRiI+Ph5BQUEIDw/H9evXpS5NUgUFBQgKCsKyZcukLsWi7Nu3DzNnzsThw4cRHR2N0tJSDB06FAUFBVKXJil/f38sXrwYcXFxOHr0KB544AGMGjUKp06dkro0i3LkyBF8/fXX6Nq1q9SlWIT77rsPaWlp+seBAwekLskiZGdno2/fvrC2tsbOnTtx+vRpLFmyBK6urlKXVjOB7klISIgwc+ZM/bJWqxX8/PyEqKgoCauyLACELVu2SF2GRbp+/boAQNi3b5/UpVgcV1dXYfXq1VKXYTHy8vKEtm3bCtHR0cLAgQOF2bNnS12SpCIjI4WgoCCpy7BIr7/+utCvXz+py6gT9ozcg5KSEsTFxSEsLEy/Ti6XIywsDIcOHZKwMmoocnNzAQBubm4SV2I5tFotNmzYgIKCAvTu3VvqcizGzJkzMWLECIP/b5q6CxcuwM/PD61atcKTTz6J5ORkqUuyCNu2bUNwcDDGjh0LLy8vdO/eHatWrZK6rNtiGLkHmZmZ0Gq18Pb2Nljv7e2N9PR0iaqihkKn02HOnDno27cvOnfuLHU5kjtx4gQcHBygUqnwwgsvYMuWLejUqZPUZVmEDRs2ID4+HlFRUVKXYjFCQ0Oxdu1a7Nq1C8uXL0dSUhL69++PvLw8qUuT3KVLl7B8+XK0bdsWu3fvxvTp0/HSSy/hu+++k7q0GjW5u/YSWYqZM2fi5MmTPM9drn379khISEBubi5+/vlnTJkyBfv27WvygSQlJQWzZ89GdHQ0bGxspC7HYgwfPlz/vGvXrggNDUXz5s3x008/4dlnn5WwMunpdDoEBwfjgw8+AAB0794dJ0+exIoVKzBlyhSJqzOOPSP3wMPDAwqFAhkZGQbrMzIy4OPjI1FV1BDMmjUL27dvx59//gl/f3+py7EISqUSbdq0Qc+ePREVFYWgoCB89tlnUpclubi4OFy/fh09evSAlZUVrKyssG/fPnz++eewsrKCVquVukSL4OLignbt2iExMVHqUiTn6+tbLcR37NjRok9jMYzcA6VSiZ49eyImJka/TqfTISYmhue6yShBEDBr1ixs2bIFf/zxB1q2bCl1SRZLp9NBo9FIXYbkhgwZghMnTiAhIUH/CA4OxpNPPomEhAQoFAqpS7QI+fn5uHjxInx9faUuRXJ9+/atNmXA+fPn0bx5c4kqujOeprlHERERmDJlCoKDgxESEoKlS5eioKAAU6dOlbo0SeXn5xv8hpKUlISEhAS4ubkhMDBQwsqkNXPmTKxfvx6//vorHB0d9WOLnJ2dYWtrK3F10pk3bx6GDx+OwMBA5OXlYf369di7dy92794tdWmSc3R0rDamyN7eHu7u7k16rNErr7yCkSNHonnz5khNTUVkZCQUCgWeeOIJqUuT3Ny5c9GnTx988MEHGDduHGJjY7Fy5UqsXLlS6tJqJvXlPI3BF198IQQGBgpKpVIICQkRDh8+LHVJkvvzzz8FANUeU6ZMkbo0SRk7JgCEb7/9VurSJPXMM88IzZs3F5RKpeDp6SkMGTJE2LNnj9RlWSxe2isI48ePF3x9fQWlUik0a9ZMGD9+vJCYmCh1WRbjt99+Ezp37iyoVCqhQ4cOwsqVK6Uu6bZkgiAIEuUgIiIiIo4ZISIiImkxjBAREZGkGEaIiIhIUgwjREREJCmGESIiIpIUwwgRERFJimGEiIiIJMUwQkRERJJiGCEiSWm1WvTp0wePPvqowfrc3FwEBATgrbfekqgyIqovnIGViCR3/vx5dOvWDatWrcKTTz4JAJg8eTL+/fdfHDlyBEqlUuIKicicGEaIyCJ8/vnnWLhwIU6dOoXY2FiMHTsWR44cQVBQkNSlEZGZMYwQkUUQBAEPPPAAFAoFTpw4gRdffBFvv/221GURUT1gGCEii3H27Fl07NgRXbp0QXx8PKysrKQuiYjqAQewEpHFWLNmDezs7JCUlISrV69KXQ4R1RP2jBCRRTh48CAGDhyIPXv24L333gMA/O9//4NMJpO4MiIyN/aMEJHkCgsL8fTTT2P69OkYPHgwvvnmG8TGxmLFihVSl0ZE9YA9I0QkudmzZ2PHjh34999/YWdnBwD4+uuv8corr+DEiRNo0aKFtAUSkVkxjBCRpPbt24chQ4Zg79696Nevn8Fr4eHhKCsr4+kaokaOYYSIiIgkxTEjREREJCmGESIiIpIUwwgRERFJimGEiIiIJMUwQkRERJJiGCEiIiJJMYwQERGRpBhGiIiISFIMI0RERCQphhEiIiKSFMMIERERSYphhIiIiCT1//m18DFMXP8MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 600x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1358505487442017, 'auc': 0.7748055458068848}\n",
      "============val_data===================\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "     Android_Adware     0.4762    0.9364    0.6314     13905\n",
      "Android_SMS_Malware     0.9001    0.3229    0.4753      6141\n",
      "  Android_Scareware     0.2958    0.0502    0.0858     10823\n",
      "             Benign     0.1307    0.1001    0.1134      2187\n",
      "\n",
      "           accuracy                         0.4769     33056\n",
      "          macro avg     0.4507    0.3524    0.3265     33056\n",
      "       weighted avg     0.4730    0.4769    0.3895     33056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = dt.evaluate(x_test,y_test)\n",
    "print(result)\n",
    "\n",
    "#scoring\n",
    "preds = dt.predict(x_test)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(6, 6))\n",
    "axes[0].plot(history.history['loss'], label='train_loss')\n",
    "axes[0].plot(history.history['val_loss'], label='val_loss')\n",
    "axes[0].set_title('Loss')\n",
    "# axes[1].plot(history.history['accuracy'], label='train_accuracy')\n",
    "# axes[1].plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "# axes[1].set_title('Accuracy')\n",
    "axes[2].plot(history.history['AUC'], label='train_AUC')\n",
    "axes[2].plot(history.history['val_AUC'], label='val_AUC')\n",
    "axes[2].set_title('AUC')\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=0.5)\n",
    "fig.suptitle('Loss and Accuracy')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()\n",
    "\n",
    "print(result)\n",
    "print('============val_data===================')\n",
    "print(classification_report(y_test, preds, target_names=names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8411cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(\n",
    "    nets=['autoint_nets','fm_nets'],\n",
    "    categorical_columns=categorical_columns,\n",
    "    auto_discrete=True,\n",
    "    auto_categorize=True,\n",
    "    metrics=['AUC'],\n",
    "    dnn_params={\n",
    "        'hidden_units':((300, 0.3, True),(300, 0.3, True)),\n",
    "        'use_bn': True,\n",
    "        'learning_rate': 0.01,\n",
    "        'activation': 'relu',\n",
    "    },\n",
    "    earlystopping_patience=5,\n",
    "    apply_gbm_features=True,\n",
    "    embeddings_output_dim=100,\n",
    "    autoint_params={\n",
    "        'num_attention': 3,\n",
    "        'num_heads': 1,\n",
    "        'dropout_rate': 0.2,\n",
    "        'use_residual': True\n",
    "    },\n",
    "    stacking_op='add',\n",
    ")\n",
    "\n",
    "dt = deeptable.DeepTable(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4fc1e92e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-20 13:01:38 I deeptables.m.deeptable.py 338 - X.Shape=(264440, 80), y.Shape=(264440,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fm_nets', 'autoint_nets'], categorical_columns=['Source Port', 'Destination Port', 'Protocol', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC'], auto_categorize=True, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=True, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((300, 0.3, True), (300, 0.3, True)), 'use_bn': True, 'learning_rate': 0.01, 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0.2, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-20 13:01:38 I deeptables.m.deeptable.py 339 - metrics:['AUC']\n",
      "04-20 13:01:38 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-20 13:01:38 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-20 13:01:39 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.34906625747680664s\n",
      "04-20 13:01:39 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-20 13:01:39 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5699803829193115s\n",
      "04-20 13:01:39 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-20 13:01:39 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.17852234840393066s\n",
      "04-20 13:01:39 I deeptables.m.preprocessor.py 398 - Data discretization...\n",
      "04-20 13:01:39 I hypernets.t.sklearn_ex.py 716 - 66 variables to discrete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-20 13:01:41 I deeptables.m.preprocessor.py 404 - Discretization taken 2.0211267471313477s\n",
      "04-20 13:01:41 I deeptables.m.preprocessor.py 423 - Extracting GBM features...\n",
      "04-20 13:01:42 I hypernets.t.sklearn_ex.py 640 - LightGBM task:multiclass\n",
      "04-20 13:01:45 I deeptables.m.preprocessor.py 434 - Extracting gbm features taken 3.9500479698181152s\n",
      "04-20 13:01:50 I deeptables.m.preprocessor.py 196 - fit_transform taken 11.781244039535522s\n",
      "04-20 13:01:50 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-20 13:01:51 I deeptables.m.preprocessor.py 249 - transform_X taken 0.6338100433349609s\n",
      "04-20 13:01:51 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-20 13:01:51 I deeptables.m.preprocessor.py 236 - transform_y taken 0.001994609832763672s\n",
      "04-20 13:01:51 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-20 13:01:51 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-20 13:01:51 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-20 13:01:52 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-20 13:01:52 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-20 13:01:53 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (174)', 'input_continuous_all: (66)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [48234, 16794, 5, 4, 9, 10, 23, 5, 5, 11, 13, 8, 5, 14, 15, 8, 6, 14, 15, 21, 23, 23, 20, 23, 20, 22, 22, 20, 22, 19, 18, 19, 18, 18, 13, 7, 6, 23, 22, 5, 8, 16, 16, 16, 3, 3, 3, 3, 3, 4, 15, 13, 14, 7, 5, 11, 5, 13, 10, 9, 4, 4, 13, 7, 13, 13, 14, 8, 14, 14, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 17466)\n",
      "---------------------------------------------------------\n",
      "nets: ['fm_nets', 'autoint_nets']\n",
      "---------------------------------------------------------\n",
      "fm: input_shape (None, 174, 100), output_shape (None, 1)\n",
      "autoint: input_shape (None, 174, 100), output_shape (None, 17400)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-20 13:01:53 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n",
      "258/258 [==============================] - 73s 199ms/step - loss: 2.4959 - auc: 0.6123 - val_loss: 2.9690 - val_auc: 0.5447\n",
      "Epoch 2/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 2.1246 - auc: 0.6826 - val_loss: 1.6265 - val_auc: 0.5927\n",
      "Epoch 3/100\n",
      "258/258 [==============================] - 50s 196ms/step - loss: 1.9782 - auc: 0.7299 - val_loss: 1.2942 - val_auc: 0.6832\n",
      "Epoch 4/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 1.8695 - auc: 0.7556 - val_loss: 1.4281 - val_auc: 0.6595\n",
      "Epoch 5/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 1.7739 - auc: 0.7785 - val_loss: 1.3517 - val_auc: 0.7006\n",
      "Epoch 6/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 1.6879 - auc: 0.8010 - val_loss: 1.3681 - val_auc: 0.7140\n",
      "Epoch 7/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 1.6303 - auc: 0.8189 - val_loss: 1.5043 - val_auc: 0.7252\n",
      "Epoch 8/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 1.5793 - auc: 0.8341 - val_loss: 1.6062 - val_auc: 0.7291\n",
      "Epoch 9/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 1.5279 - auc: 0.8462 - val_loss: 1.8997 - val_auc: 0.7374\n",
      "Epoch 10/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 1.4809 - auc: 0.8562 - val_loss: 2.3548 - val_auc: 0.7388\n",
      "Epoch 11/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 1.4346 - auc: 0.8664 - val_loss: 2.4041 - val_auc: 0.7436\n",
      "Epoch 12/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 1.3827 - auc: 0.8768 - val_loss: 2.6676 - val_auc: 0.7488\n",
      "Epoch 13/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 1.3368 - auc: 0.8851 - val_loss: 3.1561 - val_auc: 0.7538\n",
      "Epoch 14/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 1.2974 - auc: 0.8927 - val_loss: 3.0063 - val_auc: 0.7590\n",
      "Epoch 15/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 1.2557 - auc: 0.9004 - val_loss: 3.4250 - val_auc: 0.7638\n",
      "Epoch 16/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 1.2214 - auc: 0.9069 - val_loss: 3.5331 - val_auc: 0.7642\n",
      "Epoch 17/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 1.1879 - auc: 0.9124 - val_loss: 3.6385 - val_auc: 0.7690\n",
      "Epoch 18/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 1.1592 - auc: 0.9176 - val_loss: 3.8694 - val_auc: 0.7682\n",
      "Epoch 19/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 1.1294 - auc: 0.9225 - val_loss: 3.8035 - val_auc: 0.7697\n",
      "Epoch 20/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 1.0986 - auc: 0.9270 - val_loss: 3.8873 - val_auc: 0.7752\n",
      "Epoch 21/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 1.0777 - auc: 0.9308 - val_loss: 3.8858 - val_auc: 0.7744\n",
      "Epoch 22/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 1.0557 - auc: 0.9341 - val_loss: 3.9261 - val_auc: 0.7761\n",
      "Epoch 23/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 1.0272 - auc: 0.9378 - val_loss: 3.8206 - val_auc: 0.7781\n",
      "Epoch 24/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 1.0020 - auc: 0.9406 - val_loss: 4.0540 - val_auc: 0.7813\n",
      "Epoch 25/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 0.9768 - auc: 0.9436 - val_loss: 4.1722 - val_auc: 0.7819\n",
      "Epoch 26/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 0.9562 - auc: 0.9461 - val_loss: 4.3047 - val_auc: 0.7809\n",
      "Epoch 27/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 0.9373 - auc: 0.9481 - val_loss: 4.3598 - val_auc: 0.7841\n",
      "Epoch 28/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 0.9207 - auc: 0.9498 - val_loss: 4.4313 - val_auc: 0.7832\n",
      "Epoch 29/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 0.8999 - auc: 0.9518 - val_loss: 4.8102 - val_auc: 0.7818\n",
      "Epoch 30/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 0.8796 - auc: 0.9536 - val_loss: 4.7018 - val_auc: 0.7837\n",
      "Epoch 31/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 0.8618 - auc: 0.9550 - val_loss: 5.0616 - val_auc: 0.7861\n",
      "Epoch 32/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 0.8533 - auc: 0.9561 - val_loss: 4.8479 - val_auc: 0.7865\n",
      "Epoch 33/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 0.8343 - auc: 0.9576 - val_loss: 5.0513 - val_auc: 0.7872\n",
      "Epoch 34/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 0.8147 - auc: 0.9590 - val_loss: 5.4562 - val_auc: 0.7861\n",
      "Epoch 35/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 0.8094 - auc: 0.9597 - val_loss: 5.2953 - val_auc: 0.7890\n",
      "Epoch 36/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 0.7963 - auc: 0.9608 - val_loss: 5.6079 - val_auc: 0.7866\n",
      "Epoch 37/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 0.7829 - auc: 0.9617 - val_loss: 5.7216 - val_auc: 0.7897\n",
      "Epoch 38/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 0.7677 - auc: 0.9629 - val_loss: 5.8281 - val_auc: 0.7873\n",
      "Epoch 39/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 0.7647 - auc: 0.9633 - val_loss: 5.8322 - val_auc: 0.7889\n",
      "Epoch 40/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 0.7517 - auc: 0.9639 - val_loss: 6.1877 - val_auc: 0.7901\n",
      "Epoch 41/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 0.7468 - auc: 0.9645 - val_loss: 6.1753 - val_auc: 0.7920\n",
      "Epoch 42/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 0.7349 - auc: 0.9654 - val_loss: 6.4364 - val_auc: 0.7944\n",
      "Epoch 43/100\n",
      "258/258 [==============================] - 50s 195ms/step - loss: 0.7244 - auc: 0.9661 - val_loss: 6.6540 - val_auc: 0.7886\n",
      "Epoch 44/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 0.7166 - auc: 0.9668 - val_loss: 6.6819 - val_auc: 0.7891\n",
      "Epoch 45/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 0.7050 - auc: 0.9675 - val_loss: 6.6537 - val_auc: 0.7933\n",
      "Epoch 46/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 0.6993 - auc: 0.9679 - val_loss: 6.8930 - val_auc: 0.7935\n",
      "Epoch 47/100\n",
      "258/258 [==============================] - 50s 194ms/step - loss: 0.6927 - auc: 0.9682 - val_loss: 7.3086 - val_auc: 0.7944\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00047: early stopping\n",
      "04-20 13:41:38 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-20 13:41:38 I deeptables.m.deeptable.py 370 - Training finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-20 13:41:38 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20230420130138_fm_nets_autoint_nets/fm_nets+autoint_nets.h5\n"
     ]
    }
   ],
   "source": [
    "model, history = dt.fit(x_train, y_train, epochs=epochs, validation_data=(x_val, y_val),class_weight=class_weight,\n",
    "                        batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8285c5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-20 13:41:38 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-20 13:41:39 I deeptables.m.preprocessor.py 249 - transform_X taken 0.6193435192108154s\n",
      "04-20 13:41:39 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-20 13:41:39 I deeptables.m.preprocessor.py 236 - transform_y taken 0.002992391586303711s\n",
      "04-20 13:41:40 I deeptables.m.deepmodel.py 158 - Performing evaluation...\n",
      "04-20 13:41:40 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=256, shuffle=False, drop_remainder=False\n",
      "{'loss': 6.319366931915283, 'auc': 0.7930574417114258}\n",
      "04-20 13:41:44 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "04-20 13:41:44 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-20 13:41:44 I deeptables.m.preprocessor.py 249 - transform_X taken 0.6004047393798828s\n",
      "04-20 13:41:44 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "04-20 13:41:44 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "04-20 13:41:49 I deeptables.m.deeptable.py 559 - predict_proba taken 5.360737085342407s\n",
      "04-20 13:41:49 I deeptables.m.deeptable.py 594 - Reverse indicators to labels.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAJJCAYAAAB4RTeNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB6+UlEQVR4nO3dd3gU1cIG8Hf7pvdKAgkdpAlIDCKKRCI2sAHiBUTRK00wVq5SVDT2j6uiKIp4ryBYABsiMQheJHSQIi0QSBASSCDZ1K3n+2N2N1mSQAK7mZT39zzzTNmzkzPHyLw5c2ZGIYQQICIiIpKJUu4KEBERUcvGMEJERESyYhghIiIiWTGMEBERkawYRoiIiEhWDCNEREQkK4YRIiIikhXDCBEREcmKYYSIiIhkxTBCRB6hUCgwZ84cuatBRE0AwwiRGyxevBgKhQLbt2+XuypN1oEDB6BQKKDX61FYWCh3dYioATGMEFGj8MUXXyAyMhIA8M0338hcGyJqSAwjRCQ7IQSWLl2K0aNH49Zbb8WSJUvkrlKtSktL5a4CUbPDMELUgHbt2oWhQ4fC398fvr6+GDx4MDZv3uxSxmw248UXX0SHDh2g1+sREhKCAQMGIC0tzVkmNzcX48ePR0xMDHQ6HaKiojBs2DAcP378oj9/z549ePDBB9G2bVvo9XpERkbioYceQkFBgUu5OXPmQKFQIDMzEw8++CACAwMREBCA8ePHo6yszKWs0WjEE088gbCwMPj5+eHOO+/EyZMn69Uuf/zxB44fP45Ro0Zh1KhR+P3332vch81mw7///W90794der0eYWFhuOWWW6pdHvviiy/Qr18/eHt7IygoCAMHDsTatWudn9c2niUuLg4PPvigc91x+W3Dhg2YNGkSwsPDERMTAwA4ceIEJk2ahE6dOsHLywshISG47777avxvUFhYiCeeeAJxcXHQ6XSIiYnB2LFjkZ+fj5KSEvj4+GDatGnVvnfy5EmoVCqkpqbWsSWJmia13BUgain279+P66+/Hv7+/njmmWeg0Wjw0Ucf4cYbb8SGDRuQkJAAQAoCqampmDBhAvr16weDwYDt27dj586duPnmmwEA99xzD/bv34+pU6ciLi4OZ86cQVpaGrKzsxEXF1drHdLS0nDs2DGMHz8ekZGR2L9/Pz7++GPs378fmzdvhkKhcCk/YsQIxMfHIzU1FTt37sQnn3yC8PBwvP76684yEyZMwBdffIHRo0ejf//+WLduHW677bZ6tc2SJUvQrl07XHPNNejWrRu8vb3x5Zdf4umnn3Yp9/DDD2Px4sUYOnQoJkyYAIvFgv/973/YvHkz+vbtCwB48cUXMWfOHPTv3x8vvfQStFottmzZgnXr1mHIkCH1qpfDpEmTEBYWhlmzZjl7RrZt24ZNmzZh1KhRiImJwfHjx/Hhhx/ixhtvxF9//QVvb28AQElJCa6//nocOHAADz30EHr37o38/Hx8//33OHnyJHr16oW77roLy5cvxzvvvAOVSuX8uV9++SWEEHjggQcuq95ETYYgoiv22WefCQBi27ZttZYZPny40Gq14ujRo85tp06dEn5+fmLgwIHObT179hS33XZbrfs5f/68ACDefPPNetezrKys2rYvv/xSABC///67c9vs2bMFAPHQQw+5lL3rrrtESEiIc3337t0CgJg0aZJLudGjRwsAYvbs2Zesk8lkEiEhIeL55593+X7Pnj1dyq1bt04AEI8//ni1fdhsNiGEEEeOHBFKpVLcddddwmq11lhGCFFr3dq0aSPGjRvnXHf8dx0wYICwWCwuZWtqy4yMDAFA/Oc//3FumzVrlgAgVqxYUWu9f/nlFwFA/Pzzzy6f9+jRQ9xwww3VvkfU3PAyDVEDsFqtWLt2LYYPH462bds6t0dFRWH06NHYuHEjDAYDACAwMBD79+/HkSNHatyXl5cXtFot1q9fj/Pnz9erHl5eXs7liooK5Ofn49prrwUA7Ny5s1r5xx57zGX9+uuvR0FBgbOuq1evBgA8/vjjLuWmT59e5zr9/PPPKCgowP333+/cdv/99+PPP//E/v37ndu+/fZbKBQKzJ49u9o+HD06q1atgs1mw6xZs6BUKmssczkeeeQRlx4LwLUtzWYzCgoK0L59ewQGBrq05bfffouePXvirrvuqrXeSUlJiI6Odhkrs2/fPuzZswf/+Mc/LrveRE0FwwhRAzh79izKysrQqVOnap916dIFNpsNOTk5AICXXnoJhYWF6NixI7p3746nn34ae/bscZbX6XR4/fXX8fPPPyMiIgIDBw7EG2+8gdzc3EvW49y5c5g2bRoiIiLg5eWFsLAwxMfHAwCKioqqlW/durXLelBQEAA4Q9CJEyegVCrRrl07l3I1HWdtvvjiC8THx0On0yEzMxOZmZlo164dvL29XU7OR48eRXR0NIKDg2vd19GjR6FUKtG1a9c6//y6cLRRVeXl5Zg1axZiY2Oh0+kQGhqKsLAwFBYWurTl0aNH0a1bt4vuX6lU4oEHHsCqVaucY3KWLFkCvV6P++67z63HQtQYMYwQNTIDBw7E0aNHsWjRInTr1g2ffPIJevfujU8++cRZZvr06Th8+DBSU1Oh1+sxc+ZMdOnSBbt27brovkeMGIGFCxfisccew4oVK7B27VqsWbMGgDQ49EIX9gY4CCGu4AgrGQwG/PDDD8jKykKHDh2cU9euXVFWVoalS5e67WfVhdVqrXF71V4Qh6lTp+KVV17BiBEj8NVXX2Ht2rVIS0tDSEhIjW15KWPHjkVJSQlWrVrlvLvo9ttvR0BAQL33RdTUcAArUQMICwuDt7c3Dh06VO2zgwcPQqlUIjY21rktODgY48ePx/jx41FSUoKBAwdizpw5mDBhgrNMu3bt8OSTT+LJJ5/EkSNH0KtXL7z99tv44osvaqzD+fPnkZ6ejhdffBGzZs1ybq/tclBdtGnTBjabDUePHnXpDanpOGuyYsUKVFRU4MMPP0RoaKjLZ4cOHcILL7yAP/74AwMGDEC7du3wyy+/4Ny5c7X2jrRr1w42mw1//fUXevXqVevPDQoKqvZgNZPJhNOnT9ep3oD0LJRx48bh7bffdm6rqKiott927dph3759l9xft27dcPXVV2PJkiWIiYlBdnY23nvvvTrXh6gpY88IUQNQqVQYMmQIvvvuO5dbP/Py8rB06VIMGDAA/v7+AFDtNltfX1+0b98eRqMRAFBWVoaKigqXMu3atYOfn5+zTG11AKr3asybN+9yDwtDhw4FALz77ruXtc8vvvgCbdu2xWOPPYZ7773XZXrqqafg6+vrvFRzzz33QAiBF198sdp+HMc0fPhwKJVKvPTSS9V6J6oed7t27fD777+7fP7xxx/X2jNSE5VKVa0t33vvvWr7uOeee/Dnn39i5cqVtdbbYcyYMVi7di3mzZuHkJAQZ/sSNXfsGSFyo0WLFjkve1Q1bdo0zJ07F2lpaRgwYAAmTZoEtVqNjz76CEajEW+88YazbNeuXXHjjTeiT58+CA4Oxvbt2/HNN99gypQpAIDDhw9j8ODBGDFiBLp27Qq1Wo2VK1ciLy8Po0aNqrVu/v7+zvElZrMZrVq1wtq1a5GVlXXZx9urVy/cf//9+OCDD1BUVIT+/fsjPT0dmZmZl/zuqVOn8Ntvv1Ub/Oqg0+mQnJyMr7/+Gu+++y4GDRqEMWPG4N1338WRI0dwyy23wGaz4X//+x8GDRqEKVOmoH379nj++efx8ssv4/rrr8fdd98NnU6Hbdu2ITo62vm8jgkTJuCxxx7DPffcg5tvvhl//vknfvnll2q9Mxdz++2347///S8CAgLQtWtXZGRk4Ndff0VISIhLuaeffhrffPMN7rvvPjz00EPo06cPzp07h++//x4LFixAz549nWVHjx6NZ555BitXrsTEiROh0WjqXB+iJk22+3iImhHHLaC1TTk5OUIIIXbu3CmSk5OFr6+v8Pb2FoMGDRKbNm1y2dfcuXNFv379RGBgoPDy8hKdO3cWr7zyijCZTEIIIfLz88XkyZNF586dhY+PjwgICBAJCQniq6++umQ9T548Ke666y4RGBgoAgICxH333SdOnTpV7VZXx629Z8+erfE4s7KynNvKy8vF448/LkJCQoSPj4+44447RE5OziVv7X377bcFAJGenl5rmcWLFwsA4rvvvhNCCGGxWMSbb74pOnfuLLRarQgLCxNDhw4VO3bscPneokWLxNVXXy10Op0ICgoSN9xwg0hLS3N+brVaxbPPPitCQ0OFt7e3SE5OFpmZmbXe2lvTLdvnz58X48ePF6GhocLX11ckJyeLgwcPVtuHEEIUFBSIKVOmiFatWgmtVitiYmLEuHHjRH5+frX93nrrrQJAtd8LouZMIUQDjg4jIqKLuuuuu7B379469S4RNRccM0JE1EicPn0aP/30E8aMGSN3VYgaFMeMEBHJLCsrC3/88Qc++eQTaDQa/POf/5S7SkQNij0jREQy27BhA8aMGYOsrCx8/vnniIyMlLtKRA2KY0aIiIhIVuwZISIiIlkxjBAREZGsGEaIiIhIVgwjREREJCuGESIiIpIVwwgRERHJimGEiIiIZMUwQkRERLJiGCEiIiJZMYwQERGRrBhGiIiISFYMI0RERCQrhhEiIiKSFcMIERERyYphhIiIiGTFMEJERESyYhghIiIiWTGMEBERkawYRoiIiEhWDCNEREQkK4YRInKrxYsXQ6FQYPv27XJXhYiaCIYRIiIikhXDCBEREcmKYYSIGtyuXbswdOhQ+Pv7w9fXF4MHD8bmzZtdypjNZrz44ovo0KED9Ho9QkJCMGDAAKSlpTnL5ObmYvz48YiJiYFOp0NUVBSGDRuG48ePN/AREdGVUMtdASJqWfbv34/rr78e/v7+eOaZZ6DRaPDRRx/hxhtvxIYNG5CQkAAAmDNnDlJTUzFhwgT069cPBoMB27dvx86dO3HzzTcDAO655x7s378fU6dORVxcHM6cOYO0tDRkZ2cjLi5OxqMkovpQCCGE3JUgouZj8eLFGD9+PLZt24a+fftW+/yuu+7C6tWrceDAAbRt2xYAcPr0aXTq1AlXX301NmzYAADo1asXYmJi8OOPP9b4cwoLCxEUFIQ333wTTz31lOcOiIg8jpdpiKjBWK1WrF27FsOHD3cGEQCIiorC6NGjsXHjRhgMBgBAYGAg9u/fjyNHjtS4Ly8vL2i1Wqxfvx7nz59vkPoTkWcwjBBRgzl79izKysrQqVOnap916dIFNpsNOTk5AICXXnoJhYWF6NixI7p3746nn34ae/bscZbX6XR4/fXX8fPPPyMiIgIDBw7EG2+8gdzc3AY7HiJyD4YRImqUBg4ciKNHj2LRokXo1q0bPvnkE/Tu3RuffPKJs8z06dNx+PBhpKamQq/XY+bMmejSpQt27dolY82JqL4YRoiowYSFhcHb2xuHDh2q9tnBgwehVCoRGxvr3BYcHIzx48fjyy+/RE5ODnr06IE5c+a4fK9du3Z48sknsXbtWuzbtw8mkwlvv/22pw+FiNyIYYSIGoxKpcKQIUPw3Xffudx+m5eXh6VLl2LAgAHw9/cHABQUFLh819fXF+3bt4fRaAQAlJWVoaKiwqVMu3bt4Ofn5yxDRE0Db+0lIo9YtGgR1qxZU237nDlzkJaWhgEDBmDSpElQq9X46KOPYDQa8cYbbzjLde3aFTfeeCP69OmD4OBgbN++Hd988w2mTJkCADh8+DAGDx6MESNGoGvXrlCr1Vi5ciXy8vIwatSoBjtOIrpyvLWXiNzKcWtvbXJycnD27FnMmDEDf/zxB2w2GxISEvDKK68gMTHRWe6VV17B999/j8OHD8NoNKJNmzYYM2YMnn76aWg0GhQUFGD27NlIT09HTk4O1Go1OnfujCeffBL33XdfQxwqEbkJwwgRERHJimNGiIiISFYMI0RERCQrhhEiIiKSFcMIERERyYphhIiIiGTFMEJERESyanEPPbPZbDh16hT8/PygUCjkrg4REVGTIYRAcXExoqOjoVS6rz+jxYWRU6dOubz7goiIiOonJycHMTExbttfiwsjfn5+AKSGdLwDg4iIiC7NYDAgNjbWeS51lxYXRhyXZvz9/RlGiIiILoO7hzlwACsREVFzZK4ATm6XuxZ1wjBCRETU3OT9BSwcBPxnGHDumNy1uSSGESIiouZCCGDrQuDjG4EzfwEaL6A4V+5aXVKLGzNCRETULJXmA99NBg6vkdbb3wwM/wDwDZe3XnXAMEJERNTUZaYDqyYCJXmASgvc/DKQ8E+giTxPi2GEiIioqbIYgfSXgIz3pfWwzsA9nwKR3eStVz0xjBARETVFZw8D3z4M5O6R1q+ZAAyZK40TaWIYRoiIiJoSczmw87/Ar7MBcxngFQwMmw90vlXuml02hhEiIqKmoOAosH0RsOsLoKJQ2tb2RmD4AsA/Ss6aXTGGESIiosbKagGO/AJs+wQ4uq5ye2BrIHGqdGnGjS+skwvDCBERkadZjICpVLqsotYDWl9Arav9bpfiPGDXf4DtiwHDSftGBdDhZimAtE8ClKqGqr3HMYwQERFdibJzwN5vgKwNgNEghQ5jiTQ32ec2c/XvKVSAzlcKJlpfQOsjrSuUwPE/Kr/jHQJcPQboOx4IimvQQ2soDCNERET1ZbUAR9Ol8RuHfq45bNREqaksK6xARZE01SSmn9QL0nUYoNG7p96NFMMIERFRXZ05AOxeAvy5HCg9U7k9sgfQ7R7Av5W9t8Oneo+HxgdQqQGb1d5r4ug5KXHtSTGXAVG9gKgesh1mQ2MYISIiupjy89JlmN1LgVM7K7d7hwI9RgK97gciu9d9f0oVoPeXJgLAMEJERFSz88eBzR9Kz/Qwl0rblGqg4y1ArwekwaQqjaxVbC4YRoiIiKo6uR3Y9B5w4HtA2KRt4V2B3mOB7vcBPqHy1q8ZYhghIiKyWaWBqJveA3I2V25vNxjoP1V6uFgTeelcU8QwQkREzVPe/sq32Co10iUVlUZaV2mkbUoVcGg1kDEfOHdM+p5SA/QYASROBiKukvcYWogmGUb+/vtvPPvss/j5559RVlaG9u3b47PPPkPfvn3lrhoREcnNcBr4ZQawf2X9vqcPBPo+BPR7tMk/Xr2paXJh5Pz587juuuswaNAg/PzzzwgLC8ORI0cQFBQkd9WIiEhOVov02PR1cwFTsfTwsLAugM0CWE2A1Sw948NqkspaTdIUHA8kPCYNStX5yn0ULVKTCyOvv/46YmNj8dlnnzm3xcfHy1gjIiKS3ckdwI/Tgdw90nqrvsDt7wBRPWWtFtVNk3u7zvfff4++ffvivvvuQ3h4OK6++mosXLiw1vJGoxEGg8FlIiKiZqL8PPDjE8Ang6Ugog8Abv8/4OE0BpEmpMmFkWPHjuHDDz9Ehw4d8Msvv2DixIl4/PHH8fnnn9dYPjU1FQEBAc4pNja2gWtMRERuJwTw5zLg/WuA7YsACKDn/cCUHdK4j2bwJtuWRCGEEHJXoj60Wi369u2LTZs2Obc9/vjj2LZtGzIyMqqVNxqNMBqNznWDwYDY2FgUFRXB359PvyMiahQqiqS7X3L3SXe12MzSMz6EsM8vWD53FDi5TfpuaCfgtreB+OvlPYYWwGAwICAgwO3n0CY3ZiQqKgpdu3Z12dalSxd8++23NZbX6XTQ6XQNUTUiIroUmw0oPAHk7ZOCR94+6fJKYXb996XWAzc8AyROBdRa99eVGkyTCyPXXXcdDh065LLt8OHDaNOmjUw1IiKiGlktQMER4NRu4PSf0pS7V7rTpSb+rYCIbkBYJ0DjJd0No1BKDxtzLMO+rNICnYYCQfy3vzlocmHkiSeeQP/+/fHqq69ixIgR2Lp1Kz7++GN8/PHHcleNiKjlspqlN9qe/hM4vdsePPYBlvLqZVVaIKyz9HK5iG5AZDdp7h3c4NWmxqHJjRkBgB9//BEzZszAkSNHEB8fj5SUFDzyyCN1+q6nrncREbVIVgvw55fAb68Cxaeqf671BSJ7SHe2RPUEonoAoR35grkmylPn0CYZRq4EwwgRkRsIIb3LJf1F4OxBaZvOvzJ0RF8tzYPb8c6WZoQDWImIyL3OHQO2fgJ4BQHtbwKirq5bcMjeDKTNrnyhnFcQcP1TwDUTAI3es3WmZolhhIiopREC2PVfYM0MwFQibfttLuAVDLS7CWg/WJr7Rbp+7+wh4NcXgUM/SetqL+DaicB10wCvwAY9BGpeGEaIiFqS0nzgh2nAwR+l9dhrAZ9Q4NgGoPwcsO8baQKkQaXtbgLirgcO/gDs+kJ6xodCCVz9D+DGGYB/tHzHQs0GwwgRUUtxeC3w3WSg9Ayg1AA3PQ/0fxxQqqS7YU5uAzLTgaPp0u24efbngGx6t3IfnW8HBs+Sbr8lchMOYCUiau5MpcDaF+yPTYd0W+3dH1/83S2l+cDR36RgcvwPIDgOGPQC0DqhQapMjRMHsBIRUf39vQNY8ShQkCmtJ0wEkmZLDxW7GJ9QoMd90kTkYQwjRESNSd5+wHAaiBtwZXemWIzAH/8G1r8GCCvgFwUM/0AaA0LUyDCMEBE1BuXngV/nADsWS+v6AOCqu6U30cb2kx6Jfik2K3D8f8Deb4C/vgeMRdL2q+4CbnuHTzilRothhIhITkIAe78GfvkXUHpW2uYTJi3v+EyagttKoaTHyOrvYhECOLVTCiD7VgAluZWf+UUDSXOAHiPqFmaIZMIBrEREcik4CvyUAhxbL62HdgJu/z+gdSJw/Hfgz2VSD4e5tPI7bQYAPUdJg08P/igFmXPHKj/XBwJXDQe63we07s+nn5Jb8XHwbsIwQkRuZzUDR9YCSrX08je/qIv3RFiMwMZ5wP/eBqxGQK0HBj4F9J8GqLWuZY0lwIEfgD+XAln/A1DDP9kab+kNtt3vA9oNrr4PIjfh3TRERI3R6T3Sszty91Ru8w6RQklkd+klcZHdgZAOgEotBYofnwAKjkhl290E3Pa2dCmmJjpfoNf90lSYA+z9Ctj9JXA+Swoe3e+TgojO1/PHSuQh7BkhIrocFiOw4Q3gj3mAzSJdHvGLAvIPS3evXEilA4LjK18q5xMO3JIKdLun/uM5HP9scxwINTD2jBARNRYnt0u9IY5g0XUYcOtbgG84YC4HzhwAcvdWTnn7pHfAnD0IQAH0fUh6iunlvs+FIYSaGYYRIqK6MpUBv70CbP5AekeLT5h0iaXrsMoyGi+gVW9pcrDZpMsqZ/4CgtsBEV0bvu5EjRjDCBFRXRzfCHw3RQoVANBjlHSZpS7P7lAqgZB20kRE1TCMEBHVxFQqDRgtzAYOrZae9wEA/q2A2+cBHYfIWj2i5oRhhIhaHqtFeqhYSR5QnCsFjqJsae6Yygqqf6/PeODmlwA9B78TuRPDCBE1PxUG4OQ26c6W4lyg5IwUPBxTaT5qfF7HhXT+QGAb6amnCf8E4gd6vOpELRHDCBE1bUIAhSeA7C1Ajn3K249Lhg2FUrq91i8CCIiVQkdgLBDYWpoCYi//bhciqpcmHUZee+01zJgxA9OmTcO8efPkrg4RNQRzuXS7bM7WyvBRkle9XGAb6ZHp/tHSLbe+kYBvhBQ+fCOkB5MpVQ1ffyKqpsmGkW3btuGjjz5Cjx495K4KEXmKxQSc2Q/8vRM4tQs4tVu6PfbCh4opNVLwiE0AWidIc79IWapMRPXXJMNISUkJHnjgASxcuBBz586VuzpE5C6mUuDAj1Jvx6md0uUWq6l6OZ8woFUfe/i4Foi+Wnq+BxE1SU0yjEyePBm33XYbkpKSLhlGjEYjjEajc91gMHi6ekRUX7l7ge2fAXu+AkzFrp/pA6UHiEVfXTn5t+JTSImakSYXRpYtW4adO3di27ZtdSqfmpqKF1980cO1ImpBhJAebV5+HigvtM/PS+9qCW0PhHUBtN6X3o+pDNi/Unp+x8kq/z8HxQNdbgei7QEkKI7Bg6iZa1JhJCcnB9OmTUNaWhr0en2dvjNjxgykpKQ41w0GA2JjYz1VRSLPO38COPQzoNYBATFSL0FAK0AfcPHv2axAUQ5QcBQ4d8w+PwpUFNkL2E/4CkX1ZasJqCisDB42S+0/R6GUHnke2Q2IsE+R3Sp7M84csPeCLKv82Uo10Pk26Tke8TdITywlohajSb21d9WqVbjrrrugUlWOgLdarVAoFFAqlTAajS6f1YRv7aUmyWoGDq8BdiwGMtNR422rWj8pnAS0kk78/q0Ao6EydJw/XvP4i8ul0gJewYBXkHQLrFItBY2y/JrLO95qe/ZA5bbA1kCfB4Fe/5DuciGiRo1v7QUwePBg7N2712Xb+PHj0blzZzz77LOXDCJETc7548DO/wC7vnC9fTXuekDjDRj+BopOSr0WpmLpRF/1ZH8hx2vsg9sBIW2luU9o5edCwBl0qi4r1fbQESSFCq8gacDohZdPhJDqmbcPyN1XOc8/LNWxohBQqIBOQ4G+44G2N7EXhIiaVhjx8/NDt27dXLb5+PggJCSk2naiJstqtr8L5XPg6Do4A4FPGHD1P4DeY4Hgtq7fMZYAhlPSZRjD30DR39Jc5ye9nC3Y/pI2/1aefbaGQiHdUusXCbRPqtxuMQJnD0rhKqYf4B/luToQUZPTpMIIUbMjhBQgHL0IefuAExlA6ZnKMu1uki5ldBwKqLU170fnC4R1lKbGSK2TngMS1VPumhBRI9Tkw8j69evlrgJR3ZjKpAd2OS9h7JcmY1H1sj7hVXpB4hu+rkREDajJhxGiRqk4T3p2Ru4eaZ63DyjIBIStelmlBgjrZL/z5CogqgfQ5jpApWn4ehMRyYBhhMgdsrdI4zxy90pT1cssVfmEVd7q6rjtNbRj7ZdfiIhaAIYRoithMQHpLwIZ71/wgQII7QBEdpemCPuct68SEVXDMEJ0ufIzgW8fAk7/Ka13u0e65TayOxDeBdD6yFs/IqImgmGEqL6EAHYvBVY/DZhLpWduDJsvPUGUiIjqjWGEWrbiPOkx6pq6vV4AFUXAj08A+76V1uOuB+7+GPCP9lwdiYiaOYYRapmEAH5/C/jtFekZGLEJQPxA6b0o0VcDqhr+18jZCnz7MFCYLT1F9Kbngeume/YhYkRELQDDCLU85grg+6nA3q+kdUsFkLVBmvCy9I6XuOvs4WQgENYZ2DgPWJ8KCCsQ2Aa451Mg9ho5j4KIqNlgGHEHqwU4+CPQYUjdXp1O8ik5Cyx/AMjZIr1v5da3gDb9gazfgWPrgeMbpfenHF4jTQCg1kuBBQC63wfc9val35BLRER1xjDiDv8dDhz/H3D7POnlX9Q4nTkALB0hXWbRBwAj/gO0vVH6LKwT0O8RwGaVnhOS9bs0ndgkDVLV+krBpeeo6i+HIyKiK8Iw4g6dbpXCyJaPpHeI8GTV+Bz5Ffj6QenNtsFtgdFfSc8BuZBSBUT3kqbrHpeeI3JmPxDQGvAJaeBKExG1DHx3tztc/QCg8ZFe3Z61Qe7a0IW2fAwsvU8KIm0GABPSaw4iNVFrpQGtDCJERB7DMOIO+gCg12hpectH8taFKlktwE9PAT8/Lb0T5up/AGNWAt7BcteMiIiqYBhxl4R/SvNDPwPnsuStC0nPD1l6H7BtIQAFcPNLwJ3v8x0wRESNEMOIu4R2ANonARDA1oVy16blOnsI+G4KMK8bcHQdoPEGRn4BXDeNY3mIiBophhE3sdoEkDBRWtn1X8BYIm+FWhIhgON/AEtHAfP7Se1vNQEx1wAPrQG63C53DYmI6CJ4N40b/HboDN5Ycwiv390bPULaAwWZwJ9fSreKkudYLcDBH4A/3gVO7bRvVEjviOk/FWh9razVIyKiumEYuUJCCLyz9jAOnDZg+AcZ+LDDnUgueEcayNr3YUDJzie3sVmB8vNA6Vkg639AxvtA4QnpM5VOGkScOLnud8oQEVGjwDByhRQKBT4bfw1e/vEvfLf7FFIOX4Utem/4FhwBjq2zjyOhOvt7B5CZLgWO0nzXefk56a6YqryCpR6oax4BfMPkqTMREV0RhRBCyF2JhmQwGBAQEICioiL4+/u7dd+/HTyD51fuxcOlC/Gw+mcc8E1A5MQfEeTDOzguSQjgj3lA+kvVA8eFvIKAgFig91ig1wN8BD8RUQPx1DmUYcTNSowWfPp9OqbuGwmlQuBu1bsYf+fNuL1HFBS8m6NmxmJg1STgwPfSeodkILIb4B0K+IQBPlXm3iGASiNvfYmIWihPnUOb3GWa1NRUrFixAgcPHoSXlxf69++P119/HZ06dZK7agAAX50a0+5LRqFhMAJzfsWdxh8x9ctQrNr1N14e3g3RgV5yV7FxOXtYenFd/mFAqQGGvg70fYi34RIRtSBNbnTlhg0bMHnyZGzevBlpaWkwm80YMmQISktL5a6ai8AbpwAARus2IkhVjvSDZ3DzOxvw3Ld78N3uv3HGUCFzDRuBv74HFg6SgohfNDD+Z+CahxlEiIhamCZ/mebs2bMIDw/Hhg0bMHDgwEuW9/RlGichgA+uBc4exNnr5uCxzATsOHHepUjbMB9c2zYEiW1DcG3bEIT56TxXn8bEagHWvSyNEQGk98Xc9xngGy5rtYiI6OJ4maYWRUVFAIDg4JrfN2I0GmE0Gp3rBoOhQeoFhUJ6RPyPTyDsr8/x9eSp2HD0HP44ko/NWQXYf8qAY2dLcexsKZZuyQYAdAj3RWK7EPRuHYSu0f5oG+oDtarJdV5dXGk+8M1DlS8UTJwCJL0IqJr8ryIREV2mJt0zYrPZcOedd6KwsBAbN26sscycOXPw4osvVtvu8Z4RADCVAu90BSoKgfuXA51uqfz5ZWZsySpAxrECbD52DgdOVw9JWrUSnSP90DXKH12j/dE1yh+do/zhq2uiJ+6/dwJfjQWKcqS3HA97D+h2j9y1IiKiOuLdNDWYOHEifv75Z2zcuBExMTE1lqmpZyQ2NrZhwggArJ0JbHoXaHsjMPa7WoudLzVhS5YUTPafKsKB08UoMVpqLBsX4o1OkX7oGOGH9uG+6Bjhh/hQH+g1Kg8dxBUqOAr8/iawZ7l0225wO2DUEiC8i9w1IyKiemAYucCUKVPw3Xff4ffff0d8fHydv9dgY0YcCrOBf/eUTsKTtgDhnev0NZtNIOd8Gf46ZcBfpw3O+emimge+KhVAmxAfezjxRYdwKajEh/rAR66elPPHpRCy+0tAWKVtV90F3PFvQB8gT52IiOiyMYzYCSEwdepUrFy5EuvXr0eHDvV79HeDhxEAWP4P4MAPQJ/xwB3zrmhX50pN+OuUAYfzinHkTDGO5JXgcF4xDBU196IAQIS/DvGhPogP9UXbUB9pOcwHsUHe0Ko9MCalMBv4/S1g9xLAZq9XhyHAjc8Brfq4/+cREVGDYBixmzRpEpYuXYrvvvvO5dkiAQEB8PK69DM8ZAkjxzcCi28D1F7AkwekJ4i6kRACZ4uNOHKmBEfyinH4TAky80qQebYE50pNtX5PpVSgVaAXIv31CPPXIcJPj3B/HSL8dQj30yPCX4cwPz389eq6PbCt6G/gf28DO/8D2MzStnY3ATf+C4i9xk1HS0REcmEYsavtpPjZZ5/hwQcfvOT3ZQkjQgALrgfy9gI9R0sP9tI3zM8uKjMjq6AUWfklyDpbimP5pciyT2Uma532odcoEeKjQ6ifDqE+WoT66hDqp0W4HmiDk2hlPIaw87sQcPgbKKz28BN/AzDoX3xzLhFRM8Iw4iayhBEA2PetdEsrAPiEA4NnSu9VUcoz6FQIgTPFRpwoKMOZ4gqcMRiRV1yBs/b5GYMReYYKGCosUMCGWMVZdFZko5MiB52UOeisyEGcIhdqhet7ZLbYOmORZjSy/Xsj1NceXOzzEF8dQny1CPHRIthHixAfHby0jXTQLRERVcMw4iayhREAOPwL8Mu/gIJMaT2yB3DLa0DcdQ1bj0sxlgB/bwdOZMB6IgOKv7dDaa75CbfFCj8cVbbBAWsMvjf1QYatK4C6P0HV0esSbA8oF528tQjw0kCp5BNaiYjkwDDiJrKGEQCwmIBtC4H1rwNG6YFt6DoMuPllIKhN7d+zWqTLPNlbgJNbAYsRCIqTpsA29nlrQKOvf51KzgDZGUD2Zml+ek/l3S8OKh0Q1gmIuAoI7wpEdAXCrwL8Ip2PbzdZbDhXakJ+idE+mVBgXy4oMeGsfX6+zISCUhNMlku8nbcGSgUQ5G3vWXH2vOgQ5qer0hNjv6Tkq4VOzZ4XIiJ3YRhxE9nDiENpPrBuLrDzc+m2X5UO6D8FGJAC6HyBCgNwchuQs0UKCSe3A7X0Trjwi5ZCTVCc9IZbqxmwGqXwYjECVpN9bl8vOQOcz6q+n4BYoHWiNOaj9bVAaCe3PiVVCIFSkxXnSkw4V2bCuVIpqJwrrZwcoeV8qTQvvsgdQ7Xx1akR5KNBsD3ABNl7WIJ8pMtFQT5aBHppEOCtQaCX1POi1yj5hmUiohowjLhJowkjDrn7gF9mAFm/S+u+kYBvGJC3XwopVekCpLtSYq+VntNReEJ6lodjMpVcZiUUUo9H62ulABKbAATGXv4xeYjZasP5MimoFJSYnL0v+SVG5BcbXddLjDBbL+9XW6tWIsBLI4UUx+StQZC3FFwC7QEm0L4twEuDIB8tfLQqhhgiatYYRtyk0YURQLrb5uBPwNrnpVDhENhGCgixCdI8rAugrOW5IEIAZefswSRLmpefB9R6QK0DVFpprtZJvTCOZa0vEN3L7bcby00IAUO5xd7rIvWunCuVemEcPS2OuaHcjMJyM4rKzbDaLv9/B5VSAT+9Wpp0Gvh7qeGn18BPr4a/XgN/feW6r77qZ9Kyr04NbwYaImrEGEbcpFGGEQeLUXo4mlIl9X74R8ldoxbFcemosMyEonIzisqkgOIIKufLTCgqk+aFZZXbzpeZL2v8S01USgV8dWpnMPGpsuyrU8NHp4a3TgVfrdr5mY9ODR+d9LmvXg0fbeV2jzzUjohaLL61tyVQ64Du98pdixZLoagMAjH17CiqMFtRWGZGcYUZhgoLiivMKK6wwGCfO9fLzSgxWuxlpO0lRmnZahOw2oQUhMrNbjkmrVrpPCbnpK8MMn56Nbw0Kug1KnhplNDbl/Uuyyp42Se9Rgm9VlrWNLc3ShORbBhGiNxAr1EhMkCFyIDLuJsJUq9MudlqDygWlBrtk8mKUqMFJResu2wzWlFSZb3EaIHR3lNjsthwzmK66JN4L5daqYCXRgWdRgUvrRJeGqknx0cr9dT4aB29Nmr42Ht5fHQqeGulnh4vbZVljQre9nUOICZqeRhGiBoBhUJhPzGrEeGGnk+z1VYlsFhRYjSjxGhFSYWl2nKZyYoKsw0VFiuMZivKzfZ1s9U+2ezbpMkxrMZiEyg2WlBcy9ulL5dCAXhrVPCyB5XK4KKCl8Z1m16jgl5dtSfHtUdHr1ZCZ9+uU6ugU0ufO+YqPrOGqFFgGCFqhjQqJQK9tQj01rp1v0IImKw2VJik8FJusjqDSrnJ6tKTU2ayoMRoRZnRglL7cql9e7nJijL7VG6Wtjt6c4SAtJ86vq7gSqiVCujUSmjVUljR2pe1KiV0Gmnu+Mwl8Kgrw5Aj2HhVucR1YXlpvXKbWqlg7w9RFQwjRFRnCoXC3sOgQgA0bt231SZdqnKElVKjFeVmS2VocQYYS5UQJIWiCpNVml/QoyP19thgdM5tMFkrBxtbbAIWZ/BxzzidulAq4Aw5OrUUfBzLjjCkVimgVCigViqgUiqluUoBlX2bWqVwhh6XXh+N0qUHSFclMDl6kXT2XiNHGRXDEcmMYYSIGoWqdxJ5ktUmYLJIAaXCHlSkdXtYsVRuM1ltMJqledVLVsaaLmFZpDLGqiGpyiUvY5U7rmwC9u3uuQvLHRQKQKWQApBCIf33cCwrFQpoVApoVEr7VH1Zq75g3R6oHOWkzyvXqwYvx+dVe6YUCkABhX0OKJUKKOz1VCgqg1qN9XEEOqWCQauJYBghohZFpVTAyz7mpCEJIZyBp2pPjdEiBRVnD449ENmEgMUqYBXSXVYWm4DNPrfabDBbxQX7qgxXRmcYct1eNRhdeDu6EIBFCADN72kPjkCldM4rlxUKQG0PLpoqAcplXSnN1SolNPaAo1FJPUqOz1UqBTRKBZRKKSQ55ipF9W1qpRSaLvy5aqUSWrU0d4RBpSMY2tdVVcKiVEY6Dkd5lTOASeFSev6RptGPj2IYISJqAAqFwjmwFm6+xHU5bDYpzFSYrbAKAZsQsNkgzS9cFlIIMlukMUNmqw0Wq4DZanOum602mC0CZpsNZosUlqp+ZrGvm+xByPFdqQdKwFSlN8psERAQsAkpxAkAEFJ9BKTg5LgV3vmz7fWx1PDgQiEghTrYd9TC/PT4AFwVHSB3NS6KYYSIqAVSytRD5GmO3iOztbKHyRFqrPZlm01A2MONo+fJEZgstirBpkrAqfq5xSacn1mr/DxHT1ZlD5Z9EgJWa+0/yzmvEqpsVerqqGfVgGi1L1ttjvBYWcbxcxwae68IwDBCRETNiFKpgFapgFathI9O7trIyxFQVE1gzAzDCBERUTOkVCqgROMPIgDA5zkTERGRrFpcz4jjvYAGg0HmmhARETUtjnOnu9+x2+LCSHFxMQAgNjZW5poQERE1TcXFxQgIcN8dOgrh7njTyNlsNpw6dQp+fn5uexCOwWBAbGwscnJy3PpKZaoZ27thsb0bFtu7YbG960cIgeLiYkRHR0OpdN9IjxbXM6JUKhETE+ORffv7+/OXuQGxvRsW27thsb0bFtu77tzZI+LAAaxEREQkK4YRIiIikhXDiBvodDrMnj0bOl0Lf8JOA2F7Nyy2d8Niezcstnfj0OIGsBIREVHjwp4RIiIikhXDCBEREcmKYYSIiIhkxTBCREREsmIYcYP58+cjLi4Oer0eCQkJ2Lp1q9xVahZ+//133HHHHYiOjoZCocCqVatcPhdCYNasWYiKioKXlxeSkpJw5MgReSrbxKWmpuKaa66Bn58fwsPDMXz4cBw6dMilTEVFBSZPnoyQkBD4+vrinnvuQV5enkw1bto+/PBD9OjRw/mgrcTERPz888/Oz9nWnvXaa69BoVBg+vTpzm1sc3kxjFyh5cuXIyUlBbNnz8bOnTvRs2dPJCcn48yZM3JXrckrLS1Fz549MX/+/Bo/f+ONN/Duu+9iwYIF2LJlC3x8fJCcnIyKiooGrmnTt2HDBkyePBmbN29GWloazGYzhgwZgtLSUmeZJ554Aj/88AO+/vprbNiwAadOncLdd98tY62brpiYGLz22mvYsWMHtm/fjptuugnDhg3D/v37AbCtPWnbtm346KOP0KNHD5ftbHOZCboi/fr1E5MnT3auW61WER0dLVJTU2WsVfMDQKxcudK5brPZRGRkpHjzzTed2woLC4VOpxNffvmlDDVsXs6cOSMAiA0bNgghpLbVaDTi66+/dpY5cOCAACAyMjLkqmazEhQUJD755BO2tQcVFxeLDh06iLS0NHHDDTeIadOmCSH4+90YsGfkCphMJuzYsQNJSUnObUqlEklJScjIyJCxZs1fVlYWcnNzXdo+ICAACQkJbHs3KCoqAgAEBwcDAHbs2AGz2ezS3p07d0br1q3Z3lfIarVi2bJlKC0tRWJiItvagyZPnozbbrvNpW0B/n43Bi3uRXnulJ+fD6vVioiICJftEREROHjwoEy1ahlyc3MBoMa2d3xGl8dms2H69Om47rrr0K1bNwBSe2u1WgQGBrqUZXtfvr179yIxMREVFRXw9fXFypUr0bVrV+zevZtt7QHLli3Dzp07sW3btmqf8fdbfgwjRORi8uTJ2LdvHzZu3Ch3VZq1Tp06Yffu3SgqKsI333yDcePGYcOGDXJXq1nKycnBtGnTkJaWBr1eL3d1qAa8THMFQkNDoVKpqo24zsvLQ2RkpEy1ahkc7cu2d68pU6bgxx9/xG+//YaYmBjn9sjISJhMJhQWFrqUZ3tfPq1Wi/bt26NPnz5ITU1Fz5498e9//5tt7QE7duzAmTNn0Lt3b6jVaqjVamzYsAHvvvsu1Go1IiIi2OYyYxi5AlqtFn369EF6erpzm81mQ3p6OhITE2WsWfMXHx+PyMhIl7Y3GAzYsmUL2/4yCCEwZcoUrFy5EuvWrUN8fLzL53369IFGo3Fp70OHDiE7O5vt7SY2mw1Go5Ft7QGDBw/G3r17sXv3bufUt29fPPDAA85ltrm8eJnmCqWkpGDcuHHo27cv+vXrh3nz5qG0tBTjx4+Xu2pNXklJCTIzM53rWVlZ2L17N4KDg9G6dWtMnz4dc+fORYcOHRAfH4+ZM2ciOjoaw4cPl6/STdTkyZOxdOlSfPfdd/Dz83NeJw8ICICXlxcCAgLw8MMPIyUlBcHBwfD398fUqVORmJiIa6+9VubaNz0zZszA0KFD0bp1axQXF2Pp0qVYv349fvnlF7a1B/j5+TnHPzn4+PggJCTEuZ1tLjO5b+dpDt577z3RunVrodVqRb9+/cTmzZvlrlKz8NtvvwkA1aZx48YJIaTbe2fOnCkiIiKETqcTgwcPFocOHZK30k1UTe0MQHz22WfOMuXl5WLSpEkiKChIeHt7i7vuukucPn1avko3YQ899JBo06aN0Gq1IiwsTAwePFisXbvW+Tnb2vOq3torBNtcbgohhJApBxERERFxzAgRERHJi2GEiIiIZMUwQkRERLJiGCEiIiJZMYwQERGRrBhGiIiISFYMI0RERCQrhhEiIiKSFcMIERERyYphhIiIiGTFMEJERESyYhghIiIiWTGMEBERkawYRoiIiEhWDCNEREQkK4YRIiIikhXDCBEREcmKYYSIiIhkxTBCREREsmIYISIiIlkxjBAREZGsGEaIiIhIVgwjREREJCuGESIiIpIVwwgRERHJimGEiIiIZMUwQkRERLJiGCEiIiJZMYwQERGRrBhGiIiISFYMI0RERCQrhhEiIiKSlcfCyO+//4477rgD0dHRUCgUWLVq1SW/s379evTu3Rs6nQ7t27fH4sWLq5WZP38+4uLioNfrkZCQgK1bt7q/8kRERNRgPBZGSktL0bNnT8yfP79O5bOysnDbbbdh0KBB2L17N6ZPn44JEybgl19+cZZZvnw5UlJSMHv2bOzcuRM9e/ZEcnIyzpw546nDICIiIg9TCCGEx3+IQoGVK1di+PDhtZZ59tln8dNPP2Hfvn3ObaNGjUJhYSHWrFkDAEhISMA111yD999/HwBgs9kQGxuLqVOn4rnnnvPoMRAREZFnqOWugENGRgaSkpJctiUnJ2P69OkAAJPJhB07dmDGjBnOz5VKJZKSkpCRkVHrfo1GI4xGo3PdZrPh3LlzCAkJgUKhcO9BEBERNWNCCBQXFyM6OhpKpfsurjSaMJKbm4uIiAiXbRERETAYDCgvL8f58+dhtVprLHPw4MFa95uamooXX3zRI3UmIiJqiXJychATE+O2/TWaMOIpM2bMQEpKinO9qKgIrVu3Rk5ODvz9/WWsGRERUdNiMBgQGxsLPz8/t+630YSRyMhI5OXluWzLy8uDv78/vLy8oFKpoFKpaiwTGRlZ6351Oh10Ol217f7+/gwjREREl8HdwxwazXNGEhMTkZ6e7rItLS0NiYmJAACtVos+ffq4lLHZbEhPT3eWISIioqbHY2GkpKQEu3fvxu7duwFIt+7u3r0b2dnZAKTLJ2PHjnWWf+yxx3Ds2DE888wzOHjwID744AN89dVXeOKJJ5xlUlJSsHDhQnz++ec4cOAAJk6ciNLSUowfP95Th0FEREQe5rHLNNu3b8egQYOc645xG+PGjcPixYtx+vRpZzABgPj4ePz000944okn8O9//xsxMTH45JNPkJyc7CwzcuRInD17FrNmzUJubi569eqFNWvWVBvUSkRERE1HgzxnpDExGAwICAhAUVERx4wQERHVg6fOoY1mzAgRERG1TAwjREREJCuGESIiIpIVwwgRERHJimGEiIiIZMUwQkRERLJiGCEiIiJZMYwQERGRrBhGiIiISFYMI0RERCQrhhEiIiKSFcMIERERyYphhIiIiGTFMEJERESyYhghIiIiWTGMEBERkawYRoiIiEhWDCNEREQkK4+Hkfnz5yMuLg56vR4JCQnYunVrrWVvvPFGKBSKatNtt93mLPPggw9W+/yWW27x9GEQERGRh6g9ufPly5cjJSUFCxYsQEJCAubNm4fk5GQcOnQI4eHh1cqvWLECJpPJuV5QUICePXvivvvucyl3yy234LPPPnOu63Q6zx0EEREReZRHe0beeecdPPLIIxg/fjy6du2KBQsWwNvbG4sWLaqxfHBwMCIjI51TWloavL29q4URnU7nUi4oKMiTh0FEREQe5LEwYjKZsGPHDiQlJVX+MKUSSUlJyMjIqNM+Pv30U4waNQo+Pj4u29evX4/w8HB06tQJEydOREFBQa37MBqNMBgMLhMRERE1Hh4LI/n5+bBarYiIiHDZHhERgdzc3Et+f+vWrdi3bx8mTJjgsv2WW27Bf/7zH6Snp+P111/Hhg0bMHToUFit1hr3k5qaioCAAOcUGxt7+QdFREREbufRMSNX4tNPP0X37t3Rr18/l+2jRo1yLnfv3h09evRAu3btsH79egwePLjafmbMmIGUlBTnusFgYCAhIiJqRDzWMxIaGgqVSoW8vDyX7Xl5eYiMjLzod0tLS7Fs2TI8/PDDl/w5bdu2RWhoKDIzM2v8XKfTwd/f32UiIiKixsNjYUSr1aJPnz5IT093brPZbEhPT0diYuJFv/v111/DaDTiH//4xyV/zsmTJ1FQUICoqKgrrjMRERE1PI/eTZOSkoKFCxfi888/x4EDBzBx4kSUlpZi/PjxAICxY8dixowZ1b736aefYvjw4QgJCXHZXlJSgqeffhqbN2/G8ePHkZ6ejmHDhqF9+/ZITk725KEQERGRh3h0zMjIkSNx9uxZzJo1C7m5uejVqxfWrFnjHNSanZ0NpdI1Dx06dAgbN27E2rVrq+1PpVJhz549+Pzzz1FYWIjo6GgMGTIEL7/8Mp81QkRE1EQphBBC7ko0JIPBgICAABQVFXH8CBERUT146hzKd9MQERGRrBhGiIiISFYMI0RERCQrhhEiIiKSFcMIERERyYphhIiIiGTFMEJERESyYhghIiIiWTGMEBERkawYRoiIiEhWDCNEREQkK4YRIiIikhXDCBEREcmKYYSIiIhkxTBCREREsmIYISIiIlkxjBAREZGsPB5G5s+fj7i4OOj1eiQkJGDr1q21ll28eDEUCoXLpNfrXcoIITBr1ixERUXBy8sLSUlJOHLkiKcPg4iIiDzEo2Fk+fLlSElJwezZs7Fz50707NkTycnJOHPmTK3f8ff3x+nTp53TiRMnXD5/44038O6772LBggXYsmULfHx8kJycjIqKCk8eChEREXmIR8PIO++8g0ceeQTjx49H165dsWDBAnh7e2PRokW1fkehUCAyMtI5RUREOD8TQmDevHl44YUXMGzYMPTo0QP/+c9/cOrUKaxatcqTh0JEREQe4rEwYjKZsGPHDiQlJVX+MKUSSUlJyMjIqPV7JSUlaNOmDWJjYzFs2DDs37/f+VlWVhZyc3Nd9hkQEICEhISL7pOIiIgaL4+Fkfz8fFitVpeeDQCIiIhAbm5ujd/p1KkTFi1ahO+++w5ffPEFbDYb+vfvj5MnTwKA83v12afRaITBYHCZiIiIqPFoVHfTJCYmYuzYsejVqxduuOEGrFixAmFhYfjoo48ue5+pqakICAhwTrGxsW6sMREREV0pj4WR0NBQqFQq5OXluWzPy8tDZGRknfah0Whw9dVXIzMzEwCc36vPPmfMmIGioiLnlJOTU99DISIiIg/yWBjRarXo06cP0tPTndtsNhvS09ORmJhYp31YrVbs3bsXUVFRAID4+HhERka67NNgMGDLli217lOn08Hf399lIiIiosZD7cmdp6SkYNy4cejbty/69euHefPmobS0FOPHjwcAjB07Fq1atUJqaioA4KWXXsK1116L9u3bo7CwEG+++SZOnDiBCRMmAJDutJk+fTrmzp2LDh06ID4+HjNnzkR0dDSGDx/uyUMhIiIiD/FoGBk5ciTOnj2LWbNmITc3F7169cKaNWucA1Czs7OhVFZ2zpw/fx6PPPIIcnNzERQUhD59+mDTpk3o2rWrs8wzzzyD0tJSPProoygsLMSAAQOwZs2aag9HIyIioqZBIYQQcleiIRkMBgQEBKCoqIiXbIiIiOrBU+fQRnU3DREREbU8DCNEREQkK4YRIiIikhXDCBEREcmKYYSIiIhkxTBCREREsmIYISIiIlkxjBAREZGsGEaIiIhIVgwjREREJCuGESIiIpIVwwgRERHJimGEiIiIZMUwQkRERLJiGCEiIiJZMYwQERGRrBhGiIiISFYMI0RERCQrj4eR+fPnIy4uDnq9HgkJCdi6dWutZRcuXIjrr78eQUFBCAoKQlJSUrXyDz74IBQKhct0yy23ePowiIiIyEM8GkaWL1+OlJQUzJ49Gzt37kTPnj2RnJyMM2fO1Fh+/fr1uP/++/Hbb78hIyMDsbGxGDJkCP7++2+XcrfccgtOnz7tnL788ktPHgYRERF5kEIIITy184SEBFxzzTV4//33AQA2mw2xsbGYOnUqnnvuuUt+32q1IigoCO+//z7Gjh0LQOoZKSwsxKpVqy6rTgaDAQEBASgqKoK/v/9l7YOIiKgl8tQ51GM9IyaTCTt27EBSUlLlD1MqkZSUhIyMjDrto6ysDGazGcHBwS7b169fj/DwcHTq1AkTJ05EQUGBW+tOREREDUftqR3n5+fDarUiIiLCZXtERAQOHjxYp308++yziI6Odgk0t9xyC+6++27Ex8fj6NGj+Ne//oWhQ4ciIyMDKpWq2j6MRiOMRqNz3WAwXOYRERERkSd4LIxcqddeew3Lli3D+vXrodfrndtHjRrlXO7evTt69OiBdu3aYf369Rg8eHC1/aSmpuLFF19skDoTERFR/XnsMk1oaChUKhXy8vJctufl5SEyMvKi333rrbfw2muvYe3atejRo8dFy7Zt2xahoaHIzMys8fMZM2agqKjIOeXk5NTvQIiIiMijPBZGtFot+vTpg/T0dOc2m82G9PR0JCYm1vq9N954Ay+//DLWrFmDvn37XvLnnDx5EgUFBYiKiqrxc51OB39/f5eJiIiIGg+P3tqbkpKChQsX4vPPP8eBAwcwceJElJaWYvz48QCAsWPHYsaMGc7yr7/+OmbOnIlFixYhLi4Oubm5yM3NRUlJCQCgpKQETz/9NDZv3ozjx48jPT0dw4YNQ/v27ZGcnOzJQyEiIiIP8eiYkZEjR+Ls2bOYNWsWcnNz0atXL6xZs8Y5qDU7OxtKZWUe+vDDD2EymXDvvfe67Gf27NmYM2cOVCoV9uzZg88//xyFhYWIjo7GkCFD8PLLL0On03nyUIiIiMhDPPqckcaIzxkhIiK6PE3uOSNEREREdcEwQkRERLJiGCEiIiJZMYwQERGRrBhGiIiISFYMI0RERCQrhhEiIiKSFcMIERERyYphhIiIiGTFMEJERESyYhghIiIiWTGMEBERkawYRoiIiEhWDCNEREQkK4YRIiIikhXDCBEREcmKYYSIiIhkxTBCREREsvJ4GJk/fz7i4uKg1+uRkJCArVu3XrT8119/jc6dO0Ov16N79+5YvXq1y+dCCMyaNQtRUVHw8vJCUlISjhw54slDICIiIg/yaBhZvnw5UlJSMHv2bOzcuRM9e/ZEcnIyzpw5U2P5TZs24f7778fDDz+MXbt2Yfjw4Rg+fDj27dvnLPPGG2/g3XffxYIFC7Blyxb4+PggOTkZFRUVnjwUIiIi8hCFEEJ4aucJCQm45ppr8P777wMAbDYbYmNjMXXqVDz33HPVyo8cORKlpaX48ccfnduuvfZa9OrVCwsWLIAQAtHR0XjyySfx1FNPAQCKiooQERGBxYsXY9SoUZesk8FgQEBAAIqKiuDv7++mIyUiImr+PHUOVbttTxcwmUzYsWMHZsyY4dymVCqRlJSEjIyMGr+TkZGBlJQUl23JyclYtWoVACArKwu5ublISkpyfh4QEICEhARkZGTUGEaMRiOMRqNzvaioCIDUoERERFR3jnOnu/sxPBZG8vPzYbVaERER4bI9IiICBw8erPE7ubm5NZbPzc11fu7YVluZC6WmpuLFF1+stj02NrZuB0JEREQuCgoKEBAQ4Lb9eSyMNBYzZsxw6W0pLCxEmzZtkJ2d7daGpNoZDAbExsYiJyeHl8YaCNu84bHNGx7bvOEVFRWhdevWCA4Odut+PRZGQkNDoVKpkJeX57I9Ly8PkZGRNX4nMjLyouUd87y8PERFRbmU6dWrV4371Ol00Ol01bYHBATwl7eB+fv7s80bGNu84bHNGx7bvOEple69/8Vjd9NotVr06dMH6enpzm02mw3p6elITEys8TuJiYku5QEgLS3NWT4+Ph6RkZEuZQwGA7Zs2VLrPomIiKhx8+hlmpSUFIwbNw59+/ZFv379MG/ePJSWlmL8+PEAgLFjx6JVq1ZITU0FAEybNg033HAD3n77bdx2221YtmwZtm/fjo8//hgAoFAoMH36dMydOxcdOnRAfHw8Zs6ciejoaAwfPtyTh0JEREQe4tEwMnLkSJw9exazZs1Cbm4uevXqhTVr1jgHoGZnZ7t09fTv3x9Lly7FCy+8gH/961/o0KEDVq1ahW7dujnLPPPMMygtLcWjjz6KwsJCDBgwAGvWrIFer69TnXQ6HWbPnl3jpRvyDLZ5w2ObNzy2ecNjmzc8T7W5R58zQkRERHQpfDcNERERyYphhIiIiGTFMEJERESyYhghIiIiWTXLMDJ//nzExcVBr9cjISEBW7duvWj5r7/+Gp07d4Zer0f37t2xevXqBqpp81GfNl+4cCGuv/56BAUFISgoCElJSZf8b0TV1ff33GHZsmVQKBS8Hf4y1LfNCwsLMXnyZERFRUGn06Fjx47896We6tvm8+bNQ6dOneDl5YXY2Fg88cQTfKt7Pfz++++44447EB0dDYVC4Xw33MWsX78evXv3hk6nQ/v27bF48eL6/2DRzCxbtkxotVqxaNEisX//fvHII4+IwMBAkZeXV2P5P/74Q6hUKvHGG2+Iv/76S7zwwgtCo9GIvXv3NnDNm676tvno0aPF/Pnzxa5du8SBAwfEgw8+KAICAsTJkycbuOZNV33b3CErK0u0atVKXH/99WLYsGENU9lmor5tbjQaRd++fcWtt94qNm7cKLKyssT69evF7t27G7jmTVd923zJkiVCp9OJJUuWiKysLPHLL7+IqKgo8cQTTzRwzZuu1atXi+eff16sWLFCABArV668aPljx44Jb29vkZKSIv766y/x3nvvCZVKJdasWVOvn9vswki/fv3E5MmTnetWq1VER0eL1NTUGsuPGDFC3HbbbS7bEhISxD//+U+P1rM5qW+bX8hisQg/Pz/x+eefe6qKzc7ltLnFYhH9+/cXn3zyiRg3bhzDSD3Vt80//PBD0bZtW2EymRqqis1Ofdt88uTJ4qabbnLZlpKSIq677jqP1rO5qksYeeaZZ8RVV13lsm3kyJEiOTm5Xj+rWV2mMZlM2LFjB5KSkpzblEolkpKSkJGRUeN3MjIyXMoDQHJycq3lydXltPmFysrKYDab3f7ipebqctv8pZdeQnh4OB5++OGGqGazcjlt/v333yMxMRGTJ09GREQEunXrhldffRVWq7Whqt2kXU6b9+/fHzt27HBeyjl27BhWr16NW2+9tUHq3BK56xzarN7am5+fD6vV6nzCq0NERAQOHjxY43dyc3NrLJ+bm+uxejYnl9PmF3r22WcRHR1d7ReaanY5bb5x40Z8+umn2L17dwPUsPm5nDY/duwY1q1bhwceeACrV69GZmYmJk2aBLPZjNmzZzdEtZu0y2nz0aNHIz8/HwMGDIAQAhaLBY899hj+9a9/NUSVW6TazqEGgwHl5eXw8vKq036aVc8INT2vvfYali1bhpUrV9b5kf5UP8XFxRgzZgwWLlyI0NBQuavTYthsNoSHh+Pjjz9Gnz59MHLkSDz//PNYsGCB3FVrttavX49XX30VH3zwAXbu3IkVK1bgp59+wssvvyx31egSmlXPSGhoKFQqFfLy8ly25+XlITIyssbvREZG1qs8ubqcNnd466238Nprr+HXX39Fjx49PFnNZqW+bX706FEcP34cd9xxh3ObzWYDAKjVahw6dAjt2rXzbKWbuMv5PY+KioJGo4FKpXJu69KlC3Jzc2EymaDVaj1a56buctp85syZGDNmDCZMmAAA6N69u/NdZs8//7zbX3tPtZ9D/f3969wrAjSznhGtVos+ffogPT3duc1msyE9PR2JiYk1ficxMdGlPACkpaXVWp5cXU6bA8Abb7yBl19+GWvWrEHfvn0boqrNRn3bvHPnzti7dy92797tnO68804MGjQIu3fvRmxsbENWv0m6nN/z6667DpmZmc7gBwCHDx9GVFQUg0gdXE6bl5WVVQscjjAo+Bo2j3DbObR+Y2sbv2XLlgmdTicWL14s/vrrL/Hoo4+KwMBAkZubK4QQYsyYMeK5555zlv/jjz+EWq0Wb731ljhw4ICYPXs2b+2tp/q2+WuvvSa0Wq345ptvxOnTp51TcXGxXIfQ5NS3zS/Eu2nqr75tnp2dLfz8/MSUKVPEoUOHxI8//ijCw8PF3Llz5TqEJqe+bT579mzh5+cnvvzyS3Hs2DGxdu1a0a5dOzFixAi5DqHJKS4uFrt27RK7du0SAMQ777wjdu3aJU6cOCGEEOK5554TY8aMcZZ33Nr79NNPiwMHDoj58+fz1l6H9957T7Ru3VpotVrRr18/sXnzZudnN9xwgxg3bpxL+a+++kp07NhRaLVacdVVV4mffvqpgWvc9NWnzdu0aSMAVJtmz57d8BVvwur7e14Vw8jlqW+bb9q0SSQkJAidTifatm0rXnnlFWGxWBq41k1bfdrcbDaLOXPmiHbt2gm9Xi9iY2PFpEmTxPnz5xu+4k3Ub7/9VuO/z452HjdunLjhhhuqfadXr15Cq9WKtm3bis8++6zeP1chBPuuiIiISD7NaswIERERNT0MI0RERCQrhhEiIiKSFcMIERERyYphhIiIiGTFMEJERESyYhghIiIiWTGMEBERkawYRoiIiEhWDCNEREQkK4YRIiIikhXDCBEREcmKYYSIiIhkxTBCREREsmIYISIiIlkxjBCRx3zwwQdQKBRISEio9tnx48ehUCjw1ltv1fjdt956CwqFAsePH6/22cqVKzF06FCEhoZCq9UiOjoaI0aMwLp169x9CETUABhGiMhjlixZgri4OGzduhWZmZlXvD8hBMaPH4+7774beXl5SElJwYIFCzB58mQcO3YMgwcPxqZNm9xQcyJqSGq5K0BEzVNWVhY2bdqEFStW4J///CeWLFmC2bNnX9E+3377bSxevBjTp0/HO++8A4VC4fzs+eefx3//+1+o1fxnjaipYc8IEXnEkiVLEBQUhNtuuw333nsvlixZckX7Ky8vR2pqKjp37uy8hHOhMWPGoF+/flf0c4io4TGMEJFHLFmyBHfffTe0Wi3uv/9+HDlyBNu2bbvs/W3cuBHnzp3D6NGjoVKp3FhTIpIbwwgRud2OHTtw8OBBjBo1CgAwYMAAxMTEXFHvyIEDBwAA3bt3d0sdiajxYBghIrdbsmQJIiIiMGjQIACAQqHAyJEjsWzZMlit1svap8FgAAD4+fm5rZ5E1DgwjBCRW1mtVixbtgyDBg1CVlYWMjMzkZmZiYSEBOTl5SE9Pb1e+3OMDfH39wcAFBcXu73ORCQvhhEicqt169bh9OnTWLZsGTp06OCcRowYAQDOSzV6vR6ANDC1JmVlZS7lOnfuDADYu3evR+tPRA2P98ARkVstWbIE4eHhmD9/frXPVqxYgZUrV2LBggUICwuDt7c3Dh06VON+Dh06BG9vb4SGhgKQxp0EBQXhyy+/xL/+9S8OYiVqRtgzQkRuU15ejhUrVuD222/HvffeW22aMmUKiouL8f3330OlUmHIkCH44YcfkJ2d7bKf7Oxs/PDDDxgyZIgzdHh7e+PZZ5/FgQMH8Oyzz0IIUe3nf/HFF9i6dWuDHCsRuY9C1PR/NBHRZVi+fDlGjRqFVatWYdiwYdU+t9lsiIyMxLXXXovvv/8eBw4cwLXXXguNRoNHH30UcXFxOH78OD7++GOYzWZs3rwZXbp0cfn+gw8+iP/+97/o3bs37r33XkRGRiI3NxerVq3C1q1bsWnTJiQmJjbkYRPRFWIYISK3ufPOO5GWloaCggJ4e3vXWGb8+PFYsmQJTp8+jZCQEBw8eBBz5szBb7/9hnPnziE4OBg33XQTZs+e7RwncqFvv/0WH3/8MbZv3w6DwYCwsDAMHDgQEydOxA033ODJQyQiD2AYISIiIllxzAgRERHJimGEiIiIZMUwQkRERLJiGCEiIiJZMYwQERGRrBhGiIiISFYt7nHwNpsNp06dgp+fn/MFXERERHRpQggUFxcjOjoaSqX7+jNaXBg5deoUYmNj5a4GERFRk5WTk4OYmBi37a/FhRE/Pz8AUkM6XklOREREl2YwGBAbG+s8l7pLiwsjjksz/v7+DCNERESXwd3DHDiAlYiIiGTV4npGiIiI3MlmE7DYBCw2G8xWAYtVmputNlhs0txstcFSZZvFKpWX5hcsW22wCgGbkPZttQnYhGOCtG6zfy5cP7PZXMsJITBpUHtE+OvlbqaLYhghIqIGZbMJmKw2GM02VFisMJptMFqsMFoq51ab68nVesFJ1iakE7rVJmC2SeXNVgGrrerJvnLdERisNUxmm4DJYoXJYoPJaoPZImC02qR1i9UlWFiqhAmrPUTYGvnrZkf1a80wQkRE8rLaBCrMVpSbrSg3WWG0WFFusknrZivMFhssNhtM9r/qLVbpBF/1L3npxCyFBWluc86NVbab7b0CjmWT/aRedbvJapO7STxOpVRAo1JAo1RCrVJAo1JCo5KW1UqFc1mlVEKjVNjLK6FSSp+r7JNSoYBSqYBKAeeyUoHKzxTSulJZ87JKoUCIj1bu5rgkhhEiogZmswkYLbbKgGAPCY71CkdPgdlm70Gwupz0pZ6EyjBRYXJ8z4pys32/psp9myyN9+SvVAB6jQo6tRI6tQo6jf2k7TwRSydU6YRcZZuy9hO54yTvXFYooLLP1fbvqZRwztVKJbRqadKppZ+vVVVuc6w7gkRNwUFtX3cEEKWSz7GqD4YRIqJLsNoESowWlBotKHFMFa7LpUYLSkwWlBmtKDVJ62UmK0qNFpTat5WZrCgzWVBhli8c6DVK6DUqeNknnUYFrUo6mWrsf8Gr7SfXyhNw5YlaV+WkrbUHCJeTuH27VlW5rlEpXNZ1aiV09gCiUfE+CmIYIaJmSAiBcrMVhnILSoxmFFdUhobiC4JEcYUFZVWCQqlR6lVoqPCgVSmh1yjhpZXCgd4eEPRVTthVT/zSiVxalgKF9F1nwKiyH71GBe8qn+nU/IudGieGESJq1EwWG86XmVBQYpLmpSacKzGisNyMwjIzDOVmFJabUVRuRmGZCUXlFhSVm2C2un9UoUalgK9ODV+9Gr46DXx1Kvu6tOyjVcNbp4avTgVvrRo+jnmVZW+ta2BQMRwQMYwQUcOpMFtxrlQKFUVlZpwvM+N8mQmFZSacL5PCRWGZCefKTDhXasK5EhOKjZbL/nkqpQJ+erUUGHTqymW9Bn56Nfx0avjopIDgmDsCw4XLvno1dGqVG1uDiBwYRojoilWYrcgvMSLPYMQZQwXyDBXIKzYiz1CBMwYjcu3biisuL1goFUCwjxbBPloEeWsR4qtFoLcWgV4aBNinQG8N/L00CPTSIsBbg0AvDby1Kr4Qk6gJYBghohpVmK3IM1Qgt6gCZ0uMKCgxIb/EiPwSEwpKjCgoleb5JSaU1KP3Qq1UINBbiyBvDYK8peDgWA701iLQvhziK4WPYG8tArw0HOtA1IwxjBC1QMUVZvxdWI5TheU4VWjvyTBUINdgRF5RBfKKK1BYZq7XPrUqJcL9dYjw1yPCX4dwP71zWZrrEe6vg59Ozd4KInLBMELUzAghcK7UhOxzZfi7sBx/ny93Bo+T9uW6Xi7Ra5SI9Ncj3E+PUD8tQnx0CPHVIsRXhzD7PMRHmvvrGTKI6PIwjBA1QSaLDX8XliP7XJk0FZTal8uRc66sTpdNAr01aBXohagAL0QG6BDhp0dEgB6R9l6MSH89/L0YMIjI8xhGiBqpMpMFJwrK7FMpTpyzzwvKcKqw/JLvw4j01yM22AvRgV5oFeiFVkHSckygNPfR8X9/Imoc+K8RkYyMFiuyC8pw9GwpsvJLkZVfgqx8KXCcKTZe9LteGhVaB3sjNtgbbUK80TrY27keE+QFvYa3oRJR08AwQtQAzpWacDDXgMwzJTh2thTH7MHj7/MX7+EI9NagTbA32oT4oE2INI8L8UbrEG+E+ep4CYWImgWGESI3qjBbkXmmBAdOG3AotxiH8opxMLcYZy/Sy+GnUyM+zAfxoT5oG+qLuFBvxNnDR6B343/bJhHRlWIYIbpMpUYL9v1dhD0ni/DnyUL8ddqA4/mltfZ0tA72RscIP7QL80HbMB/Eh/oiPtQHob5a9nAQUYvGMEJUB0aLFQdOF2PPyUL8mVOEPScLkXm2BKKG4BHkrUGnSD90jvRH50g/dIr0Q8cIPw4YJSKqBf91JLqAxWrDkTMl2HOyEHtOSj0fB3MNNb54LSpAjx4xAegRE4hurQLQJdIPYX4cy0FEVB8MI9Si2WwCWQWlLsFj/6miGl8ZH+yjlYJHKyl89IgNQLifXoZaExE1Lwwj1KIUlZuxO6cQO0+cx87s89idU1jj00j9dGp0axWAHrEB6NEqED1iAhAT5MUeDyIiD2AYoWbLZhM4ll+CnScKsTNbCh9HzlQf56HXKHFVdID9covU6xEf4sMXsxERNRCGEWpWTheV4/fDZ7Hh8Fn8kVmAovLqL3trE+KN3q2D0Lt1IK5uHYTOkX5Qq5Qy1JaIiACGEWriKsxWbDt+zhlADueVuHyuUyvRMyYQvdtI4aN3myCE+upkqi0REdWEYYSaFCEEsvJLneEj41iBy2BTpQLoGRuIgR3CMLBjGHrEBEDDXg8iokaNYYQaPUOFGZsy87HhcD5+P3wWfxeWu3we7qfDDR3DcEOnMAxoH8qnlhIRNTEMI9ToWG0Ce04W4vfD+fj9yFnszimEtcpjTbUqJfrGBeGGjlLvR+dIP97lQkTUhDGMUKOQc64MGzPzsfFIPv44mo/CMteBp23DfOyXXkJxbdsQeGv5q0tE1FzwX3SSRWGZCZuOFjgDSPa5MpfP/fRqXNcuFAM7huH6DqGIDfaWqaZERORpDCPUIKw24bzrZWNmPvb+XeTyvA+VUoGrYwMxoEMoBrQPRa/YQN5uS0TUQjCMkMdYrDZsyTqH1XtP45f9ucgvMbl83iHcF9e1l8JHQttg+Ok1MtWUiIjkxDBCbmW22rDpaAF+tgeQ81XGfvjr1bipczgGdJDueokM4HtdiIiIYYTcwGoT+P3IWfy05zTS/spzeeppsI8WQ7pGYGj3KCS2DYFWzUsvRETkimGELtuZ4gos35qDL7dm41RRhXN7qK8Ot3SLwK3dotAvPphjP4iI6KIYRqhehBDIOFaAJZuz8cv+XFjsz/8I8tbgzp7RuLV7FPrGBUPFl8wREVEdMYxQnRSVm/HtjpNYsuUEjp4tdW7v0yYI/7i2NYZ2i4Jeo5KxhkRE1FQxjNBFHckrxsL/HcP3f55yvgPGR6vC8Ktb4YGENuga7S9zDYmIqKljGKEa7fu7CO+vy8Sa/bnObZ0i/PCPa1tj+NWteBsuERG5jewjC+fPn4+4uDjo9XokJCRg69atFy0/b948dOrUCV5eXoiNjcUTTzyBioqKi36H6m778XMYt2grbn9vozOIJF8Vga8fS8Sa6ddjTGIcgwgREbmVrD0jy5cvR0pKChYsWICEhATMmzcPycnJOHToEMLDw6uVX7p0KZ577jksWrQI/fv3x+HDh/Hggw9CoVDgnXfekeEImgchBP7ILMB7645gS9Y5AIBSAdzZMxqTBrVHxwg/mWtIRETNmUKIqg/lblgJCQm45ppr8P777wMAbDYbYmNjMXXqVDz33HPVyk+ZMgUHDhxAenq6c9uTTz6JLVu2YOPGjXX6mQaDAQEBASgqKoK/f8se7yCEwK8HzuD93zLxZ04hAECjUuCe3jF47IZ2iAv1kbeCRETUqHjqHCpbz4jJZMKOHTswY8YM5zalUomkpCRkZGTU+J3+/fvjiy++wNatW9GvXz8cO3YMq1evxpgxY2r9OUajEUaj0bluMBjcdxBN2P5TRZi5ah92ZhcCAHRqJe7v1xqPDmyL6EAveStHREQtimxhJD8/H1arFRERES7bIyIicPDgwRq/M3r0aOTn52PAgAEQQsBiseCxxx7Dv/71r1p/TmpqKl588UW31r0pK64w4520w/h803HYBOCtVWFsYhweHhCPMD+d3NUjIpKHzQaYioHyQqCi8IJ5kVQmtAMQ2hEIbAOoPHD6FAIoPw8YTgGWCmkdova5zQJYzYDFCFhN0rLVBFiNlcsWE9B3POAT6v76ulGTuptm/fr1ePXVV/HBBx8gISEBmZmZmDZtGl5++WXMnDmzxu/MmDEDKSkpznWDwYDY2NiGqnKjIYTA93+ewtyfDuBssdRTdFv3KLxwexdEBbAnhIhqYSoDygoApRrQBwAaL0BRj4caWs3S90vzgbJ8wFwO6PylfXkFSnOtb932KYR04jWXAaZSadlSYZ+XV1m2z83lUjlTCWAskcKGsdi+XGJfLgaMBilwCFvdjkmlBULa28NJJyCsk7QcFC8dh80ihRubRZqEtXKb1QgUnwaK/gYMf9vnJyvXzWV1b9u66jSUYaQ2oaGhUKlUyMvLc9mel5eHyMjIGr8zc+ZMjBkzBhMmTAAAdO/eHaWlpXj00Ufx/PPPQ6msfnOQTqeDTtey/+LPPFOCWd/tw6ajBQCAuBBvvDSsGwZ2DJO5ZkQtmM0qnQAriqS/wJ3LRVIAEFapjHNuu2DdCljtJzubWTrp26xVli3Sd1QaQK2XTqBqPaDW2Sf7slIjnZDLCi6YzklzS7lrvZUaKUBUnRyhQtiA0gIpdDjCh6NX4WIUKtd9aX2lQGEqA8yl9rl9qmtguFwqnf14AiuPSx8otWf+EaDgiFS3M39Jkyd4hwAaH0ABAApAobSHNYXrXKmR/vuqtPbJvqzWum7TN/7xkbKFEa1Wiz59+iA9PR3Dhw8HIA1gTU9Px5QpU2r8TllZWbXAoVJJT/2UcRxuo1VmsuD9dZlY+L9jMFsFdGolJg9qj0cHtuXTUqlxEUL6C7b0rHQSs5TbT7LWyr8uL1xXKO3/8OqqnGx1rttsFvs+zwIlZyuXq04VRfZ/1LVS13vVf9Sr/mOv8aoyebvO1XppbjFW+Yvb8Ze4fd3x17njr3BjExq/ptRIIUDYw06ZPWjUlUIJeAVLf51rvKT2qCiSLoHYzNJ+y89J0/k67lOlBdRe0n9rjb5K0LpgWesjhRudL6D1A3R+9mX7Np2/tM0RPjSX6Cm22YCibCmYnD0E5B+qXC4/V/24FSpAqZJ6lhQq6XfMNwLwbwUEtAL8Y+zzVkBADOAffek6NEOyXqZJSUnBuHHj0LdvX/Tr1w/z5s1DaWkpxo8fDwAYO3YsWrVqhdTUVADAHXfcgXfeeQdXX3218zLNzJkzcccddzhDCUnS/srDnO/34+9C6a+amzqHY84dV6F1iLfMNaNmzWZz7f422U++jvUKQ5UgkA+UnqkMChf+Bd5SaHyq9zBovKUTmONEplBesF7lBKdUS4FJqaoMT47tCqUUyCwV9slUeQnDarRf0jBKJ2PvECkseIfYp+DKZa2vVFdTac09OY5goVBU2U9o5dwrUKrfhYSQLqVU3Wd5ofR7o9YDWm+pfbTeUptofewB0NszYzbqQqkEguKkqcPNrp+Zyuz/rdT2/058R1ddyRpGRo4cibNnz2LWrFnIzc1Fr169sGbNGueg1uzsbJeekBdeeAEKhQIvvPAC/v77b4SFheGOO+7AK6+8ItchNDp/F5Zjzvf7kfaXdPmrVaAXZt/RFTd3jYCC/2NQ+Xmg4ChQkFk5VRhcewNqWgYqr9ObSqR/dJ3LpVWm4iurn9oL8AmTTj6Of9AdJ9aq6wqV9Je61VT9xOrcZpJOBj5hNUyhgG+4tKwPkHpcnIP/TJWXOhyDAh3jEMzl0mSxz81lgLnCPi+X/hrX+VX5q9vP/te4b+V2vb/0V7gjgDjatynQ2Y8roJV79qdQSP+ttd6Af5R79iknLf/Yu1x1fs7IqVOnEB0d7en6eFxzfc6I2WrDZ39k4f/SjqDcbIVaqcCE69vi8cHt4a1tUuOUWy4hpGv0xaft3bmOv3IvuISgtP/lazVVXkd3nhirzE1lUndy1fBRVtAwx6JUV3Z/6/wrT8Y6P/tfzGGAryMYhEvhwCdMKkdEjZbszxm56qqrMH/+fIwePdptP5zcY8eJc3h+5T4czJX+Kr0mLgiv3NWdT05tjExlQOEJ4PwJ4Pzx6sumEs/XwS8aCGkn3Q0Q0l7qjq/aC+CY26osC2G/9u4t/XWv9amcNFWWHQFErWMXNRHVWZ3DyCuvvIJ//vOfWLlyJT766CMEBwd7sl5UB4VlJry+5iC+3JoDAAjy1mDGrV1wb+8YKJU8EXhE+XkpPDhCRNV5+fkqdzwIaVBe1TsghK1udwL42O9yspqkuyUclw1qo9RUXlN3GVzpZQ8e7SvDR3Bb9j4QUaNTr8fBZ2Vl4eGHH8Zff/2FhQsX4o477vBk3TyiOVymEUJgxc6/8erqAygoNQEARvSNwXNDuyDYRytz7Zo4m026THLumH06Ks3PHwfOZwPGOtymeCm6ACCojX2Kkx6gFBQvrQfESncGXEg4HnBkqrxt03GHR1Mac0BETZrsl2kAID4+HuvWrcP777+Pu+++G126dIFa7bqLnTt3uq1yVN3J82V45ps9zmeGdIzwxdzh3dEvnj1VdWa1AEU5wPksKWScOwacy5LGVpzPkgYqXoxPmD1AxEkBItAeLHzCpLEezjsfFBesK6U7BLwC619nhaJyDAkRUTNT75GNJ06cwIoVKxAUFIRhw4ZVCyPkGY7ekDnf70ex0QK9Rolpgzvi4QHx0KqrP+ytxaswVAkb9rljvTBHumxSG4VKChfBbYHgdtLcGTxaS2MjiIjIbeqVJBYuXIgnn3wSSUlJ2L9/P8LC+ATPhnCu1IR/rdiLNftzAQB92gThnRE90SakBZ8UHe9wcF5OyapcPp8lPbviYlS6ymcFhNgDR3C8NA+IZQ8EEVEDqnMYueWWW7B161a8//77GDt2rCfrRFWsO5iHZ77Zi/wSI9RKBZ64uSMeu6EdVC1hgKrFVOVyiv2OE+cdKMcv/Zhp71ApYATF2cdkxFWu+0ZKDy8iIiLZ1TmMWK1W7NmzBzExMZ6sD9mVGi2Y+9MBfLk1GwDQIdwX/zeyF7q1CpC5Zpep/DxwcgdQkic9B6PqA6ScD5GyPzyq7JwUNgx/A7jE+Gr/VlLQcPRqOOZB8U3ifQxERFSPMJKWlubJelAVO06cR8pXu3GiQHp744QB8XgquVPTeZ+MzQacPQic3ArkbJPm+Ycvb18a7yqDReMqx244llvgOxyIiJobjj5tRGw2gf/79TDm/5YJmwCiA/R4a0RP9G/XSF/97Bi3YTgFFJ0ETu2SgsfJHTXfAusYDOp4BoZab38mhl56DLhjuz6wMnD4hPHhWUREzRzDSCPy3rpMvLcuEwBw99WtMPvOqxDgJfNAytIC4MRGKWwYTknP4DCcBopPAcW5td8Gq/EGWvUBYq4BYhOkuU9Iw9adiIiaBIaRRuK3Q2cwL126lDF3eDf849o28lRECOl12IdWA4fXADlbLv3UUO8Q6UmfEV3t4aMfEH6VfG/VJCKiJoVni0Ygu6AM05fthhDAP65t3fBBxGoBsjOk8HFotXR7bFXhVwFhnQD/aMAvSnq7pl+0fR4lvYeEiIjoMjGMyKzCbMVjX+xAUbkZvWIDMfP2rg3zgy0mIDMN2L8SOJIGVBRWfqbSAnHXA52GAh1vAQJjG6ZORETUIjGMyEgIgedX7sNfpw0I8dHiw3/0hk7twTtmhABObgP+XAbsXyENPnXwCpaCR6dbgHY3SW9eJSIiagAMIzJasiUb3+48CaUCeG/01YgK8NBtqgVHgT1fAXuWSw8Qc/CNBLrfC3S5QxrroWwitw4TEVGzwjAik53Z5/HiD/sBAM/e0tn9t+9WFAF7vwb+XC7dbuug8ZHCR8+RQPwNDCBERCQ7hhEZ5JcYMemLnTBbBYZ2i8SjA9u69weUnAEWJVcORFUogbaDgJ6jgM638UVvRETUqDCMNDCL1YapS3ch11CBdmE+ePO+nlC486FexmJgyb1SEPFvBSROBrrdA/hFuu9nEBERuRHDSAN785dDyDhWAB+tCh+N6QNfnRv/E1hMwPJ/AKf/lF4SN+4H6Y20REREjRhfW9qAft57Gh/9Ll06efO+nmgf7sY7Vmw2YNVE4Nh6aVzIA18ziBARUZPAMNJAjp0twVNf/wkA+OfAtri1e1T1QqUFwOLbgRX/BIrz6r5zIYC1zwP7vgGUamDkf4FWvd1UcyIiIs9iGGkgqT8fRKnJimvbBuPp5E41F9r2CXD8f8CeZcD8a4Cd/5WCxqX88W9g8wfS8vAPgfaD3VdxIiIiD2MYaQA7TpxH2l95UCqAucO7Q62qodmtFmDHYmnZN1K6Nff7KcB/7pSeE1Kb3V8Cv86WlofMBXqMcHv9iYiIPIlhxMOEEHjzl4MAgHv7xKB9uG/NBQ+vkd6E6x0CPL4TuPllQO0FZP0OfNgf2Ph/gNXs+p0jacB3k6XlxClA/6kePBIiIiLPYBjxsP8dycfmY+egVSkxLalj7QW3L5LmvR6QngNy3ePApAyg7Y2ApQL4dQ6wcBBwapdU7uR24KuxgLAC3UdI4YWIiKgJYhjxIKlX5BAAYExiG7QKrOVx7+eOAUfTpeW+4yu3B8cDY1ZJ40D0gUDuXmDhTcBPTwJL7gPMZdJ7ZIbNB5T8T0lERE0Tz2Ae9PO+XOz9uwg+WhUm3XiR22y3fybN2w0Ggi94GqtCAfQaDUzZDnS7FxA2aaBr+Tkg+mpgxH8BtdZzB0FERORhDCMeYrHa8NZaqVdkwvVtEeKrq7mguQLY9YW0fM3Dte/QNwy491Ng9NdAUDwQ0V1a1tUyBoWIiKiJ4BNYPWTFzr9x7Gwpgrw1mHB9fO0F//pO6uXwbwV0SL70jjsOATrcLC278zHyREREMmEY8YAKsxXzfj0MAJg8qD389JraC2//VJr3eRBQ1fE/B0MIERE1I7xM4wFLtmTjVFEFogL0+Me1bWovmLsPyNkiPTW199iGqyAREVEjwjDiZiVGC+b/lgkAmDa4A/QaVe2FHb0inW/jW3WJiKjFYhhxs0/+dwznSk1oG+qDe/vE1F7QWAzs+Upa7nuRgatERETNHMOIG50rNeGT/2UBAJ4c0qnmx7477FkOmEqAkA5A/MAGqiEREVHjwzDiRh/8lokSowXdWvljaLeLXHYRAthmf+Jq34c4IJWIiFo0hhE3OVVYjv9sPgEAeDq5M5TKiwSMnC3Amf3Su2d63d9ANSQiImqcGEbc5N30IzBZbEiID8bADqEXL7zNPnC12z2AV5DnK0dERNSIMYy4wbGzJfh6x0kAwDO3dIbiYpddSguAv1ZJy9c85PnKERERNXKyh5H58+cjLi4Oer0eCQkJ2Lp160XLFxYWYvLkyYiKioJOp0PHjh2xevXqBqptzd5OOwyrTSCpSzj6tLlET8fuLwCrCYjqBbTq0yD1IyIiasxkfQLr8uXLkZKSggULFiAhIQHz5s1DcnIyDh06hPDw8GrlTSYTbr75ZoSHh+Obb75Bq1atcOLECQQGBjZ85au4vn0odp04j6eSO128oM1W+VK8i72HhoiIqAVRCCGEXD88ISEB11xzDd5//30AgM1mQ2xsLKZOnYrnnnuuWvkFCxbgzTffxMGDB6HRXOQR6xdhMBgQEBCAoqIi+Pv7X1H9q7JYbRe/lRcAMn8FvrgH0AUATx4AtD5u+/lERESe5qlzqGyXaUwmE3bs2IGkpKTKyiiVSEpKQkZGRo3f+f7775GYmIjJkycjIiIC3bp1w6uvvgqr1VrrzzEajTAYDC6TJ1wyiACVt/P2HMUgQkREZCdbGMnPz4fVakVERITL9oiICOTm5tb4nWPHjuGbb76B1WrF6tWrMXPmTLz99tuYO3durT8nNTUVAQEBzik2Ntatx1FnRSeBwz9Ly305cJWIiMhB9gGs9WGz2RAeHo6PP/4Yffr0wciRI/H8889jwYIFtX5nxowZKCoqck45OTkNWGM7IYA1zwHCBrQZAIR3bvg6EBERNVKyDWANDQ2FSqVCXl6ey/a8vDxERtb89NKoqChoNBqoVJUvn+vSpQtyc3NhMpmg1WqrfUen00Gn07m38vWV8T5w4AdAqQFufkneuhARETUysvWMaLVa9OnTB+np6c5tNpsN6enpSExMrPE71113HTIzM2Gz2ZzbDh8+jKioqBqDSKNwYhOQNltaviUViOHtvERERFXJepkmJSUFCxcuxOeff44DBw5g4sSJKC0txfjx4wEAY8eOxYwZM5zlJ06ciHPnzmHatGk4fPgwfvrpJ7z66quYPHmyXIdwccV5wNcPAsIKdL8PuGaC3DUiIiJqdGR9zsjIkSNx9uxZzJo1C7m5uejVqxfWrFnjHNSanZ0NpbIyL8XGxuKXX37BE088gR49eqBVq1aYNm0ann32WbkOoXZWC/DNQ0BJHhDWGbjj33whHhERUQ1kfc6IHDx1j3Q1abOBP+YBWl/gkd+AsI6e+1lEREQNoNk9Z6RZO/iTFEQA4M73GESIiIgugmHE3c4dA1ZOlJYTJgLd7pa3PkRERI0cw4g7mcuB5WMBYxEQm8DbeImIiOqAYcSdVj8F5O0FvEOB+xYD6kZ6uzEREVEjwjDiLjv/A+z6AlAogXs/Bfyj5a4RERFRk8Aw4g6n/wR+ekpaHvQ80PZGWatDRETUlDCMXCkhgO+mAFYj0PEWYECK3DUiIiJqUhhGrpRCIY0P6XQrcNcCQMkmJSIiqg9Zn8DabIS0A+7/Uu5aEBERNUn8M56IiIhkxTBCREREsmpxl2kcr+IxGAwy14SIiKhpcZw73f1auxYXRoqLiwFIbwAmIiKi+isuLkZAQIDb9tfi3tprs9lw6tQp+Pn5QaFQuGWfBoMBsbGxyMnJ8eybgAkA27uhsb0bFtu7YbG960cIgeLiYkRHR0PpxrtHW1zPiFKpRExMjEf27e/vz1/mBsT2blhs74bF9m5YbO+6c2ePiAMHsBIREZGsGEaIiIhIVgwjbqDT6TB79mzodDq5q9IisL0bFtu7YbG9Gxbbu3FocQNYiYiIqHFhzwgRERHJimGEiIiIZMUwQkRERLJiGCEiIiJZMYy4wfz58xEXFwe9Xo+EhARs3bpV7io1C7///jvuuOMOREdHQ6FQYNWqVS6fCyEwa9YsREVFwcvLC0lJSThy5Ig8lW3iUlNTcc0118DPzw/h4eEYPnw4Dh065FKmoqICkydPRkhICHx9fXHPPfcgLy9Ppho3bR9++CF69OjhfNBWYmIifv75Z+fnbGvPeu2116BQKDB9+nTnNra5vBhGrtDy5cuRkpKC2bNnY+fOnejZsyeSk5Nx5swZuavW5JWWlqJnz56YP39+jZ+/8cYbePfdd7FgwQJs2bIFPj4+SE5ORkVFRQPXtOnbsGEDJk+ejM2bNyMtLQ1msxlDhgxBaWmps8wTTzyBH374AV9//TU2bNiAU6dO4e6775ax1k1XTEwMXnvtNezYsQPbt2/HTTfdhGHDhmH//v0A2NaetG3bNnz00Ufo0aOHy3a2ucwEXZF+/fqJyZMnO9etVquIjo4WqampMtaq+QEgVq5c6Vy32WwiMjJSvPnmm85thYWFQqfTiS+//FKGGjYvZ86cEQDEhg0bhBBS22o0GvH11187yxw4cEAAEBkZGXJVs1kJCgoSn3zyCdvag4qLi0WHDh1EWlqauOGGG8S0adOEEPz9bgzYM3IFTCYTduzYgaSkJOc2pVKJpKQkZGRkyFiz5i8rKwu5ubkubR8QEICEhAS2vRsUFRUBAIKDgwEAO3bsgNlsdmnvzp07o3Xr1mzvK2S1WrFs2TKUlpYiMTGRbe1BkydPxm233ebStgB/vxuDFveiPHfKz8+H1WpFRESEy/aIiAgcPHhQplq1DLm5uQBQY9s7PqPLY7PZMH36dFx33XXo1q0bAKm9tVotAgMDXcqyvS/f3r17kZiYiIqKCvj6+mLlypXo2rUrdu/ezbb2gGXLlmHnzp3Ytm1btc/4+y0/hhEicjF58mTs27cPGzdulLsqzVqnTp2we/duFBUV4ZtvvsG4ceOwYcMGuavVLOXk5GDatGlIS0uDXq+XuzpUA16muQKhoaFQqVTVRlzn5eUhMjJSplq1DI72Zdu715QpU/Djjz/it99+Q0xMjHN7ZGQkTCYTCgsLXcqzvS+fVqtF+/bt0adPH6SmpqJnz57497//zbb2gB07duDMmTPo3bs31Go11Go1NmzYgHfffRdqtRoRERFsc5kxjFwBrVaLPn36ID093bnNZrMhPT0diYmJMtas+YuPj0dkZKRL2xsMBmzZsoVtfxmEEJgyZQpWrlyJdevWIT4+3uXzPn36QKPRuLT3oUOHkJ2dzfZ2E5vNBqPRyLb2gMGDB2Pv3r3YvXu3c+rbty8eeOAB5zLbXF68THOFUlJSMG7cOPTt2xf9+vXDvHnzUFpaivHjx8tdtSavpKQEmZmZzvWsrCzs3r0bwcHBaN26NaZPn465c+eiQ4cOiI+Px8yZMxEdHY3hw4fLV+kmavLkyVi6dCm+++47+Pn5Oa+TBwQEwMvLCwEBAXj44YeRkpKC4OBg+Pv7Y+rUqUhMTMS1114rc+2bnhkzZmDo0KFo3bo1iouLsXTpUqxfvx6//PIL29oD/Pz8nOOfHHx8fBASEuLczjaXmdy38zQH7733nmjdurXQarWiX79+YvPmzXJXqVn47bffBIBq07hx44QQ0u29M2fOFBEREUKn04nBgweLQ4cOyVvpJqqmdgYgPvvsM2eZ8vJyMWnSJBEUFCS8vb3FXXfdJU6fPi1fpZuwhx56SLRp00ZotVoRFhYmBg8eLNauXev8nG3teVVv7RWCbS43hRBCyJSDiIiIiDhmhIiIiOTFMEJERESyYhghIiIiWTGMEBERkawYRoiIiEhWDCNEREQkK4YRIiIikhXDCBEREcmKYYSIZGW1WtG/f3/cfffdLtuLiooQGxuL559/XqaaEVFD4RNYiUh2hw8fRq9evbBw4UI88MADAICxY8fizz//xLZt26DVamWuIRF5EsMIETUK7777LubMmYP9+/dj69atuO+++7Bt2zb07NlT7qoRkYcxjBBRoyCEwE033QSVSoW9e/di6tSpeOGFF+SuFhE1AIYRImo0Dh48iC5duqB79+7YuXMn1Gq13FUiogbAAaxE1GgsWrQI3t7eyMrKwsmTJ+WuDhE1EPaMEFGjsGnTJtxwww1Yu3Yt5s6dCwD49ddfoVAoZK4ZEXkae0aISHZlZWV48MEHMXHiRAwaNAiffvoptm7digULFshdNSJqAOwZISLZTZs2DatXr8aff/4Jb29vAMBHH32Ep556Cnv37kVcXJy8FSQij2IYISJZbdiwAYMHD8b69esxYMAAl8+Sk5NhsVh4uYaomWMYISIiIllxzAgRERHJimGEiIiIZMUwQkRERLJiGCEiIiJZMYwQERGRrBhGiIiISFYMI0RERCQrhhEiIiKSFcMIERERyYphhIiIiGTFMEJERESyYhghIiIiWf0/p9IEnqNjxw4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 600x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fm_nets', 'autoint_nets']\n",
      "{'loss': 6.319366931915283, 'auc': 0.7930574417114258}\n",
      "============val_data===================\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "     Android_Adware     0.6799    0.7399    0.7086     13905\n",
      "Android_SMS_Malware     0.6620    0.6305    0.6459      6141\n",
      "  Android_Scareware     0.7347    0.4756    0.5774     10823\n",
      "             Benign     0.2368    0.5487    0.3309      2187\n",
      "\n",
      "           accuracy                         0.6204     33056\n",
      "          macro avg     0.5783    0.5987    0.5657     33056\n",
      "       weighted avg     0.6652    0.6204    0.6290     33056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = dt.evaluate(x_test,y_test)\n",
    "print(result)\n",
    "\n",
    "#scoring\n",
    "preds = dt.predict(x_test)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(6, 6))\n",
    "axes[0].plot(history.history['loss'], label='train_loss')\n",
    "axes[0].plot(history.history['val_loss'], label='val_loss')\n",
    "axes[0].set_title('Loss')\n",
    "# axes[1].plot(history.history['accuracy'], label='train_accuracy')\n",
    "# axes[1].plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "# axes[1].set_title('Accuracy')\n",
    "axes[2].plot(history.history['AUC'], label='train_AUC')\n",
    "axes[2].plot(history.history['val_AUC'], label='val_AUC')\n",
    "axes[2].set_title('AUC')\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=0.5)\n",
    "fig.suptitle('Loss and Accuracy')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()\n",
    "print(config.nets)\n",
    "print(result)\n",
    "print('============val_data===================')\n",
    "print(classification_report(y_test, preds, target_names=names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6be80b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptables.models import deepnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0972b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e98d3735",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(\n",
    "    metrics=['AUC'],\n",
    "#     auto_discrete=True,\n",
    "#     auto_categorize=True,\n",
    "    embeddings_output_dim=100, \n",
    "    nets =deepnets.xDeepFM,\n",
    "    earlystopping_patience=5,\n",
    "    apply_gbm_features=True,\n",
    "#     dnn_params={\n",
    "#         'hidden_units': ((64, 0.2, True), (32, 0.2, True),(16, 0.2, True),(16, 0.2, True),(8, 0.2, True),\n",
    "#                          ),\n",
    "#         'activation': 'relu',\n",
    "#     },\n",
    ")\n",
    "dt = deeptable.DeepTable(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dcf2cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-20 13:41:49 I deeptables.m.deeptable.py 338 - X.Shape=(264440, 80), y.Shape=(264440,), batch_size=128, config=ModelConfig(name='conf-1', nets=['linear', 'dnn_nets', 'cin_nets'], categorical_columns='auto', exclude_columns=[], task='auto', pos_label=None, metrics=['AUC'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=False, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0, False), (64, 0, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-20 13:41:49 I deeptables.m.deeptable.py 339 - metrics:['AUC']\n",
      "04-20 13:41:50 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-20 13:41:50 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-20 13:41:50 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.35306715965270996s\n",
      "04-20 13:41:50 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-20 13:41:51 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5914182662963867s\n",
      "04-20 13:41:51 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-20 13:41:51 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.0s\n",
      "04-20 13:41:51 I deeptables.m.preprocessor.py 423 - Extracting GBM features...\n",
      "04-20 13:41:51 I hypernets.t.sklearn_ex.py 640 - LightGBM task:multiclass\n",
      "04-20 13:41:53 I deeptables.m.preprocessor.py 434 - Extracting gbm features taken 2.5022780895233154s\n",
      "04-20 13:41:55 I deeptables.m.preprocessor.py 196 - fit_transform taken 5.571637153625488s\n",
      "04-20 13:41:55 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-20 13:41:56 I deeptables.m.preprocessor.py 249 - transform_X taken 0.19098925590515137s\n",
      "04-20 13:41:56 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-20 13:41:56 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0029921531677246094s\n",
      "04-20 13:41:56 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-20 13:41:56 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-20 13:41:56 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "04-20 13:41:57 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "04-20 13:41:57 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-20 13:41:58 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (104)', 'input_continuous_all: (70)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 10470)\n",
      "---------------------------------------------------------\n",
      "nets: ['linear', 'dnn_nets', 'cin_nets']\n",
      "---------------------------------------------------------\n",
      "linear: input_shape (None, 174), output_shape (None, 1)\n",
      "dnn: input_shape (None, 10470), output_shape (None, 64)\n",
      "cin: input_shape (None, 104, 100), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-20 13:41:58 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "model, history = dt.fit(x_train, y_train, epochs=epochs, validation_data=(x_val, y_val),class_weight=class_weight,\n",
    "                        batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf05e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dt.evaluate(x_test,y_test)\n",
    "print(result)\n",
    "\n",
    "#scoring\n",
    "preds = dt.predict(x_test)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(6, 6))\n",
    "axes[0].plot(history.history['loss'], label='train_loss')\n",
    "axes[0].plot(history.history['val_loss'], label='val_loss')\n",
    "axes[0].set_title('Loss')\n",
    "# axes[1].plot(history.history['accuracy'], label='train_accuracy')\n",
    "# axes[1].plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "# axes[1].set_title('Accuracy')\n",
    "axes[2].plot(history.history['AUC'], label='train_AUC')\n",
    "axes[2].plot(history.history['val_AUC'], label='val_AUC')\n",
    "axes[2].set_title('AUC')\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=0.5)\n",
    "fig.suptitle('Loss and Accuracy')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()\n",
    "print(config.nets)\n",
    "print(result)\n",
    "print('============val_data===================')\n",
    "print(classification_report(y_test, preds, target_names=names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d0e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(\n",
    "    metrics=['AUC'],\n",
    "    auto_discrete=False,\n",
    "    auto_categorize=False,\n",
    "#     auto_imputation=False,\n",
    "    embeddings_output_dim=300, \n",
    "#     embeddings_initializer='glorot_uniform',\n",
    "    dense_dropout=0.3,\n",
    "#     embeddings_regularizer='l2',\n",
    "    categorical_columns=categorical_columns,\n",
    "    stacking_op='add',\n",
    "#     apply_gbm_features=True,\n",
    "    nets =['fm_nets','linear','dnn_nets'],\n",
    "    earlystopping_patience=5,\n",
    "    dnn_params={\n",
    "    'hidden_units': ((300, 0.2, False), (300, 0.2, False)),\n",
    "    'dnn_activation': 'relu'\n",
    "    },\n",
    "    \n",
    ")\n",
    "dt = deeptable.DeepTable(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f90c5d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, history = dt.fit(x_train, y_train, epochs=epochs, validation_data=(x_val, y_val),class_weight=class_weight,\n",
    "                        batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc8fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dt.evaluate(x_test,y_test)\n",
    "print(result)\n",
    "\n",
    "#scoring\n",
    "preds = dt.predict(x_test)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(6, 6))\n",
    "axes[0].plot(history.history['loss'], label='train_loss')\n",
    "axes[0].plot(history.history['val_loss'], label='val_loss')\n",
    "axes[0].set_title('Loss')\n",
    "# axes[1].plot(history.history['accuracy'], label='train_accuracy')\n",
    "# axes[1].plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "# axes[1].set_title('Accuracy')\n",
    "axes[2].plot(history.history['AUC'], label='train_AUC')\n",
    "axes[2].plot(history.history['val_AUC'], label='val_AUC')\n",
    "axes[2].set_title('AUC')\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=0.5)\n",
    "fig.suptitle('Loss and Accuracy')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()\n",
    "print(config.nets)\n",
    "print(result)\n",
    "print('============val_data===================')\n",
    "print(classification_report(y_test, preds, target_names=names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01988b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "x_ = data0.iloc[:, :-1]\n",
    "y_ = data0.iloc[:, -1:].copy()\n",
    "print('x_',x_)\n",
    "print('=====================')\n",
    "print('y_',y_)\n",
    "\n",
    "# Select the top k features using mutual information\n",
    "k = 10  # Choose the number of top features you want to select\n",
    "selector = SelectKBest(mutual_info_classif, k=k)\n",
    "selector.fit(x_, y_.values.ravel())\n",
    "\n",
    "# Get the indices of the top k features\n",
    "top_k_features_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Get the top k features\n",
    "x_top_k = x_.iloc[:, top_k_features_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7bbd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3d5723e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params, X_train, y_train, X_val, y_val, task='multiclass'):\n",
    "    senet_pooling_op, senet_reduction_ratio, bilinear_type = params\n",
    "    # 创建模型配置\n",
    "    config = ModelConfig(\n",
    "        nets=['fibi_nets'],\n",
    "        categorical_columns=categorical_columns,\n",
    "        metrics=['AUC', 'accuracy'],\n",
    "        pnn_params={\n",
    "            'outer_product_kernel_type': 'mat',\n",
    "        },\n",
    "        afm_params={\n",
    "            'attention_factor': 5,\n",
    "            'dropout_rate': 0.3,\n",
    "        },\n",
    "        fgcnn_params={\n",
    "            'fg_filters': (14, 16),\n",
    "            'fg_widths': (7, 7),\n",
    "            'fg_pool_widths': (2, 2),\n",
    "            'fg_new_feat_filters': (2, 2),\n",
    "        },\n",
    "        fibinet_params={\n",
    "            'senet_pooling_op': senet_pooling_op,\n",
    "            'senet_reduction_ratio': senet_reduction_ratio,\n",
    "            'bilinear_type': bilinear_type,\n",
    "        },\n",
    "        cross_params={\n",
    "            'num_cross_layer': 5,\n",
    "        },\n",
    "        autoint_params={\n",
    "            'num_attention': 3,\n",
    "            'num_heads': 1,\n",
    "            'dropout_rate': 0,\n",
    "            'use_residual': True\n",
    "        },\n",
    "        apply_class_weight=True,\n",
    "        stacking_op='add',\n",
    "        dnn_params={\n",
    "            'hidden_units':((128, 0.2, False),(128, 0.2, False)),\n",
    "            'activation': 'relu',\n",
    "        },\n",
    "        cin_params={\n",
    "            'cross_layer_size': (128, 128),\n",
    "            'activation': 'relu',\n",
    "            'use_residual': False,\n",
    "            'use_bias': False,\n",
    "            'direct': False,\n",
    "            'reduce_D': False,\n",
    "        },\n",
    "        earlystopping_patience=5,\n",
    "        embeddings_output_dim=100,\n",
    "        dense_dropout=0.2,\n",
    "        home_dir='E:/temp',\n",
    "\n",
    "    )\n",
    "\n",
    "    # 用更新后的配置创建和训练模型\n",
    "    print('===================================================')\n",
    "    print(config)\n",
    "    print('===================================================')\n",
    "    dt = DeepTable(config=config)\n",
    "    model, history = dt.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val),class_weight=class_weight,\n",
    "                        batch_size=1024,shuffle=True)\n",
    "\n",
    "    print(history.history)\n",
    "    best_score = np.max(history.history['val_AUC'])\n",
    "\n",
    "    return -best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "611193e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = [\n",
    "    Categorical(['max', 'mean'], name='senet_pooling_op'),\n",
    "    Integer(2, 6, name='senet_reduction_ratio'),\n",
    "    Categorical(['field_interaction', 'full'], name='bilinear_type'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "43c01766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 15:23:22 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 15:23:22 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 15:23:22 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-25 15:23:22 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 15:23:23 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.39694929122924805s\n",
      "04-25 15:23:23 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-25 15:23:23 I deeptables.m.preprocessor.py 383 - Imputation taken 0.534602165222168s\n",
      "04-25 15:23:23 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 15:23:24 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.29721593856811523s\n",
      "04-25 15:23:24 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.4626421928405762s\n",
      "04-25 15:23:24 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 15:23:24 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13215184211730957s\n",
      "04-25 15:23:24 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 15:23:24 I deeptables.m.preprocessor.py 236 - transform_y taken 0.000997304916381836s\n",
      "04-25 15:23:24 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 15:23:24 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 15:23:24 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 15:23:24 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 15:23:24 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 15:23:24 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 15:23:24 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9435 - auc: 0.7198 - accuracy: 0.4755 - val_loss: 1.0295 - val_auc: 0.8003 - val_accuracy: 0.5315\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.4381 - auc: 0.8555 - accuracy: 0.6074 - val_loss: 0.9689 - val_auc: 0.8281 - val_accuracy: 0.5558\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.0990 - auc: 0.9127 - accuracy: 0.7108 - val_loss: 0.9602 - val_auc: 0.8483 - val_accuracy: 0.5943\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.8131 - auc: 0.9490 - accuracy: 0.7841 - val_loss: 0.9731 - val_auc: 0.8623 - val_accuracy: 0.6234\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.6272 - auc: 0.9683 - accuracy: 0.8293 - val_loss: 0.9871 - val_auc: 0.8742 - val_accuracy: 0.6500\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5032 - auc: 0.9789 - accuracy: 0.8607 - val_loss: 1.0203 - val_auc: 0.8813 - val_accuracy: 0.6636\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4153 - auc: 0.9851 - accuracy: 0.8836 - val_loss: 1.0548 - val_auc: 0.8855 - val_accuracy: 0.6751\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3602 - auc: 0.9885 - accuracy: 0.8979 - val_loss: 1.0858 - val_auc: 0.8883 - val_accuracy: 0.6839\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3210 - auc: 0.9908 - accuracy: 0.9076 - val_loss: 1.1253 - val_auc: 0.8881 - val_accuracy: 0.6866\n",
      "Epoch 10/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2950 - auc: 0.9922 - accuracy: 0.9143 - val_loss: 1.1427 - val_auc: 0.8901 - val_accuracy: 0.6880\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2744 - auc: 0.9932 - accuracy: 0.9198 - val_loss: 1.1726 - val_auc: 0.8892 - val_accuracy: 0.6898\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2584 - auc: 0.9939 - accuracy: 0.9234 - val_loss: 1.1751 - val_auc: 0.8904 - val_accuracy: 0.6926\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2450 - auc: 0.9946 - accuracy: 0.9269 - val_loss: 1.1910 - val_auc: 0.8899 - val_accuracy: 0.6930\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2355 - auc: 0.9950 - accuracy: 0.9287 - val_loss: 1.2060 - val_auc: 0.8900 - val_accuracy: 0.6921\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2282 - auc: 0.9953 - accuracy: 0.9303 - val_loss: 1.2336 - val_auc: 0.8888 - val_accuracy: 0.6922\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2221 - auc: 0.9956 - accuracy: 0.9318 - val_loss: 1.2386 - val_auc: 0.8886 - val_accuracy: 0.6929\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2157 - auc: 0.9958 - accuracy: 0.9331 - val_loss: 1.2561 - val_auc: 0.8888 - val_accuracy: 0.6940\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "04-25 15:26:48 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 15:26:48 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 15:26:48 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425152322_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.943516731262207, 1.4380598068237305, 1.0989912748336792, 0.8131374716758728, 0.6271766424179077, 0.5032327175140381, 0.4152631461620331, 0.360183447599411, 0.3209531307220459, 0.29503899812698364, 0.2744392156600952, 0.25836554169654846, 0.24500833451747894, 0.2355274260044098, 0.22818301618099213, 0.2221294790506363, 0.21568843722343445], 'auc': [0.7197953462600708, 0.8555482029914856, 0.9126542210578918, 0.9490285515785217, 0.9682625532150269, 0.9788854718208313, 0.9850745797157288, 0.9885199069976807, 0.9907861351966858, 0.992215633392334, 0.9932329654693604, 0.9939427971839905, 0.9945796728134155, 0.9950383305549622, 0.9953325986862183, 0.9955584406852722, 0.9957613348960876], 'accuracy': [0.47545549273490906, 0.6073548793792725, 0.7107682824134827, 0.7840668559074402, 0.8292778134346008, 0.8607323169708252, 0.8836303949356079, 0.897869884967804, 0.9075544476509094, 0.9143480658531189, 0.9198161363601685, 0.9234226942062378, 0.9268600344657898, 0.9287144541740417, 0.9302938580513, 0.9318133592605591, 0.933149516582489], 'val_loss': [1.0294771194458008, 0.9688880443572998, 0.9601826667785645, 0.9730520248413086, 0.9870600700378418, 1.0202572345733643, 1.054758906364441, 1.0858052968978882, 1.1253355741500854, 1.1426641941070557, 1.172600507736206, 1.1751010417938232, 1.1909505128860474, 1.2059944868087769, 1.233623743057251, 1.238563060760498, 1.2560919523239136], 'val_auc': [0.8002670407295227, 0.8280901908874512, 0.8482716083526611, 0.8622530102729797, 0.874176025390625, 0.8813400268554688, 0.8854992985725403, 0.8882576823234558, 0.8880534172058105, 0.8901051878929138, 0.8891654014587402, 0.8903732299804688, 0.8899078965187073, 0.8899989724159241, 0.8888496160507202, 0.8885865211486816, 0.8888155817985535], 'val_accuracy': [0.5314571261405945, 0.5558416247367859, 0.5942826867103577, 0.6234019994735718, 0.6500355005264282, 0.6636481881141663, 0.6751006245613098, 0.6839488744735718, 0.686582624912262, 0.6880326867103577, 0.6897786259651184, 0.6926491260528564, 0.6930338740348816, 0.6920868754386902, 0.6922052502632141, 0.6928563117980957, 0.6939512491226196]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 4, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 15:26:48 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 4, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 15:26:48 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 15:26:48 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-25 15:26:48 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 15:26:49 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.3969533443450928s\n",
      "04-25 15:26:49 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-25 15:26:49 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5305917263031006s\n",
      "04-25 15:26:49 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 15:26:50 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.28623437881469727s\n",
      "04-25 15:26:50 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.440678358078003s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 15:26:50 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 15:26:50 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13763189315795898s\n",
      "04-25 15:26:50 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 15:26:50 I deeptables.m.preprocessor.py 236 - transform_y taken 0.001995086669921875s\n",
      "04-25 15:26:50 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 15:26:50 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 15:26:50 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 15:26:50 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 15:26:50 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 15:26:51 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 15:26:51 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9404 - auc: 0.7208 - accuracy: 0.4771 - val_loss: 1.0177 - val_auc: 0.8079 - val_accuracy: 0.5535\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.4057 - auc: 0.8630 - accuracy: 0.6204 - val_loss: 0.9264 - val_auc: 0.8447 - val_accuracy: 0.5808\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.0109 - auc: 0.9254 - accuracy: 0.7388 - val_loss: 0.9376 - val_auc: 0.8567 - val_accuracy: 0.6057\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.7506 - auc: 0.9551 - accuracy: 0.7970 - val_loss: 0.9753 - val_auc: 0.8644 - val_accuracy: 0.6298\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.6007 - auc: 0.9695 - accuracy: 0.8327 - val_loss: 0.9941 - val_auc: 0.8725 - val_accuracy: 0.6476\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5018 - auc: 0.9781 - accuracy: 0.8597 - val_loss: 1.0114 - val_auc: 0.8792 - val_accuracy: 0.6649\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4346 - auc: 0.9833 - accuracy: 0.8785 - val_loss: 1.0462 - val_auc: 0.8821 - val_accuracy: 0.6764\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3843 - auc: 0.9869 - accuracy: 0.8917 - val_loss: 1.0630 - val_auc: 0.8850 - val_accuracy: 0.6833\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3457 - auc: 0.9893 - accuracy: 0.9019 - val_loss: 1.0969 - val_auc: 0.8861 - val_accuracy: 0.6858\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3158 - auc: 0.9910 - accuracy: 0.9092 - val_loss: 1.1371 - val_auc: 0.8859 - val_accuracy: 0.6897\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2940 - auc: 0.9922 - accuracy: 0.9145 - val_loss: 1.1420 - val_auc: 0.8883 - val_accuracy: 0.6937\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2720 - auc: 0.9933 - accuracy: 0.9197 - val_loss: 1.1718 - val_auc: 0.8882 - val_accuracy: 0.6976\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2596 - auc: 0.9939 - accuracy: 0.9226 - val_loss: 1.1713 - val_auc: 0.8898 - val_accuracy: 0.6983\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2451 - auc: 0.9946 - accuracy: 0.9262 - val_loss: 1.2079 - val_auc: 0.8887 - val_accuracy: 0.6979\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2364 - auc: 0.9950 - accuracy: 0.9280 - val_loss: 1.2211 - val_auc: 0.8885 - val_accuracy: 0.6981\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2278 - auc: 0.9953 - accuracy: 0.9303 - val_loss: 1.2020 - val_auc: 0.8904 - val_accuracy: 0.6986\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2239 - auc: 0.9954 - accuracy: 0.9312 - val_loss: 1.2186 - val_auc: 0.8900 - val_accuracy: 0.7017\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2168 - auc: 0.9957 - accuracy: 0.9327 - val_loss: 1.2208 - val_auc: 0.8899 - val_accuracy: 0.6999\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2120 - auc: 0.9959 - accuracy: 0.9336 - val_loss: 1.2406 - val_auc: 0.8892 - val_accuracy: 0.7000\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2058 - auc: 0.9961 - accuracy: 0.9347 - val_loss: 1.2642 - val_auc: 0.8882 - val_accuracy: 0.7130\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2027 - auc: 0.9963 - accuracy: 0.9358 - val_loss: 1.2818 - val_auc: 0.8886 - val_accuracy: 0.7014\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "04-25 15:31:00 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 15:31:00 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 15:31:00 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425152648_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9404041767120361, 1.4057320356369019, 1.010908842086792, 0.7505956888198853, 0.6006608009338379, 0.5017531514167786, 0.43455633521080017, 0.38428229093551636, 0.3457326292991638, 0.3158171772956848, 0.2939854860305786, 0.2720220386981964, 0.2596350610256195, 0.2451481968164444, 0.23640111088752747, 0.22779905796051025, 0.2238730490207672, 0.216808021068573, 0.21196141839027405, 0.2058112621307373, 0.20272710919380188], 'auc': [0.720818817615509, 0.8630074858665466, 0.9254304766654968, 0.955075204372406, 0.9694982171058655, 0.978094756603241, 0.9833217263221741, 0.9868926405906677, 0.9892953038215637, 0.9910262823104858, 0.9921843409538269, 0.9932571649551392, 0.9939147233963013, 0.9945882558822632, 0.9949746131896973, 0.9953164458274841, 0.995449423789978, 0.9957218170166016, 0.9959333539009094, 0.9961372017860413, 0.9963071942329407], 'accuracy': [0.47710543870925903, 0.6204239130020142, 0.7388312220573425, 0.7969560623168945, 0.8326905369758606, 0.8597487211227417, 0.8784937858581543, 0.8916791081428528, 0.9018572568893433, 0.9092149138450623, 0.9144608974456787, 0.9196715354919434, 0.9225801229476929, 0.9262430667877197, 0.9280269742012024, 0.9302656650543213, 0.9312175512313843, 0.9326841831207275, 0.9335620403289795, 0.9347289800643921, 0.9358077645301819], 'val_loss': [1.017669439315796, 0.9264175891876221, 0.9375877976417542, 0.9752540588378906, 0.9940515160560608, 1.0114363431930542, 1.0461671352386475, 1.0629881620407104, 1.0969305038452148, 1.1370805501937866, 1.1420313119888306, 1.1717710494995117, 1.1712710857391357, 1.207900047302246, 1.2211084365844727, 1.2020301818847656, 1.2186014652252197, 1.2207955121994019, 1.2405835390090942, 1.2642004489898682, 1.2817723751068115], 'val_auc': [0.8078962564468384, 0.844710648059845, 0.8566569089889526, 0.8644000887870789, 0.8724880218505859, 0.8791725039482117, 0.8821070790290833, 0.8849911093711853, 0.8860569000244141, 0.885850191116333, 0.8882560133934021, 0.8881818056106567, 0.8898059129714966, 0.8887312412261963, 0.8884578943252563, 0.8903713822364807, 0.8900118470191956, 0.8898984789848328, 0.8892038464546204, 0.8881615400314331, 0.8886134028434753], 'val_accuracy': [0.5535038113594055, 0.5807587504386902, 0.6057054996490479, 0.629823625087738, 0.6475793123245239, 0.6648911237716675, 0.6764323115348816, 0.6832682490348816, 0.6858428120613098, 0.6897490620613098, 0.6937144994735718, 0.6975615620613098, 0.6982717514038086, 0.6979462504386902, 0.6981238126754761, 0.6986268758773804, 0.7017341256141663, 0.6998698115348816, 0.6999881863594055, 0.7129793763160706, 0.7014086246490479]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 15:31:00 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 15:31:00 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 15:31:01 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-25 15:31:01 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 15:31:01 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.3899877071380615s\n",
      "04-25 15:31:01 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-25 15:31:02 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5271062850952148s\n",
      "04-25 15:31:02 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 15:31:02 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.2957158088684082s\n",
      "04-25 15:31:02 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.45017409324646s\n",
      "04-25 15:31:02 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 15:31:02 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13614225387573242s\n",
      "04-25 15:31:02 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 15:31:02 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009982585906982422s\n",
      "04-25 15:31:02 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 15:31:02 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 15:31:02 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 15:31:03 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 15:31:03 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 15:31:03 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 15:31:03 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9466 - auc: 0.7198 - accuracy: 0.4738 - val_loss: 1.0221 - val_auc: 0.8051 - val_accuracy: 0.5416\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.4405 - auc: 0.8550 - accuracy: 0.6061 - val_loss: 0.9566 - val_auc: 0.8329 - val_accuracy: 0.5665\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.1067 - auc: 0.9106 - accuracy: 0.7084 - val_loss: 0.9473 - val_auc: 0.8500 - val_accuracy: 0.5990\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.8459 - auc: 0.9438 - accuracy: 0.7726 - val_loss: 0.9807 - val_auc: 0.8574 - val_accuracy: 0.6152\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.6964 - auc: 0.9601 - accuracy: 0.8054 - val_loss: 1.0111 - val_auc: 0.8625 - val_accuracy: 0.6277\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.6009 - auc: 0.9694 - accuracy: 0.8290 - val_loss: 1.0297 - val_auc: 0.8681 - val_accuracy: 0.6431\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5320 - auc: 0.9755 - accuracy: 0.8476 - val_loss: 1.0374 - val_auc: 0.8731 - val_accuracy: 0.6541\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4770 - auc: 0.9800 - accuracy: 0.8637 - val_loss: 1.0743 - val_auc: 0.8741 - val_accuracy: 0.6634\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4327 - auc: 0.9833 - accuracy: 0.8759 - val_loss: 1.0880 - val_auc: 0.8766 - val_accuracy: 0.6714\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3979 - auc: 0.9858 - accuracy: 0.8859 - val_loss: 1.1115 - val_auc: 0.8778 - val_accuracy: 0.6752\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3685 - auc: 0.9879 - accuracy: 0.8941 - val_loss: 1.1355 - val_auc: 0.8787 - val_accuracy: 0.6816\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3448 - auc: 0.9893 - accuracy: 0.9012 - val_loss: 1.1525 - val_auc: 0.8789 - val_accuracy: 0.6823\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3255 - auc: 0.9905 - accuracy: 0.9059 - val_loss: 1.1629 - val_auc: 0.8796 - val_accuracy: 0.6869\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3091 - auc: 0.9915 - accuracy: 0.9105 - val_loss: 1.1887 - val_auc: 0.8803 - val_accuracy: 0.6900\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2931 - auc: 0.9923 - accuracy: 0.9141 - val_loss: 1.2008 - val_auc: 0.8810 - val_accuracy: 0.6902\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2827 - auc: 0.9929 - accuracy: 0.9166 - val_loss: 1.2215 - val_auc: 0.8802 - val_accuracy: 0.6925\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2705 - auc: 0.9935 - accuracy: 0.9196 - val_loss: 1.2261 - val_auc: 0.8816 - val_accuracy: 0.6926\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2591 - auc: 0.9940 - accuracy: 0.9226 - val_loss: 1.2446 - val_auc: 0.8812 - val_accuracy: 0.6896\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2518 - auc: 0.9944 - accuracy: 0.9239 - val_loss: 1.2725 - val_auc: 0.8807 - val_accuracy: 0.6931\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2430 - auc: 0.9947 - accuracy: 0.9266 - val_loss: 1.2816 - val_auc: 0.8813 - val_accuracy: 0.6955\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2375 - auc: 0.9950 - accuracy: 0.9276 - val_loss: 1.2925 - val_auc: 0.8806 - val_accuracy: 0.6959\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2310 - auc: 0.9952 - accuracy: 0.9290 - val_loss: 1.2993 - val_auc: 0.8817 - val_accuracy: 0.6976\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2246 - auc: 0.9955 - accuracy: 0.9302 - val_loss: 1.3195 - val_auc: 0.8807 - val_accuracy: 0.6917\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2192 - auc: 0.9957 - accuracy: 0.9316 - val_loss: 1.3097 - val_auc: 0.8819 - val_accuracy: 0.7003\n",
      "Epoch 25/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2147 - auc: 0.9959 - accuracy: 0.9323 - val_loss: 1.3296 - val_auc: 0.8821 - val_accuracy: 0.6934\n",
      "Epoch 26/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2115 - auc: 0.9959 - accuracy: 0.9331 - val_loss: 1.3455 - val_auc: 0.8819 - val_accuracy: 0.6991\n",
      "Epoch 27/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2068 - auc: 0.9962 - accuracy: 0.9345 - val_loss: 1.3510 - val_auc: 0.8813 - val_accuracy: 0.6983\n",
      "Epoch 28/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2040 - auc: 0.9963 - accuracy: 0.9352 - val_loss: 1.3708 - val_auc: 0.8809 - val_accuracy: 0.6938\n",
      "Epoch 29/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2018 - auc: 0.9964 - accuracy: 0.9353 - val_loss: 1.3764 - val_auc: 0.8809 - val_accuracy: 0.6956\n",
      "Epoch 30/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1987 - auc: 0.9965 - accuracy: 0.9360 - val_loss: 1.3805 - val_auc: 0.8811 - val_accuracy: 0.6929\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00030: early stopping\n",
      "04-25 15:37:00 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 15:37:00 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 15:37:01 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425153100_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9466270208358765, 1.4405103921890259, 1.1067310571670532, 0.8459432125091553, 0.6963671445846558, 0.6008856296539307, 0.5320255160331726, 0.4769922196865082, 0.4326689839363098, 0.3978880047798157, 0.3684779703617096, 0.34475091099739075, 0.3255484402179718, 0.3091493844985962, 0.2931089699268341, 0.2827417552471161, 0.27047115564346313, 0.25908350944519043, 0.25183507800102234, 0.24297726154327393, 0.23745611310005188, 0.2309940904378891, 0.22461780905723572, 0.2191821038722992, 0.2147475779056549, 0.21145839989185333, 0.20680765807628632, 0.2040460854768753, 0.20177443325519562, 0.1986783742904663], 'auc': [0.7198449969291687, 0.8549636006355286, 0.9105857014656067, 0.9437754154205322, 0.9601426720619202, 0.9694118499755859, 0.9755131006240845, 0.9800264835357666, 0.9832627773284912, 0.9858226180076599, 0.9878507852554321, 0.9892613291740417, 0.9905135035514832, 0.9914864301681519, 0.9923065304756165, 0.9929010272026062, 0.9934977293014526, 0.9940034747123718, 0.9943602085113525, 0.9947395920753479, 0.9950049519538879, 0.9952436089515686, 0.9954931139945984, 0.9956830739974976, 0.9958657026290894, 0.9959453344345093, 0.9961925148963928, 0.9962831139564514, 0.9963640570640564, 0.9964545965194702], 'accuracy': [0.47383376955986023, 0.6060822010040283, 0.7083780169487, 0.7725808024406433, 0.8053890466690063, 0.8289675712585449, 0.8476386070251465, 0.8636937141418457, 0.8758848905563354, 0.8858796954154968, 0.8941293358802795, 0.9012120366096497, 0.9059292078018188, 0.9105158448219299, 0.9140730500221252, 0.9165797233581543, 0.9196468591690063, 0.9226118326187134, 0.9239162802696228, 0.9266203045845032, 0.9276215434074402, 0.9289823770523071, 0.930170476436615, 0.9316053986549377, 0.9322752356529236, 0.9330719709396362, 0.934510350227356, 0.9351943135261536, 0.9353283047676086, 0.9360086917877197], 'val_loss': [1.0221365690231323, 0.9565970301628113, 0.9473036527633667, 0.9807249307632446, 1.0111298561096191, 1.0296578407287598, 1.0374059677124023, 1.074288249015808, 1.0880157947540283, 1.1114784479141235, 1.1354990005493164, 1.1524696350097656, 1.162916660308838, 1.1886601448059082, 1.2008103132247925, 1.2215017080307007, 1.2261136770248413, 1.2446308135986328, 1.2725098133087158, 1.2815643548965454, 1.2925341129302979, 1.2992732524871826, 1.3195230960845947, 1.3096647262573242, 1.329628348350525, 1.345513105392456, 1.351049542427063, 1.3708229064941406, 1.3763887882232666, 1.380484700202942], 'val_auc': [0.8051150441169739, 0.8328660130500793, 0.8499575853347778, 0.8573862910270691, 0.8624550700187683, 0.8681102991104126, 0.8730688691139221, 0.8741018176078796, 0.8765836954116821, 0.8778449892997742, 0.87872713804245, 0.8789123296737671, 0.8796155452728271, 0.8802957534790039, 0.8810466527938843, 0.880172610282898, 0.8815529346466064, 0.8811547756195068, 0.8807096481323242, 0.8813239932060242, 0.8806476593017578, 0.881657063961029, 0.8807054758071899, 0.8819278478622437, 0.8820878267288208, 0.8818541765213013, 0.8813286423683167, 0.8808736801147461, 0.8809274435043335, 0.8811155557632446], 'val_accuracy': [0.5416074991226196, 0.566465437412262, 0.5989879369735718, 0.615234375, 0.6277225613594055, 0.6431403756141663, 0.6540601253509521, 0.6633522510528564, 0.6714311242103577, 0.6751893758773804, 0.681640625, 0.6822916865348816, 0.6869081258773804, 0.6900449991226196, 0.6901633739471436, 0.6924715638160706, 0.6926195621490479, 0.6896010637283325, 0.6930634379386902, 0.6955196261405945, 0.6959339380264282, 0.6976207494735718, 0.6916725635528564, 0.7003136873245239, 0.6934481263160706, 0.6991003751754761, 0.6983309388160706, 0.6938032507896423, 0.6956380009651184, 0.6929450631141663]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 2, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 15:37:01 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 2, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 15:37:01 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 15:37:02 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-25 15:37:02 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 15:37:02 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.3859896659851074s\n",
      "04-25 15:37:02 I deeptables.m.preprocessor.py 341 - Data imputation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 15:37:03 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5091555118560791s\n",
      "04-25 15:37:03 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 15:37:03 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.29025888442993164s\n",
      "04-25 15:37:03 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.4127957820892334s\n",
      "04-25 15:37:03 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 15:37:03 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13614320755004883s\n",
      "04-25 15:37:03 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 15:37:03 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009970664978027344s\n",
      "04-25 15:37:03 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 15:37:03 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 15:37:03 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 15:37:03 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 15:37:03 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 15:37:04 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 15:37:04 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9481 - auc: 0.7166 - accuracy: 0.4683 - val_loss: 1.0347 - val_auc: 0.7976 - val_accuracy: 0.5199\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.4291 - auc: 0.8573 - accuracy: 0.6096 - val_loss: 0.9411 - val_auc: 0.8392 - val_accuracy: 0.5738\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.0730 - auc: 0.9166 - accuracy: 0.7207 - val_loss: 0.9407 - val_auc: 0.8534 - val_accuracy: 0.5985\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.8216 - auc: 0.9467 - accuracy: 0.7768 - val_loss: 0.9789 - val_auc: 0.8578 - val_accuracy: 0.6168\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.6815 - auc: 0.9612 - accuracy: 0.8082 - val_loss: 1.0086 - val_auc: 0.8631 - val_accuracy: 0.6286\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5858 - auc: 0.9704 - accuracy: 0.8333 - val_loss: 1.0393 - val_auc: 0.8667 - val_accuracy: 0.6387\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5172 - auc: 0.9764 - accuracy: 0.8517 - val_loss: 1.0486 - val_auc: 0.8716 - val_accuracy: 0.6492\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4675 - auc: 0.9803 - accuracy: 0.8658 - val_loss: 1.0650 - val_auc: 0.8746 - val_accuracy: 0.6573\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4286 - auc: 0.9834 - accuracy: 0.8764 - val_loss: 1.0849 - val_auc: 0.8761 - val_accuracy: 0.6631\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3986 - auc: 0.9856 - accuracy: 0.8849 - val_loss: 1.0935 - val_auc: 0.8781 - val_accuracy: 0.6684\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3696 - auc: 0.9876 - accuracy: 0.8934 - val_loss: 1.1197 - val_auc: 0.8788 - val_accuracy: 0.6716\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3465 - auc: 0.9892 - accuracy: 0.8994 - val_loss: 1.1397 - val_auc: 0.8795 - val_accuracy: 0.6760\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3289 - auc: 0.9903 - accuracy: 0.9045 - val_loss: 1.1555 - val_auc: 0.8805 - val_accuracy: 0.6795\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3101 - auc: 0.9913 - accuracy: 0.9092 - val_loss: 1.1832 - val_auc: 0.8797 - val_accuracy: 0.6801\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2937 - auc: 0.9922 - accuracy: 0.9134 - val_loss: 1.1957 - val_auc: 0.8801 - val_accuracy: 0.6816\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2803 - auc: 0.9929 - accuracy: 0.9160 - val_loss: 1.2057 - val_auc: 0.8815 - val_accuracy: 0.6834\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2700 - auc: 0.9935 - accuracy: 0.9189 - val_loss: 1.2131 - val_auc: 0.8821 - val_accuracy: 0.6856\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2586 - auc: 0.9940 - accuracy: 0.9219 - val_loss: 1.2369 - val_auc: 0.8824 - val_accuracy: 0.6857\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2485 - auc: 0.9945 - accuracy: 0.9239 - val_loss: 1.2518 - val_auc: 0.8829 - val_accuracy: 0.6881\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2398 - auc: 0.9949 - accuracy: 0.9262 - val_loss: 1.2707 - val_auc: 0.8826 - val_accuracy: 0.6888\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2335 - auc: 0.9951 - accuracy: 0.9286 - val_loss: 1.2942 - val_auc: 0.8818 - val_accuracy: 0.6964\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2285 - auc: 0.9953 - accuracy: 0.9292 - val_loss: 1.2966 - val_auc: 0.8829 - val_accuracy: 0.6907\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2223 - auc: 0.9956 - accuracy: 0.9306 - val_loss: 1.3180 - val_auc: 0.8816 - val_accuracy: 0.6903\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2166 - auc: 0.9958 - accuracy: 0.9313 - val_loss: 1.3190 - val_auc: 0.8830 - val_accuracy: 0.6914\n",
      "Epoch 25/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2130 - auc: 0.9959 - accuracy: 0.9327 - val_loss: 1.3235 - val_auc: 0.8831 - val_accuracy: 0.6923\n",
      "Epoch 26/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2104 - auc: 0.9960 - accuracy: 0.9332 - val_loss: 1.3470 - val_auc: 0.8824 - val_accuracy: 0.6928\n",
      "Epoch 27/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2055 - auc: 0.9962 - accuracy: 0.9343 - val_loss: 1.3597 - val_auc: 0.8822 - val_accuracy: 0.6925\n",
      "Epoch 28/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2011 - auc: 0.9963 - accuracy: 0.9355 - val_loss: 1.3580 - val_auc: 0.8833 - val_accuracy: 0.6950\n",
      "Epoch 29/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1983 - auc: 0.9965 - accuracy: 0.9363 - val_loss: 1.3498 - val_auc: 0.8843 - val_accuracy: 0.6956\n",
      "Epoch 30/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1970 - auc: 0.9965 - accuracy: 0.9356 - val_loss: 1.4063 - val_auc: 0.8822 - val_accuracy: 0.7007\n",
      "Epoch 31/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1937 - auc: 0.9966 - accuracy: 0.9371 - val_loss: 1.4076 - val_auc: 0.8820 - val_accuracy: 0.6936\n",
      "Epoch 32/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1914 - auc: 0.9967 - accuracy: 0.9371 - val_loss: 1.3942 - val_auc: 0.8831 - val_accuracy: 0.6954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1900 - auc: 0.9967 - accuracy: 0.9376 - val_loss: 1.4085 - val_auc: 0.8825 - val_accuracy: 0.6941\n",
      "Epoch 34/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1881 - auc: 0.9967 - accuracy: 0.9380 - val_loss: 1.4197 - val_auc: 0.8823 - val_accuracy: 0.6987\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00034: early stopping\n",
      "04-25 15:43:48 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 15:43:48 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 15:43:48 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425153701_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.948061227798462, 1.4290564060211182, 1.0730420351028442, 0.8216177225112915, 0.6815186738967896, 0.5857614874839783, 0.517196536064148, 0.46751827001571655, 0.42861995100975037, 0.3986481726169586, 0.3695872128009796, 0.3465123176574707, 0.3288988173007965, 0.31007060408592224, 0.29368236660957336, 0.2802952527999878, 0.2699753940105438, 0.2585974633693695, 0.2485022395849228, 0.23981624841690063, 0.2334570735692978, 0.2284831702709198, 0.22227053344249725, 0.216557577252388, 0.21298958361148834, 0.21040204167366028, 0.2054629772901535, 0.201094388961792, 0.1983031928539276, 0.19699326157569885, 0.19373300671577454, 0.19139307737350464, 0.19001327455043793, 0.1881479173898697], 'auc': [0.7165911793708801, 0.8572641611099243, 0.9165531396865845, 0.9467105269432068, 0.9612338542938232, 0.9703885316848755, 0.9764016270637512, 0.980346143245697, 0.9834228157997131, 0.9856023788452148, 0.9876261949539185, 0.9891807436943054, 0.9902710914611816, 0.9913151264190674, 0.9921669960021973, 0.9929273128509521, 0.9934507012367249, 0.993952214717865, 0.9944745898246765, 0.9948656558990479, 0.995116651058197, 0.9952936172485352, 0.995639443397522, 0.9958028793334961, 0.9959118962287903, 0.996024489402771, 0.9961940050125122, 0.9963399767875671, 0.9964699149131775, 0.996512234210968, 0.9966022372245789, 0.996680736541748, 0.9967324733734131, 0.9967489838600159], 'accuracy': [0.4683269262313843, 0.6096464395523071, 0.7207242846488953, 0.7767584919929504, 0.8082341551780701, 0.8332968950271606, 0.8517035245895386, 0.865766704082489, 0.8764313459396362, 0.8849454522132874, 0.8933960199356079, 0.8994457721710205, 0.9045436382293701, 0.9091796875, 0.9133503437042236, 0.9160085916519165, 0.9189100861549377, 0.921934962272644, 0.9239444732666016, 0.926225483417511, 0.9285557866096497, 0.9291763305664062, 0.9305723905563354, 0.9313198328018188, 0.932701826095581, 0.9332094788551331, 0.9342671036720276, 0.9355363249778748, 0.9363225102424622, 0.9356490969657898, 0.9371157288551331, 0.9371016025543213, 0.9375881552696228, 0.937958300113678], 'val_loss': [1.0346826314926147, 0.9411287307739258, 0.9407042264938354, 0.9788727164268494, 1.0086199045181274, 1.039327621459961, 1.0486398935317993, 1.0649970769882202, 1.0849071741104126, 1.0935420989990234, 1.1197210550308228, 1.1397274732589722, 1.1554763317108154, 1.183194875717163, 1.1957447528839111, 1.2056787014007568, 1.2131386995315552, 1.2368779182434082, 1.2518489360809326, 1.2707066535949707, 1.2941958904266357, 1.2966216802597046, 1.3180453777313232, 1.319008231163025, 1.3234500885009766, 1.3469774723052979, 1.359737515449524, 1.357987403869629, 1.3497976064682007, 1.4062941074371338, 1.4076383113861084, 1.3941570520401, 1.4085140228271484, 1.419679880142212], 'val_auc': [0.7976210713386536, 0.8391565084457397, 0.8533905744552612, 0.8578346967697144, 0.8630810379981995, 0.866676390171051, 0.8715521097183228, 0.8745823502540588, 0.8761138319969177, 0.8780686855316162, 0.8788473010063171, 0.8795164823532104, 0.8805403113365173, 0.8797459006309509, 0.880135715007782, 0.8814948797225952, 0.8821256756782532, 0.8823980689048767, 0.8828667998313904, 0.8825674653053284, 0.8818128108978271, 0.8829362988471985, 0.8815964460372925, 0.883025050163269, 0.8830992579460144, 0.8823580145835876, 0.8821547627449036, 0.8832810521125793, 0.8842974305152893, 0.8822176456451416, 0.8819942474365234, 0.8830528259277344, 0.8824779391288757, 0.8823426961898804], 'val_accuracy': [0.5198567509651184, 0.5738044381141663, 0.5984552502632141, 0.6168323755264282, 0.6286103129386902, 0.6387014389038086, 0.6492069363594055, 0.657285749912262, 0.663145124912262, 0.6684422492980957, 0.6716086864471436, 0.6760475635528564, 0.6794507503509521, 0.6800721883773804, 0.681581437587738, 0.6833866238594055, 0.685606062412262, 0.6856652498245239, 0.6880622506141663, 0.6888316869735718, 0.6964370012283325, 0.6907256245613098, 0.6903113126754761, 0.6914358139038086, 0.6922940611839294, 0.6928266882896423, 0.6925307512283325, 0.6950165629386902, 0.6956380009651184, 0.7007279992103577, 0.6936256885528564, 0.6954308748245239, 0.6940696239471436, 0.6986860632896423]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 2, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 15:43:48 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 2, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 15:43:48 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 15:43:49 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 15:43:50 I deeptables.m.preprocessor.py 249 - transform_X taken 0.9504809379577637s\n",
      "04-25 15:43:50 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 15:43:50 I deeptables.m.preprocessor.py 236 - transform_y taken 0.006981372833251953s\n",
      "04-25 15:43:50 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 15:43:50 I deeptables.m.preprocessor.py 249 - transform_X taken 0.1411287784576416s\n",
      "04-25 15:43:50 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 15:43:50 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009975433349609375s\n",
      "04-25 15:43:50 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 15:43:50 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 15:43:50 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 15:43:50 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 15:43:50 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 15:43:51 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 15:43:51 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9432 - auc: 0.7192 - accuracy: 0.4659 - val_loss: 1.0291 - val_auc: 0.7998 - val_accuracy: 0.5158\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.4034 - auc: 0.8627 - accuracy: 0.6177 - val_loss: 0.9270 - val_auc: 0.8441 - val_accuracy: 0.5811\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.0388 - auc: 0.9218 - accuracy: 0.7294 - val_loss: 0.9322 - val_auc: 0.8574 - val_accuracy: 0.6055\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.7861 - auc: 0.9510 - accuracy: 0.7867 - val_loss: 0.9741 - val_auc: 0.8625 - val_accuracy: 0.6198\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.6290 - auc: 0.9668 - accuracy: 0.8226 - val_loss: 0.9931 - val_auc: 0.8716 - val_accuracy: 0.6412\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5204 - auc: 0.9766 - accuracy: 0.8517 - val_loss: 1.0164 - val_auc: 0.8789 - val_accuracy: 0.6573\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4434 - auc: 0.9827 - accuracy: 0.8740 - val_loss: 1.0487 - val_auc: 0.8824 - val_accuracy: 0.6665\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3875 - auc: 0.9866 - accuracy: 0.8895 - val_loss: 1.0695 - val_auc: 0.8861 - val_accuracy: 0.6749\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3455 - auc: 0.9893 - accuracy: 0.9010 - val_loss: 1.0959 - val_auc: 0.8876 - val_accuracy: 0.6797\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3158 - auc: 0.9910 - accuracy: 0.9083 - val_loss: 1.1346 - val_auc: 0.8882 - val_accuracy: 0.6840\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2910 - auc: 0.9924 - accuracy: 0.9151 - val_loss: 1.1566 - val_auc: 0.8889 - val_accuracy: 0.6869\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2740 - auc: 0.9933 - accuracy: 0.9189 - val_loss: 1.1789 - val_auc: 0.8889 - val_accuracy: 0.6896\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2561 - auc: 0.9940 - accuracy: 0.9233 - val_loss: 1.2005 - val_auc: 0.8892 - val_accuracy: 0.7033\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2463 - auc: 0.9946 - accuracy: 0.9262 - val_loss: 1.1998 - val_auc: 0.8902 - val_accuracy: 0.6915\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2363 - auc: 0.9950 - accuracy: 0.9281 - val_loss: 1.2298 - val_auc: 0.8884 - val_accuracy: 0.7035\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2288 - auc: 0.9953 - accuracy: 0.9300 - val_loss: 1.2342 - val_auc: 0.8891 - val_accuracy: 0.6914\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2210 - auc: 0.9956 - accuracy: 0.9319 - val_loss: 1.2613 - val_auc: 0.8880 - val_accuracy: 0.7052\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2164 - auc: 0.9958 - accuracy: 0.9328 - val_loss: 1.2676 - val_auc: 0.8884 - val_accuracy: 0.7064\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2121 - auc: 0.9959 - accuracy: 0.9337 - val_loss: 1.2805 - val_auc: 0.8870 - val_accuracy: 0.6936\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "04-25 15:47:36 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 15:47:36 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 15:47:37 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425154348_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.943166971206665, 1.403398036956787, 1.0388027429580688, 0.7860847115516663, 0.6290461421012878, 0.5203945636749268, 0.4434020221233368, 0.3875214159488678, 0.34553825855255127, 0.3157682716846466, 0.2910063564777374, 0.273956835269928, 0.2561328113079071, 0.2463482767343521, 0.23631426692008972, 0.2287941724061966, 0.22101029753684998, 0.21638163924217224, 0.21207036077976227], 'auc': [0.719232976436615, 0.8626793026924133, 0.9217808842658997, 0.9509735703468323, 0.9668450355529785, 0.9766191840171814, 0.9827321171760559, 0.9865970015525818, 0.9892750382423401, 0.9910173416137695, 0.9924293160438538, 0.9932621717453003, 0.9940493106842041, 0.9945573806762695, 0.9949594736099243, 0.9952564835548401, 0.9955965876579285, 0.9958006739616394, 0.9959437251091003], 'accuracy': [0.4659225642681122, 0.6177339553833008, 0.7294393181800842, 0.7867497801780701, 0.8225793838500977, 0.8517140746116638, 0.8739776015281677, 0.8895180225372314, 0.9010146260261536, 0.9083053469657898, 0.9150707721710205, 0.9188677668571472, 0.9232781529426575, 0.9262430667877197, 0.9280869364738464, 0.9299730658531189, 0.9318909645080566, 0.9328145980834961, 0.9337489008903503], 'val_loss': [1.0291342735290527, 0.9269517660140991, 0.9321776628494263, 0.9741390347480774, 0.9931171536445618, 1.0163570642471313, 1.0486836433410645, 1.0695288181304932, 1.0959246158599854, 1.1345564126968384, 1.1566393375396729, 1.178884744644165, 1.2005454301834106, 1.1998287439346313, 1.229750633239746, 1.2342206239700317, 1.2613402605056763, 1.2676281929016113, 1.2805122137069702], 'val_auc': [0.7997782230377197, 0.8440665006637573, 0.857375979423523, 0.8625245690345764, 0.8715782165527344, 0.8789060115814209, 0.8823750019073486, 0.8861367106437683, 0.8875977396965027, 0.888213574886322, 0.888931930065155, 0.8888835310935974, 0.8892248868942261, 0.8902151584625244, 0.8883900046348572, 0.8890829086303711, 0.888041615486145, 0.8884076476097107, 0.8869901299476624], 'val_accuracy': [0.5158321261405945, 0.5810546875, 0.6054983139038086, 0.6197916865348816, 0.6411576867103577, 0.6573153138160706, 0.6665186882019043, 0.6749230623245239, 0.6796875, 0.6840376257896423, 0.6869081258773804, 0.6896306872367859, 0.7033321261405945, 0.6915246248245239, 0.7034505009651184, 0.69140625, 0.7051964998245239, 0.7064393758773804, 0.6936256885528564]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 4, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 15:47:37 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 4, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 15:47:37 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 15:47:37 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 15:47:38 I deeptables.m.preprocessor.py 249 - transform_X taken 0.951491117477417s\n",
      "04-25 15:47:38 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 15:47:38 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0069811344146728516s\n",
      "04-25 15:47:38 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 15:47:38 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13763213157653809s\n",
      "04-25 15:47:38 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 15:47:38 I deeptables.m.preprocessor.py 236 - transform_y taken 0.001995563507080078s\n",
      "04-25 15:47:38 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 15:47:38 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 15:47:38 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 15:47:38 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 15:47:38 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 15:47:39 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 15:47:39 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9437 - auc: 0.7198 - accuracy: 0.4853 - val_loss: 1.0099 - val_auc: 0.8116 - val_accuracy: 0.5522\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.4240 - auc: 0.8586 - accuracy: 0.6137 - val_loss: 0.9318 - val_auc: 0.8421 - val_accuracy: 0.5877\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.0753 - auc: 0.9159 - accuracy: 0.7196 - val_loss: 0.9323 - val_auc: 0.8551 - val_accuracy: 0.6140\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.8207 - auc: 0.9470 - accuracy: 0.7791 - val_loss: 0.9638 - val_auc: 0.8620 - val_accuracy: 0.6272\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.6752 - auc: 0.9620 - accuracy: 0.8100 - val_loss: 0.9935 - val_auc: 0.8667 - val_accuracy: 0.6290\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5813 - auc: 0.9708 - accuracy: 0.8334 - val_loss: 1.0158 - val_auc: 0.8710 - val_accuracy: 0.6432\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5203 - auc: 0.9761 - accuracy: 0.8504 - val_loss: 1.0519 - val_auc: 0.8715 - val_accuracy: 0.6605\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4738 - auc: 0.9799 - accuracy: 0.8637 - val_loss: 1.0735 - val_auc: 0.8738 - val_accuracy: 0.6669\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4349 - auc: 0.9828 - accuracy: 0.8748 - val_loss: 1.0792 - val_auc: 0.8764 - val_accuracy: 0.6732\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4021 - auc: 0.9854 - accuracy: 0.8847 - val_loss: 1.1063 - val_auc: 0.8764 - val_accuracy: 0.6795\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3751 - auc: 0.9873 - accuracy: 0.8919 - val_loss: 1.1175 - val_auc: 0.8784 - val_accuracy: 0.6863\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3498 - auc: 0.9889 - accuracy: 0.8992 - val_loss: 1.1544 - val_auc: 0.8774 - val_accuracy: 0.6869\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3279 - auc: 0.9902 - accuracy: 0.9043 - val_loss: 1.1639 - val_auc: 0.8795 - val_accuracy: 0.6920\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3128 - auc: 0.9912 - accuracy: 0.9092 - val_loss: 1.1883 - val_auc: 0.8782 - val_accuracy: 0.6923\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2967 - auc: 0.9920 - accuracy: 0.9130 - val_loss: 1.1996 - val_auc: 0.8805 - val_accuracy: 0.6961\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2832 - auc: 0.9928 - accuracy: 0.9166 - val_loss: 1.2209 - val_auc: 0.8791 - val_accuracy: 0.6952\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2696 - auc: 0.9934 - accuracy: 0.9198 - val_loss: 1.2360 - val_auc: 0.8800 - val_accuracy: 0.6984\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2599 - auc: 0.9939 - accuracy: 0.9222 - val_loss: 1.2473 - val_auc: 0.8811 - val_accuracy: 0.6994\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2527 - auc: 0.9943 - accuracy: 0.9238 - val_loss: 1.2598 - val_auc: 0.8806 - val_accuracy: 0.7006\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2417 - auc: 0.9948 - accuracy: 0.9266 - val_loss: 1.2653 - val_auc: 0.8817 - val_accuracy: 0.7023\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2347 - auc: 0.9951 - accuracy: 0.9282 - val_loss: 1.2892 - val_auc: 0.8814 - val_accuracy: 0.7034\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2323 - auc: 0.9952 - accuracy: 0.9287 - val_loss: 1.2780 - val_auc: 0.8829 - val_accuracy: 0.7057\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2248 - auc: 0.9955 - accuracy: 0.9307 - val_loss: 1.3088 - val_auc: 0.8817 - val_accuracy: 0.7027\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2197 - auc: 0.9957 - accuracy: 0.9311 - val_loss: 1.3038 - val_auc: 0.8830 - val_accuracy: 0.7062\n",
      "Epoch 25/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2153 - auc: 0.9959 - accuracy: 0.9327 - val_loss: 1.3166 - val_auc: 0.8824 - val_accuracy: 0.7127\n",
      "Epoch 26/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2118 - auc: 0.9960 - accuracy: 0.9334 - val_loss: 1.3355 - val_auc: 0.8816 - val_accuracy: 0.7059\n",
      "Epoch 27/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2065 - auc: 0.9962 - accuracy: 0.9343 - val_loss: 1.3442 - val_auc: 0.8821 - val_accuracy: 0.7067\n",
      "Epoch 28/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2035 - auc: 0.9963 - accuracy: 0.9353 - val_loss: 1.3643 - val_auc: 0.8813 - val_accuracy: 0.7058\n",
      "Epoch 29/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1994 - auc: 0.9964 - accuracy: 0.9358 - val_loss: 1.3800 - val_auc: 0.8819 - val_accuracy: 0.7080\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00029: early stopping\n",
      "04-25 15:53:24 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 15:53:24 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 15:53:24 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425154737_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.943673849105835, 1.4239572286605835, 1.0753206014633179, 0.8207409381866455, 0.6751621961593628, 0.5813091397285461, 0.5202823281288147, 0.47382527589797974, 0.4349384009838104, 0.4020829498767853, 0.375078946352005, 0.3497507870197296, 0.32788580656051636, 0.3127809464931488, 0.29665541648864746, 0.28322461247444153, 0.2696164548397064, 0.2599073052406311, 0.2526707053184509, 0.24171726405620575, 0.23472829163074493, 0.23226535320281982, 0.22475047409534454, 0.21968500316143036, 0.21528834104537964, 0.21175023913383484, 0.20652511715888977, 0.2034943699836731, 0.19940653443336487], 'auc': [0.7197927236557007, 0.8586001992225647, 0.9159106016159058, 0.9470226168632507, 0.9620479941368103, 0.9708371758460999, 0.9761093854904175, 0.9798818826675415, 0.9828406572341919, 0.9853528141975403, 0.9872558116912842, 0.9889049530029297, 0.9902085065841675, 0.991183340549469, 0.9920350909233093, 0.9927696585655212, 0.9934057593345642, 0.9939125776290894, 0.9943008422851562, 0.9947567582130432, 0.9950588345527649, 0.9951993227005005, 0.9955196380615234, 0.9956737160682678, 0.9958738684654236, 0.9960029125213623, 0.9961845278739929, 0.9962749481201172, 0.9964223504066467], 'accuracy': [0.485326886177063, 0.6136655211448669, 0.7195890545845032, 0.7791135311126709, 0.8099510669708252, 0.833427369594574, 0.8503920435905457, 0.8636796474456787, 0.8747884631156921, 0.884666919708252, 0.8918800950050354, 0.8991954922676086, 0.904296875, 0.9091585278511047, 0.9129695892333984, 0.9166466593742371, 0.9198196530342102, 0.9221817255020142, 0.923778772354126, 0.9265885949134827, 0.9282314777374268, 0.9286897778511047, 0.9306640625, 0.9311047792434692, 0.9327300190925598, 0.9334104061126709, 0.9342777132987976, 0.9352859854698181, 0.9357513785362244], 'val_loss': [1.0099029541015625, 0.9318220019340515, 0.9323049187660217, 0.9637829661369324, 0.9935113191604614, 1.0157718658447266, 1.0519044399261475, 1.0734984874725342, 1.0791573524475098, 1.1062580347061157, 1.1174758672714233, 1.1543772220611572, 1.1639069318771362, 1.1883277893066406, 1.1995549201965332, 1.220862627029419, 1.2359853982925415, 1.2472624778747559, 1.2598313093185425, 1.2653224468231201, 1.2891919612884521, 1.278000831604004, 1.3087579011917114, 1.303775429725647, 1.3165696859359741, 1.335545539855957, 1.3442049026489258, 1.3643440008163452, 1.3800042867660522], 'val_auc': [0.8116092681884766, 0.842078447341919, 0.8550945520401001, 0.8620102405548096, 0.8666940927505493, 0.8709821701049805, 0.8715335726737976, 0.8737644553184509, 0.8764111995697021, 0.8764329552650452, 0.878409206867218, 0.8773727416992188, 0.879459798336029, 0.8781518936157227, 0.880486786365509, 0.8790738582611084, 0.8799929618835449, 0.8810603618621826, 0.8805766701698303, 0.8817484974861145, 0.8814079165458679, 0.8828656673431396, 0.8817456364631653, 0.8829554915428162, 0.8823925256729126, 0.8816070556640625, 0.8821402192115784, 0.8812809586524963, 0.8819392919540405], 'val_accuracy': [0.5521721243858337, 0.5876834988594055, 0.6139914989471436, 0.6272490620613098, 0.6290246248245239, 0.6432291865348816, 0.6605113744735718, 0.666933000087738, 0.6731770634651184, 0.6795099377632141, 0.6862571239471436, 0.6868785619735718, 0.6920276880264282, 0.6922644376754761, 0.6961115002632141, 0.6951941251754761, 0.6984197497367859, 0.6994258761405945, 0.7005504369735718, 0.7022963762283325, 0.7034209370613098, 0.7056699991226196, 0.7026811242103577, 0.7062322497367859, 0.7127130627632141, 0.7059363126754761, 0.7067353129386902, 0.7058475613594055, 0.7080373764038086]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 4, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 15:53:24 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 4, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 15:53:24 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 15:53:25 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-25 15:53:25 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 15:53:25 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.3889596462249756s\n",
      "04-25 15:53:25 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-25 15:53:25 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5106558799743652s\n",
      "04-25 15:53:25 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 15:53:26 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.28523755073547363s\n",
      "04-25 15:53:26 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.4097566604614258s\n",
      "04-25 15:53:26 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 15:53:26 I deeptables.m.preprocessor.py 249 - transform_X taken 0.1356372833251953s\n",
      "04-25 15:53:26 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 15:53:26 I deeptables.m.preprocessor.py 236 - transform_y taken 0.000997781753540039s\n",
      "04-25 15:53:26 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 15:53:26 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 15:53:26 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 15:53:26 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 15:53:26 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 15:53:27 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 15:53:27 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9443 - auc: 0.7201 - accuracy: 0.4697 - val_loss: 1.0389 - val_auc: 0.7960 - val_accuracy: 0.5241\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.4325 - auc: 0.8562 - accuracy: 0.6069 - val_loss: 0.9570 - val_auc: 0.8329 - val_accuracy: 0.5637\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.0992 - auc: 0.9120 - accuracy: 0.7093 - val_loss: 0.9551 - val_auc: 0.8487 - val_accuracy: 0.5938\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.8417 - auc: 0.9444 - accuracy: 0.7725 - val_loss: 0.9893 - val_auc: 0.8559 - val_accuracy: 0.6119\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.7001 - auc: 0.9595 - accuracy: 0.8034 - val_loss: 1.0069 - val_auc: 0.8637 - val_accuracy: 0.6287\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.6057 - auc: 0.9690 - accuracy: 0.8285 - val_loss: 1.0492 - val_auc: 0.8650 - val_accuracy: 0.6376\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5311 - auc: 0.9756 - accuracy: 0.8489 - val_loss: 1.0663 - val_auc: 0.8693 - val_accuracy: 0.6494\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4760 - auc: 0.9801 - accuracy: 0.8645 - val_loss: 1.0905 - val_auc: 0.8717 - val_accuracy: 0.6555\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4319 - auc: 0.9835 - accuracy: 0.8765 - val_loss: 1.0980 - val_auc: 0.8757 - val_accuracy: 0.6638\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3956 - auc: 0.9860 - accuracy: 0.8871 - val_loss: 1.1314 - val_auc: 0.8757 - val_accuracy: 0.6700\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3684 - auc: 0.9879 - accuracy: 0.8943 - val_loss: 1.1513 - val_auc: 0.8764 - val_accuracy: 0.6735\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3441 - auc: 0.9895 - accuracy: 0.9004 - val_loss: 1.1686 - val_auc: 0.8784 - val_accuracy: 0.6780\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3220 - auc: 0.9907 - accuracy: 0.9062 - val_loss: 1.1976 - val_auc: 0.8775 - val_accuracy: 0.6798\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3059 - auc: 0.9917 - accuracy: 0.9108 - val_loss: 1.2046 - val_auc: 0.8792 - val_accuracy: 0.6817\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2908 - auc: 0.9924 - accuracy: 0.9144 - val_loss: 1.2160 - val_auc: 0.8804 - val_accuracy: 0.6842\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2790 - auc: 0.9930 - accuracy: 0.9174 - val_loss: 1.2367 - val_auc: 0.8800 - val_accuracy: 0.6838\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2670 - auc: 0.9936 - accuracy: 0.9206 - val_loss: 1.2592 - val_auc: 0.8793 - val_accuracy: 0.6865\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2573 - auc: 0.9940 - accuracy: 0.9227 - val_loss: 1.2697 - val_auc: 0.8800 - val_accuracy: 0.6881\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2502 - auc: 0.9944 - accuracy: 0.9243 - val_loss: 1.2809 - val_auc: 0.8796 - val_accuracy: 0.6872\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2405 - auc: 0.9948 - accuracy: 0.9266 - val_loss: 1.2924 - val_auc: 0.8803 - val_accuracy: 0.6903\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "04-25 15:57:26 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 15:57:26 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 15:57:27 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425155324_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.944301962852478, 1.432498574256897, 1.0992375612258911, 0.8416720032691956, 0.7001461386680603, 0.6056876182556152, 0.531078577041626, 0.4759629964828491, 0.4318973124027252, 0.39558646082878113, 0.3683848977088928, 0.3440552353858948, 0.32201752066612244, 0.3059089481830597, 0.29082539677619934, 0.2790438234806061, 0.2669824957847595, 0.25731900334358215, 0.2501603364944458, 0.2405322790145874], 'auc': [0.7200670838356018, 0.8562064170837402, 0.9119901061058044, 0.9443556666374207, 0.9595385193824768, 0.9689710736274719, 0.9756100177764893, 0.9801092147827148, 0.9834755659103394, 0.9860302805900574, 0.9878603219985962, 0.9894501566886902, 0.9906785488128662, 0.9916562438011169, 0.9924035668373108, 0.9930320978164673, 0.993595540523529, 0.994034469127655, 0.9943988919258118, 0.9948492050170898], 'accuracy': [0.4696631133556366, 0.6069318056106567, 0.7092593908309937, 0.7724503874778748, 0.8033901453018188, 0.8284916281700134, 0.8488972187042236, 0.8644834160804749, 0.8764983415603638, 0.8870642781257629, 0.8942880034446716, 0.900390625, 0.906190037727356, 0.9108296036720276, 0.9144362211227417, 0.9173588156700134, 0.9206058382987976, 0.9226858615875244, 0.9242899417877197, 0.9266167879104614], 'val_loss': [1.0388847589492798, 0.9569510817527771, 0.9551310539245605, 0.9892758727073669, 1.0068532228469849, 1.0492427349090576, 1.0662834644317627, 1.0904861688613892, 1.0980125665664673, 1.1314337253570557, 1.151272177696228, 1.168639898300171, 1.197648525238037, 1.2045996189117432, 1.2159676551818848, 1.2366554737091064, 1.2592111825942993, 1.2697161436080933, 1.2808613777160645, 1.2924137115478516], 'val_auc': [0.7959977388381958, 0.832944393157959, 0.8487431406974792, 0.8559488654136658, 0.8636957406997681, 0.8650245070457458, 0.8693273663520813, 0.8716866374015808, 0.8756511807441711, 0.8757082223892212, 0.8764493465423584, 0.8784254789352417, 0.8775478601455688, 0.8791772127151489, 0.8803566694259644, 0.8799654841423035, 0.8793206214904785, 0.8800079226493835, 0.879633367061615, 0.8802674412727356], 'val_accuracy': [0.5241477489471436, 0.5637133121490479, 0.593809187412262, 0.6118608117103577, 0.6286991238594055, 0.637636125087738, 0.6494140625, 0.6555397510528564, 0.6637665629386902, 0.6700106263160706, 0.6735026240348816, 0.6779711246490479, 0.6798354387283325, 0.681699812412262, 0.6841856241226196, 0.6837713122367859, 0.6865234375, 0.6881214380264282, 0.6872336864471436, 0.6902817487716675]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 6, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 15:57:27 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 6, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 15:57:27 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 15:57:27 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-25 15:57:27 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 15:57:28 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.3904716968536377s\n",
      "04-25 15:57:28 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-25 15:57:28 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5355784893035889s\n",
      "04-25 15:57:28 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 15:57:28 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.2877483367919922s\n",
      "04-25 15:57:29 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.4431860446929932s\n",
      "04-25 15:57:29 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 15:57:29 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13913393020629883s\n",
      "04-25 15:57:29 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 15:57:29 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009975433349609375s\n",
      "04-25 15:57:29 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 15:57:29 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 15:57:29 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 15:57:29 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 15:57:29 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 15:57:29 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 15:57:29 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9415 - auc: 0.7219 - accuracy: 0.4801 - val_loss: 1.0247 - val_auc: 0.8034 - val_accuracy: 0.5456\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.4309 - auc: 0.8567 - accuracy: 0.6093 - val_loss: 0.9298 - val_auc: 0.8430 - val_accuracy: 0.5953\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.0917 - auc: 0.9129 - accuracy: 0.7128 - val_loss: 0.9326 - val_auc: 0.8537 - val_accuracy: 0.5998\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.8443 - auc: 0.9440 - accuracy: 0.7715 - val_loss: 0.9666 - val_auc: 0.8599 - val_accuracy: 0.6156\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.7096 - auc: 0.9587 - accuracy: 0.8020 - val_loss: 0.9890 - val_auc: 0.8653 - val_accuracy: 0.6395\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.6154 - auc: 0.9683 - accuracy: 0.8258 - val_loss: 1.0245 - val_auc: 0.8674 - val_accuracy: 0.6513\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5432 - auc: 0.9747 - accuracy: 0.8449 - val_loss: 1.0482 - val_auc: 0.8700 - val_accuracy: 0.6518\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4849 - auc: 0.9795 - accuracy: 0.8620 - val_loss: 1.0711 - val_auc: 0.8730 - val_accuracy: 0.6677\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4387 - auc: 0.9829 - accuracy: 0.8745 - val_loss: 1.0871 - val_auc: 0.8758 - val_accuracy: 0.6837\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4008 - auc: 0.9855 - accuracy: 0.8850 - val_loss: 1.1041 - val_auc: 0.8774 - val_accuracy: 0.6837\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3700 - auc: 0.9877 - accuracy: 0.8936 - val_loss: 1.1268 - val_auc: 0.8783 - val_accuracy: 0.6881\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3440 - auc: 0.9893 - accuracy: 0.9006 - val_loss: 1.1504 - val_auc: 0.8788 - val_accuracy: 0.6977\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3232 - auc: 0.9906 - accuracy: 0.9060 - val_loss: 1.1606 - val_auc: 0.8801 - val_accuracy: 0.6954\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3062 - auc: 0.9915 - accuracy: 0.9105 - val_loss: 1.1819 - val_auc: 0.8805 - val_accuracy: 0.7020\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2911 - auc: 0.9924 - accuracy: 0.9141 - val_loss: 1.2050 - val_auc: 0.8805 - val_accuracy: 0.7044\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2771 - auc: 0.9931 - accuracy: 0.9183 - val_loss: 1.2271 - val_auc: 0.8802 - val_accuracy: 0.7000\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2655 - auc: 0.9937 - accuracy: 0.9205 - val_loss: 1.2445 - val_auc: 0.8805 - val_accuracy: 0.7055\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2553 - auc: 0.9941 - accuracy: 0.9235 - val_loss: 1.2451 - val_auc: 0.8818 - val_accuracy: 0.7090\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2471 - auc: 0.9945 - accuracy: 0.9249 - val_loss: 1.2733 - val_auc: 0.8807 - val_accuracy: 0.7037\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2392 - auc: 0.9949 - accuracy: 0.9268 - val_loss: 1.2864 - val_auc: 0.8815 - val_accuracy: 0.7108\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2330 - auc: 0.9951 - accuracy: 0.9283 - val_loss: 1.2974 - val_auc: 0.8811 - val_accuracy: 0.7105\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2276 - auc: 0.9953 - accuracy: 0.9297 - val_loss: 1.2971 - val_auc: 0.8822 - val_accuracy: 0.7115\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2215 - auc: 0.9956 - accuracy: 0.9311 - val_loss: 1.3161 - val_auc: 0.8820 - val_accuracy: 0.7145\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2171 - auc: 0.9958 - accuracy: 0.9318 - val_loss: 1.3185 - val_auc: 0.8818 - val_accuracy: 0.7072\n",
      "Epoch 25/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2132 - auc: 0.9959 - accuracy: 0.9333 - val_loss: 1.3286 - val_auc: 0.8824 - val_accuracy: 0.7088\n",
      "Epoch 26/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2099 - auc: 0.9961 - accuracy: 0.9335 - val_loss: 1.3372 - val_auc: 0.8827 - val_accuracy: 0.7099\n",
      "Epoch 27/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2069 - auc: 0.9961 - accuracy: 0.9342 - val_loss: 1.3553 - val_auc: 0.8816 - val_accuracy: 0.7141\n",
      "Epoch 28/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2021 - auc: 0.9963 - accuracy: 0.9351 - val_loss: 1.3584 - val_auc: 0.8817 - val_accuracy: 0.7137\n",
      "Epoch 29/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2007 - auc: 0.9964 - accuracy: 0.9356 - val_loss: 1.3535 - val_auc: 0.8825 - val_accuracy: 0.7127\n",
      "Epoch 30/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1976 - auc: 0.9965 - accuracy: 0.9363 - val_loss: 1.3689 - val_auc: 0.8820 - val_accuracy: 0.7012\n",
      "Epoch 31/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1938 - auc: 0.9966 - accuracy: 0.9365 - val_loss: 1.3823 - val_auc: 0.8823 - val_accuracy: 0.7092\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00031: early stopping\n",
      "04-25 16:03:38 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 16:03:38 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 16:03:38 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425155727_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9415355920791626, 1.4309008121490479, 1.0916551351547241, 0.8443277478218079, 0.7096332311630249, 0.6154253482818604, 0.54320228099823, 0.4848769009113312, 0.43873175978660583, 0.40078282356262207, 0.37001246213912964, 0.3440439999103546, 0.32316264510154724, 0.30624842643737793, 0.29110345244407654, 0.2770511209964752, 0.2655376195907593, 0.25532159209251404, 0.2471485584974289, 0.23923856019973755, 0.23302008211612701, 0.22755704820156097, 0.22153359651565552, 0.2171253263950348, 0.213212251663208, 0.20987966656684875, 0.20685222744941711, 0.2021416276693344, 0.20069676637649536, 0.19759096205234528, 0.19377894699573517], 'auc': [0.7219398021697998, 0.8566968441009521, 0.9128888845443726, 0.9439636468887329, 0.9587143659591675, 0.9682520627975464, 0.9747110605239868, 0.9794993996620178, 0.9829244613647461, 0.9855362176895142, 0.9876906871795654, 0.9893323183059692, 0.9906460046768188, 0.9915375709533691, 0.9923853874206543, 0.9930777549743652, 0.993657112121582, 0.9941104650497437, 0.9944807291030884, 0.9948642253875732, 0.9950748085975647, 0.9953239560127258, 0.9955922365188599, 0.9957784414291382, 0.9959137439727783, 0.9960668683052063, 0.996142566204071, 0.996303915977478, 0.9963663816452026, 0.9964929223060608, 0.996596097946167], 'accuracy': [0.48013031482696533, 0.6093432903289795, 0.7128271460533142, 0.7714526653289795, 0.8019869923591614, 0.8257558941841125, 0.8448817133903503, 0.8620473146438599, 0.8745099306106567, 0.8849948048591614, 0.8935688138008118, 0.9005915522575378, 0.9060102701187134, 0.9105299711227417, 0.9140766263008118, 0.9182931184768677, 0.9205176830291748, 0.9235213994979858, 0.9249069094657898, 0.9267578125, 0.9283125400543213, 0.929747462272644, 0.9310624599456787, 0.9317886829376221, 0.9333046674728394, 0.9334738850593567, 0.9342424273490906, 0.935088574886322, 0.9356138706207275, 0.9362660646438599, 0.9364670515060425], 'val_loss': [1.024651050567627, 0.9298242330551147, 0.932570219039917, 0.9666156768798828, 0.9890304207801819, 1.0245388746261597, 1.048213243484497, 1.0711244344711304, 1.0871081352233887, 1.1040984392166138, 1.1268407106399536, 1.150425672531128, 1.1605969667434692, 1.1819041967391968, 1.204961895942688, 1.2271032333374023, 1.2444595098495483, 1.2451300621032715, 1.2733060121536255, 1.2863513231277466, 1.2973517179489136, 1.2971303462982178, 1.316050410270691, 1.3185293674468994, 1.3285913467407227, 1.3371531963348389, 1.3552981615066528, 1.358408808708191, 1.3535125255584717, 1.3688640594482422, 1.3822897672653198], 'val_auc': [0.8033933639526367, 0.8430156707763672, 0.8537160158157349, 0.859893798828125, 0.8652937412261963, 0.8674204349517822, 0.8700324296951294, 0.8730455040931702, 0.8757879137992859, 0.8774471282958984, 0.8783280849456787, 0.8788197636604309, 0.8801167011260986, 0.880462110042572, 0.880530059337616, 0.8801791071891785, 0.880488932132721, 0.8817665576934814, 0.8806767463684082, 0.881522536277771, 0.881069540977478, 0.882207453250885, 0.8820430040359497, 0.8818427324295044, 0.8823524713516235, 0.8827390074729919, 0.8815973997116089, 0.8817468881607056, 0.8824695944786072, 0.8820096850395203, 0.8822557926177979], 'val_accuracy': [0.5455729365348816, 0.5952888131141663, 0.5997573137283325, 0.6155894994735718, 0.6395300626754761, 0.651308000087738, 0.6518406867980957, 0.6677024364471436, 0.6837416887283325, 0.6837121248245239, 0.6880918741226196, 0.6976503133773804, 0.695371687412262, 0.7019708752632141, 0.7043974995613098, 0.7000473737716675, 0.7055220007896423, 0.7090139389038086, 0.7037168741226196, 0.7108487486839294, 0.7104936242103577, 0.7114701867103577, 0.7144886255264282, 0.7072088122367859, 0.7088068127632141, 0.7099313735961914, 0.7141335010528564, 0.7137488126754761, 0.7126538753509521, 0.701171875, 0.7091619372367859]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 4, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 16:03:38 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 4, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 16:03:38 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 16:03:39 I deeptables.m.preprocessor.py 242 - Transform [X]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 16:03:40 I deeptables.m.preprocessor.py 249 - transform_X taken 0.9529836177825928s\n",
      "04-25 16:03:40 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 16:03:40 I deeptables.m.preprocessor.py 236 - transform_y taken 0.008976221084594727s\n",
      "04-25 16:03:40 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 16:03:40 I deeptables.m.preprocessor.py 249 - transform_X taken 0.14012980461120605s\n",
      "04-25 16:03:40 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 16:03:40 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009975433349609375s\n",
      "04-25 16:03:40 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 16:03:40 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 16:03:40 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:03:40 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:03:40 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 16:03:41 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 16:03:41 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9442 - auc: 0.7171 - accuracy: 0.4455 - val_loss: 1.0255 - val_auc: 0.8020 - val_accuracy: 0.5330\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.3921 - auc: 0.8653 - accuracy: 0.6232 - val_loss: 0.9281 - val_auc: 0.8422 - val_accuracy: 0.5757\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.0224 - auc: 0.9245 - accuracy: 0.7344 - val_loss: 0.9185 - val_auc: 0.8536 - val_accuracy: 0.5933\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.7573 - auc: 0.9553 - accuracy: 0.7981 - val_loss: 0.9087 - val_auc: 0.8693 - val_accuracy: 0.6296\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5837 - auc: 0.9722 - accuracy: 0.8396 - val_loss: 0.9186 - val_auc: 0.8761 - val_accuracy: 0.6480\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4629 - auc: 0.9820 - accuracy: 0.8716 - val_loss: 0.9167 - val_auc: 0.8860 - val_accuracy: 0.6648\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3870 - auc: 0.9870 - accuracy: 0.8911 - val_loss: 0.9384 - val_auc: 0.8898 - val_accuracy: 0.6761\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3339 - auc: 0.9902 - accuracy: 0.9045 - val_loss: 0.9529 - val_auc: 0.8927 - val_accuracy: 0.6817\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3032 - auc: 0.9918 - accuracy: 0.9124 - val_loss: 0.9744 - val_auc: 0.8940 - val_accuracy: 0.6825\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2793 - auc: 0.9930 - accuracy: 0.9184 - val_loss: 0.9976 - val_auc: 0.8947 - val_accuracy: 0.6859\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2617 - auc: 0.9938 - accuracy: 0.9226 - val_loss: 0.9807 - val_auc: 0.8974 - val_accuracy: 0.6897\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2473 - auc: 0.9944 - accuracy: 0.9258 - val_loss: 1.0010 - val_auc: 0.8974 - val_accuracy: 0.6894\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2370 - auc: 0.9949 - accuracy: 0.9286 - val_loss: 1.0125 - val_auc: 0.8979 - val_accuracy: 0.7042\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2300 - auc: 0.9952 - accuracy: 0.9295 - val_loss: 1.0332 - val_auc: 0.8970 - val_accuracy: 0.6901\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2224 - auc: 0.9955 - accuracy: 0.9323 - val_loss: 1.0359 - val_auc: 0.8971 - val_accuracy: 0.6896\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2168 - auc: 0.9957 - accuracy: 0.9329 - val_loss: 1.0346 - val_auc: 0.8978 - val_accuracy: 0.7037\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2116 - auc: 0.9959 - accuracy: 0.9338 - val_loss: 1.0588 - val_auc: 0.8962 - val_accuracy: 0.6904\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2076 - auc: 0.9961 - accuracy: 0.9348 - val_loss: 1.0750 - val_auc: 0.8966 - val_accuracy: 0.6935\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "04-25 16:07:16 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 16:07:16 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 16:07:16 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425160338_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9441618919372559, 1.3920700550079346, 1.0224335193634033, 0.757290780544281, 0.5837405920028687, 0.46286290884017944, 0.3870196044445038, 0.33385348320007324, 0.3032057583332062, 0.2792786657810211, 0.2617280185222626, 0.24733689427375793, 0.23697899281978607, 0.22996288537979126, 0.2224372923374176, 0.21683554351329803, 0.21155492961406708, 0.20757809281349182], 'auc': [0.7171154022216797, 0.8653356432914734, 0.9245439767837524, 0.955349862575531, 0.9722424745559692, 0.9819834232330322, 0.987048864364624, 0.9901804327964783, 0.9917982816696167, 0.9929652810096741, 0.9937816858291626, 0.9944499731063843, 0.9949365854263306, 0.9951692223548889, 0.9955496191978455, 0.9957436919212341, 0.9959477782249451, 0.996113121509552], 'accuracy': [0.44546762108802795, 0.6231526136398315, 0.7343714833259583, 0.7980666160583496, 0.8396392464637756, 0.8715661764144897, 0.8911114931106567, 0.9045084118843079, 0.9123526215553284, 0.918412983417511, 0.9225871562957764, 0.9258200526237488, 0.9286404252052307, 0.9294865727424622, 0.9322611093521118, 0.9329097867012024, 0.9337806105613708, 0.9348488450050354], 'val_loss': [1.0255262851715088, 0.9281395077705383, 0.9185352921485901, 0.9086788296699524, 0.91855788230896, 0.9166615009307861, 0.9383593201637268, 0.9528710246086121, 0.9743586182594299, 0.9975824356079102, 0.9807085990905762, 1.001011848449707, 1.012511968612671, 1.033237338066101, 1.0359262228012085, 1.0346455574035645, 1.0587939023971558, 1.074967622756958], 'val_auc': [0.8020310997962952, 0.8422043323516846, 0.853574275970459, 0.8693409562110901, 0.876136302947998, 0.886031448841095, 0.8897768259048462, 0.8926507234573364, 0.8940396308898926, 0.8947123885154724, 0.8973622918128967, 0.8974461555480957, 0.897907018661499, 0.896960973739624, 0.8971129059791565, 0.8978495597839355, 0.8962318897247314, 0.8966480493545532], 'val_accuracy': [0.5329959988594055, 0.5756688117980957, 0.5933356881141663, 0.6296460628509521, 0.6480231881141663, 0.6648319363594055, 0.6760771870613098, 0.6816701889038086, 0.6824692487716675, 0.6859315633773804, 0.6897194385528564, 0.6894235610961914, 0.704160749912262, 0.6900745630264282, 0.6896010637283325, 0.7036872506141663, 0.690370500087738, 0.6935073137283325]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 4, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 16:07:16 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 4, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 16:07:16 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 16:07:17 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-25 16:07:17 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 16:07:17 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.3949437141418457s\n",
      "04-25 16:07:17 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-25 16:07:18 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5275886058807373s\n",
      "04-25 16:07:18 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 16:07:18 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.28822922706604004s\n",
      "04-25 16:07:18 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.4341750144958496s\n",
      "04-25 16:07:18 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 16:07:18 I deeptables.m.preprocessor.py 249 - transform_X taken 0.1331501007080078s\n",
      "04-25 16:07:18 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 16:07:18 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009975433349609375s\n",
      "04-25 16:07:18 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 16:07:18 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 16:07:18 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:07:18 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:07:18 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 16:07:19 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 16:07:19 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9480 - auc: 0.7153 - accuracy: 0.4506 - val_loss: 1.0276 - val_auc: 0.8016 - val_accuracy: 0.5482\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.4313 - auc: 0.8570 - accuracy: 0.6100 - val_loss: 0.9424 - val_auc: 0.8380 - val_accuracy: 0.5739\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.0815 - auc: 0.9150 - accuracy: 0.7181 - val_loss: 0.9623 - val_auc: 0.8466 - val_accuracy: 0.5964\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.8255 - auc: 0.9460 - accuracy: 0.7765 - val_loss: 0.9740 - val_auc: 0.8591 - val_accuracy: 0.6182\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.6844 - auc: 0.9608 - accuracy: 0.8061 - val_loss: 1.0078 - val_auc: 0.8633 - val_accuracy: 0.6337\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5847 - auc: 0.9704 - accuracy: 0.8328 - val_loss: 1.0288 - val_auc: 0.8679 - val_accuracy: 0.6433\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5189 - auc: 0.9761 - accuracy: 0.8512 - val_loss: 1.0502 - val_auc: 0.8713 - val_accuracy: 0.6537\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4675 - auc: 0.9804 - accuracy: 0.8653 - val_loss: 1.0698 - val_auc: 0.8734 - val_accuracy: 0.6650\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4285 - auc: 0.9834 - accuracy: 0.8768 - val_loss: 1.0918 - val_auc: 0.8749 - val_accuracy: 0.6668\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3964 - auc: 0.9857 - accuracy: 0.8855 - val_loss: 1.1027 - val_auc: 0.8769 - val_accuracy: 0.6727\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3677 - auc: 0.9877 - accuracy: 0.8937 - val_loss: 1.1203 - val_auc: 0.8777 - val_accuracy: 0.6747\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3446 - auc: 0.9892 - accuracy: 0.9005 - val_loss: 1.1429 - val_auc: 0.8786 - val_accuracy: 0.6846\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3246 - auc: 0.9905 - accuracy: 0.9055 - val_loss: 1.1698 - val_auc: 0.8787 - val_accuracy: 0.6838\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3073 - auc: 0.9914 - accuracy: 0.9102 - val_loss: 1.1723 - val_auc: 0.8803 - val_accuracy: 0.6875\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2936 - auc: 0.9923 - accuracy: 0.9131 - val_loss: 1.1834 - val_auc: 0.8803 - val_accuracy: 0.6915\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2798 - auc: 0.9930 - accuracy: 0.9166 - val_loss: 1.1975 - val_auc: 0.8816 - val_accuracy: 0.6958\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2696 - auc: 0.9934 - accuracy: 0.9199 - val_loss: 1.2139 - val_auc: 0.8808 - val_accuracy: 0.6924\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2577 - auc: 0.9941 - accuracy: 0.9221 - val_loss: 1.2348 - val_auc: 0.8813 - val_accuracy: 0.6884\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2491 - auc: 0.9944 - accuracy: 0.9243 - val_loss: 1.2437 - val_auc: 0.8817 - val_accuracy: 0.6921\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2406 - auc: 0.9948 - accuracy: 0.9265 - val_loss: 1.2528 - val_auc: 0.8828 - val_accuracy: 0.7018\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2333 - auc: 0.9952 - accuracy: 0.9282 - val_loss: 1.2740 - val_auc: 0.8820 - val_accuracy: 0.6912\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2271 - auc: 0.9954 - accuracy: 0.9300 - val_loss: 1.2741 - val_auc: 0.8829 - val_accuracy: 0.6989\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2231 - auc: 0.9956 - accuracy: 0.9310 - val_loss: 1.2863 - val_auc: 0.8830 - val_accuracy: 0.6937\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2166 - auc: 0.9958 - accuracy: 0.9317 - val_loss: 1.3020 - val_auc: 0.8818 - val_accuracy: 0.6977\n",
      "Epoch 25/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2142 - auc: 0.9959 - accuracy: 0.9326 - val_loss: 1.3090 - val_auc: 0.8832 - val_accuracy: 0.6998\n",
      "Epoch 26/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2094 - auc: 0.9961 - accuracy: 0.9335 - val_loss: 1.3054 - val_auc: 0.8839 - val_accuracy: 0.7064\n",
      "Epoch 27/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2063 - auc: 0.9962 - accuracy: 0.9346 - val_loss: 1.3211 - val_auc: 0.8839 - val_accuracy: 0.7062\n",
      "Epoch 28/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2018 - auc: 0.9964 - accuracy: 0.9355 - val_loss: 1.3356 - val_auc: 0.8834 - val_accuracy: 0.7078\n",
      "Epoch 29/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1982 - auc: 0.9965 - accuracy: 0.9363 - val_loss: 1.3498 - val_auc: 0.8828 - val_accuracy: 0.7059\n",
      "Epoch 30/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1971 - auc: 0.9965 - accuracy: 0.9363 - val_loss: 1.3537 - val_auc: 0.8827 - val_accuracy: 0.6983\n",
      "Epoch 31/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1943 - auc: 0.9966 - accuracy: 0.9370 - val_loss: 1.3740 - val_auc: 0.8821 - val_accuracy: 0.6997\n",
      "Epoch 32/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1920 - auc: 0.9967 - accuracy: 0.9378 - val_loss: 1.3834 - val_auc: 0.8824 - val_accuracy: 0.7056\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00032: early stopping\n",
      "04-25 16:13:39 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 16:13:39 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 16:13:39 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425160716_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9479546546936035, 1.4313182830810547, 1.081494688987732, 0.82548588514328, 0.6844485998153687, 0.5847070217132568, 0.5189211368560791, 0.4674844443798065, 0.4284977316856384, 0.39637407660484314, 0.3676588237285614, 0.34457752108573914, 0.3246349096298218, 0.3073033392429352, 0.2936147153377533, 0.2797507643699646, 0.2696287930011749, 0.2576586902141571, 0.24910959601402283, 0.24062740802764893, 0.2333383709192276, 0.22705486416816711, 0.22307951748371124, 0.21664807200431824, 0.21423178911209106, 0.2094106376171112, 0.20630672574043274, 0.20180320739746094, 0.19824247062206268, 0.19714175164699554, 0.1943475753068924, 0.1919526308774948], 'auc': [0.7152584791183472, 0.8570401668548584, 0.9150440096855164, 0.9460197687149048, 0.9608381986618042, 0.970429539680481, 0.9760922193527222, 0.9803901314735413, 0.9834092855453491, 0.9857386350631714, 0.9877156615257263, 0.9892479181289673, 0.9904873967170715, 0.9914243221282959, 0.992286741733551, 0.9929780960083008, 0.9934365153312683, 0.9940645694732666, 0.9944419860839844, 0.9948276877403259, 0.9951570630073547, 0.9954047203063965, 0.9955762028694153, 0.9957880973815918, 0.9959032535552979, 0.9960938692092896, 0.9961796998977661, 0.9963593482971191, 0.9964643120765686, 0.9965226650238037, 0.9966225028038025, 0.9967153072357178], 'accuracy': [0.45061132311820984, 0.6100448369979858, 0.7181471586227417, 0.776451826095581, 0.806058943271637, 0.8328456282615662, 0.8512346148490906, 0.865294337272644, 0.8768015503883362, 0.8854883313179016, 0.8937062621116638, 0.9004822969436646, 0.9054743647575378, 0.9101844429969788, 0.9131141304969788, 0.9166396260261536, 0.919861912727356, 0.9221041798591614, 0.9242688417434692, 0.9265180826187134, 0.9281856417655945, 0.9299730658531189, 0.9309989809989929, 0.931739330291748, 0.9326277375221252, 0.9334880113601685, 0.9345844388008118, 0.9354763627052307, 0.9362660646438599, 0.9362590312957764, 0.937017023563385, 0.9377855658531189], 'val_loss': [1.027564525604248, 0.9423999190330505, 0.9623026251792908, 0.9740423560142517, 1.0078437328338623, 1.0287946462631226, 1.0501927137374878, 1.069848895072937, 1.0917996168136597, 1.1026861667633057, 1.1203157901763916, 1.1429370641708374, 1.169765591621399, 1.1723103523254395, 1.1834403276443481, 1.197502613067627, 1.2138556241989136, 1.2348079681396484, 1.243654489517212, 1.2528270483016968, 1.273988962173462, 1.2741442918777466, 1.2863316535949707, 1.3019920587539673, 1.3089839220046997, 1.305402398109436, 1.3210816383361816, 1.3355690240859985, 1.3497649431228638, 1.353730320930481, 1.3740407228469849, 1.3833988904953003], 'val_auc': [0.8015802502632141, 0.8380452990531921, 0.8465566635131836, 0.8591448068618774, 0.8632933497428894, 0.8679279088973999, 0.8713275194168091, 0.8734359741210938, 0.8748568296432495, 0.8769256472587585, 0.8776819109916687, 0.8785516619682312, 0.8786885738372803, 0.880255937576294, 0.8802530169487, 0.8816376328468323, 0.8808478116989136, 0.8813103437423706, 0.88174968957901, 0.8828068971633911, 0.8819870948791504, 0.8828824758529663, 0.8829646110534668, 0.8817508220672607, 0.8832277059555054, 0.8839158415794373, 0.8839293718338013, 0.8834415078163147, 0.8828205466270447, 0.8826736211776733, 0.882149338722229, 0.8823548555374146], 'val_accuracy': [0.5482362508773804, 0.5738932490348816, 0.5963837504386902, 0.618223249912262, 0.6336706876754761, 0.6432883739471436, 0.6536754369735718, 0.6650094985961914, 0.6667850613594055, 0.6727036237716675, 0.6746863126754761, 0.684629499912262, 0.6838304996490479, 0.6875, 0.6915246248245239, 0.6957859992980957, 0.6923828125, 0.6883878111839294, 0.6920868754386902, 0.7017933130264282, 0.6911695003509521, 0.6988932490348816, 0.6937440633773804, 0.6976503133773804, 0.6997514367103577, 0.7064098119735718, 0.7062322497367859, 0.7078006863594055, 0.7059067487716675, 0.6983309388160706, 0.6997218132019043, 0.7056108117103577]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 4, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 16:13:39 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 4, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 16:13:39 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 16:13:40 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 16:13:41 I deeptables.m.preprocessor.py 249 - transform_X taken 0.9509820938110352s\n",
      "04-25 16:13:41 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 16:13:41 I deeptables.m.preprocessor.py 236 - transform_y taken 0.006981372833251953s\n",
      "04-25 16:13:41 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 16:13:41 I deeptables.m.preprocessor.py 249 - transform_X taken 0.14013147354125977s\n",
      "04-25 16:13:41 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 16:13:41 I deeptables.m.preprocessor.py 236 - transform_y taken 0.001995086669921875s\n",
      "04-25 16:13:41 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 16:13:41 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 16:13:41 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:13:41 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:13:41 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 16:13:42 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 16:13:42 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9433 - auc: 0.7203 - accuracy: 0.4828 - val_loss: 1.0309 - val_auc: 0.7999 - val_accuracy: 0.5355\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.4282 - auc: 0.8577 - accuracy: 0.6093 - val_loss: 0.9488 - val_auc: 0.8356 - val_accuracy: 0.5701\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.0798 - auc: 0.9151 - accuracy: 0.7174 - val_loss: 0.9454 - val_auc: 0.8518 - val_accuracy: 0.5987\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.8289 - auc: 0.9459 - accuracy: 0.7754 - val_loss: 0.9665 - val_auc: 0.8601 - val_accuracy: 0.6153\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.6892 - auc: 0.9606 - accuracy: 0.8056 - val_loss: 0.9988 - val_auc: 0.8645 - val_accuracy: 0.6297\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5921 - auc: 0.9699 - accuracy: 0.8308 - val_loss: 1.0365 - val_auc: 0.8668 - val_accuracy: 0.6385\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5267 - auc: 0.9756 - accuracy: 0.8489 - val_loss: 1.0557 - val_auc: 0.8711 - val_accuracy: 0.6498\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4767 - auc: 0.9797 - accuracy: 0.8627 - val_loss: 1.0806 - val_auc: 0.8730 - val_accuracy: 0.6586\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4362 - auc: 0.9828 - accuracy: 0.8750 - val_loss: 1.0880 - val_auc: 0.8750 - val_accuracy: 0.6613\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4022 - auc: 0.9853 - accuracy: 0.8843 - val_loss: 1.1163 - val_auc: 0.8756 - val_accuracy: 0.6682\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3760 - auc: 0.9872 - accuracy: 0.8916 - val_loss: 1.1315 - val_auc: 0.8766 - val_accuracy: 0.6771\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3510 - auc: 0.9888 - accuracy: 0.8987 - val_loss: 1.1554 - val_auc: 0.8775 - val_accuracy: 0.6760\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3312 - auc: 0.9901 - accuracy: 0.9037 - val_loss: 1.1703 - val_auc: 0.8781 - val_accuracy: 0.6793\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3128 - auc: 0.9912 - accuracy: 0.9081 - val_loss: 1.1961 - val_auc: 0.8784 - val_accuracy: 0.6807\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2987 - auc: 0.9919 - accuracy: 0.9121 - val_loss: 1.2025 - val_auc: 0.8800 - val_accuracy: 0.6871\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2837 - auc: 0.9928 - accuracy: 0.9159 - val_loss: 1.2287 - val_auc: 0.8788 - val_accuracy: 0.6868\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2730 - auc: 0.9933 - accuracy: 0.9182 - val_loss: 1.2450 - val_auc: 0.8789 - val_accuracy: 0.6865\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2621 - auc: 0.9938 - accuracy: 0.9213 - val_loss: 1.2518 - val_auc: 0.8798 - val_accuracy: 0.6890\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2521 - auc: 0.9943 - accuracy: 0.9238 - val_loss: 1.2782 - val_auc: 0.8786 - val_accuracy: 0.6865\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2454 - auc: 0.9946 - accuracy: 0.9255 - val_loss: 1.2854 - val_auc: 0.8798 - val_accuracy: 0.6880\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "04-25 16:17:42 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 16:17:42 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 16:17:42 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425161339_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9433459043502808, 1.4281727075576782, 1.0798187255859375, 0.8289065361022949, 0.6891868114471436, 0.5921371579170227, 0.5266978144645691, 0.47666963934898376, 0.436172753572464, 0.40220868587493896, 0.375997930765152, 0.35095950961112976, 0.3311948776245117, 0.31284940242767334, 0.2987459897994995, 0.2836751937866211, 0.2730249762535095, 0.26209747791290283, 0.25214719772338867, 0.2453906089067459], 'auc': [0.7202510237693787, 0.8576863408088684, 0.9150762557983398, 0.9458588361740112, 0.9606017470359802, 0.9698805809020996, 0.9755762219429016, 0.9796692132949829, 0.9827963709831238, 0.9853248000144958, 0.9872490763664246, 0.9888361096382141, 0.9900935888290405, 0.991152822971344, 0.9919480681419373, 0.99275803565979, 0.9932804107666016, 0.9937816262245178, 0.9943020343780518, 0.9946313500404358], 'accuracy': [0.4828414022922516, 0.609325647354126, 0.7174208760261536, 0.7753800749778748, 0.8056252598762512, 0.8307761549949646, 0.8488655090332031, 0.8626854419708252, 0.874950647354126, 0.8842650055885315, 0.8915804028511047, 0.8987230658531189, 0.9036975502967834, 0.908129096031189, 0.912084698677063, 0.9158922433853149, 0.9182331562042236, 0.9213497042655945, 0.9238422513008118, 0.9255309104919434], 'val_loss': [1.0309057235717773, 0.9488329887390137, 0.9453909397125244, 0.9665156006813049, 0.998825192451477, 1.0365384817123413, 1.0557397603988647, 1.080605387687683, 1.0879696607589722, 1.1162751913070679, 1.1314572095870972, 1.1553839445114136, 1.1703178882598877, 1.1961473226547241, 1.2024928331375122, 1.2287431955337524, 1.2450332641601562, 1.251823902130127, 1.2782418727874756, 1.2854257822036743], 'val_auc': [0.7999261617660522, 0.8356366157531738, 0.8517727851867676, 0.8601198196411133, 0.8644697666168213, 0.8667700886726379, 0.8711041212081909, 0.8729561567306519, 0.8750321269035339, 0.8755518794059753, 0.8766083121299744, 0.8775284290313721, 0.8781359791755676, 0.87842857837677, 0.8799684047698975, 0.8787684440612793, 0.8788604140281677, 0.8798078894615173, 0.8786497116088867, 0.8798302412033081], 'val_accuracy': [0.5354521870613098, 0.5701349377632141, 0.598691999912262, 0.6152639389038086, 0.6297052502632141, 0.6384647488594055, 0.6497691869735718, 0.6585878133773804, 0.6613399386405945, 0.6682054996490479, 0.6770537495613098, 0.6760179996490479, 0.6792731881141663, 0.6806936264038086, 0.6871153116226196, 0.6867897510528564, 0.6865234375, 0.6890388131141663, 0.6864938735961914, 0.6879734992980957]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 3, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 16:17:43 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 3, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 16:17:43 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 16:17:43 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-25 16:17:43 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 16:17:44 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.3919515609741211s\n",
      "04-25 16:17:44 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-25 16:17:44 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5340769290924072s\n",
      "04-25 16:17:44 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 16:17:44 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.2927367687225342s\n",
      "04-25 16:17:45 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.4496541023254395s\n",
      "04-25 16:17:45 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 16:17:45 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13663482666015625s\n",
      "04-25 16:17:45 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 16:17:45 I deeptables.m.preprocessor.py 236 - transform_y taken 0.000997781753540039s\n",
      "04-25 16:17:45 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 16:17:45 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 16:17:45 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:17:45 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:17:45 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 16:17:46 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 16:17:46 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 45ms/step - loss: 1.9393 - auc: 0.7197 - accuracy: 0.4707 - val_loss: 1.0275 - val_auc: 0.8019 - val_accuracy: 0.5344\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.4090 - auc: 0.8615 - accuracy: 0.6168 - val_loss: 0.9311 - val_auc: 0.8417 - val_accuracy: 0.5762\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.0105 - auc: 0.9250 - accuracy: 0.7371 - val_loss: 0.9275 - val_auc: 0.8529 - val_accuracy: 0.5988\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.7413 - auc: 0.9556 - accuracy: 0.7988 - val_loss: 0.9114 - val_auc: 0.8689 - val_accuracy: 0.6256\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.6068 - auc: 0.9686 - accuracy: 0.8287 - val_loss: 0.9216 - val_auc: 0.8738 - val_accuracy: 0.6354\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5315 - auc: 0.9752 - accuracy: 0.8483 - val_loss: 0.9427 - val_auc: 0.8754 - val_accuracy: 0.6429\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4812 - auc: 0.9791 - accuracy: 0.8619 - val_loss: 0.9598 - val_auc: 0.8776 - val_accuracy: 0.6519\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4482 - auc: 0.9818 - accuracy: 0.8705 - val_loss: 0.9713 - val_auc: 0.8792 - val_accuracy: 0.6538\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4193 - auc: 0.9839 - accuracy: 0.8777 - val_loss: 0.9808 - val_auc: 0.8804 - val_accuracy: 0.6573\n",
      "Epoch 10/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3926 - auc: 0.9859 - accuracy: 0.8852 - val_loss: 0.9972 - val_auc: 0.8816 - val_accuracy: 0.6639\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3698 - auc: 0.9875 - accuracy: 0.8911 - val_loss: 1.0089 - val_auc: 0.8827 - val_accuracy: 0.6667\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3480 - auc: 0.9889 - accuracy: 0.8973 - val_loss: 1.0182 - val_auc: 0.8847 - val_accuracy: 0.6710\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3294 - auc: 0.9901 - accuracy: 0.9031 - val_loss: 1.0324 - val_auc: 0.8862 - val_accuracy: 0.6878\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3115 - auc: 0.9912 - accuracy: 0.9074 - val_loss: 1.0437 - val_auc: 0.8873 - val_accuracy: 0.6790\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2984 - auc: 0.9919 - accuracy: 0.9110 - val_loss: 1.0669 - val_auc: 0.8866 - val_accuracy: 0.6784\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2823 - auc: 0.9928 - accuracy: 0.9157 - val_loss: 1.0903 - val_auc: 0.8866 - val_accuracy: 0.6804\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2702 - auc: 0.9934 - accuracy: 0.9184 - val_loss: 1.0936 - val_auc: 0.8874 - val_accuracy: 0.6971\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2607 - auc: 0.9938 - accuracy: 0.9208 - val_loss: 1.1145 - val_auc: 0.8875 - val_accuracy: 0.6825\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2511 - auc: 0.9943 - accuracy: 0.9230 - val_loss: 1.1327 - val_auc: 0.8868 - val_accuracy: 0.6986\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2410 - auc: 0.9948 - accuracy: 0.9261 - val_loss: 1.1396 - val_auc: 0.8875 - val_accuracy: 0.6985\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2338 - auc: 0.9950 - accuracy: 0.9274 - val_loss: 1.1460 - val_auc: 0.8883 - val_accuracy: 0.7000\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.2264 - auc: 0.9954 - accuracy: 0.9291 - val_loss: 1.1883 - val_auc: 0.8867 - val_accuracy: 0.6889\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2209 - auc: 0.9956 - accuracy: 0.9309 - val_loss: 1.1867 - val_auc: 0.8870 - val_accuracy: 0.6893\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2144 - auc: 0.9958 - accuracy: 0.9318 - val_loss: 1.1830 - val_auc: 0.8892 - val_accuracy: 0.6931\n",
      "Epoch 25/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2103 - auc: 0.9960 - accuracy: 0.9328 - val_loss: 1.2051 - val_auc: 0.8874 - val_accuracy: 0.6892\n",
      "Epoch 26/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2052 - auc: 0.9962 - accuracy: 0.9341 - val_loss: 1.2167 - val_auc: 0.8882 - val_accuracy: 0.6922\n",
      "Epoch 27/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2036 - auc: 0.9963 - accuracy: 0.9344 - val_loss: 1.2167 - val_auc: 0.8885 - val_accuracy: 0.6937\n",
      "Epoch 28/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1995 - auc: 0.9964 - accuracy: 0.9356 - val_loss: 1.2460 - val_auc: 0.8869 - val_accuracy: 0.6922\n",
      "Epoch 29/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1961 - auc: 0.9965 - accuracy: 0.9361 - val_loss: 1.2462 - val_auc: 0.8868 - val_accuracy: 0.6944\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00029: early stopping\n",
      "04-25 16:23:32 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 16:23:32 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 16:23:32 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425161743_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9393315315246582, 1.409049391746521, 1.0105031728744507, 0.741330623626709, 0.6067765951156616, 0.5314865112304688, 0.48119863867759705, 0.44821539521217346, 0.4193110167980194, 0.392625629901886, 0.36977362632751465, 0.34800684452056885, 0.32940027117729187, 0.31153398752212524, 0.2983947694301605, 0.28228452801704407, 0.2701774835586548, 0.26072144508361816, 0.25107115507125854, 0.24101199209690094, 0.23375442624092102, 0.2264201045036316, 0.2208796739578247, 0.21439975500106812, 0.2102668136358261, 0.20524166524410248, 0.20359599590301514, 0.19954350590705872, 0.19605857133865356], 'auc': [0.71974778175354, 0.8615366816520691, 0.9249938130378723, 0.9556251764297485, 0.9686316251754761, 0.9751961827278137, 0.9791479706764221, 0.9817901253700256, 0.9839204549789429, 0.9859001636505127, 0.9874723553657532, 0.9889097213745117, 0.9900525808334351, 0.9911707043647766, 0.9919413924217224, 0.9928203225135803, 0.9934254288673401, 0.9938466548919678, 0.994303822517395, 0.9947828650474548, 0.9950487017631531, 0.9953752756118774, 0.9955855011940002, 0.9958302974700928, 0.9960071444511414, 0.9961754679679871, 0.9962677955627441, 0.9963777661323547, 0.9965184926986694], 'accuracy': [0.470678448677063, 0.6168137788772583, 0.7371143102645874, 0.7988492846488953, 0.8287278413772583, 0.8482697010040283, 0.8619204163551331, 0.8704591393470764, 0.8776793479919434, 0.8852274417877197, 0.8910727500915527, 0.8973199129104614, 0.9030770659446716, 0.9073746204376221, 0.9109776616096497, 0.9156877398490906, 0.9184165000915527, 0.920849084854126, 0.9229679107666016, 0.9261373281478882, 0.9274170994758606, 0.9290916919708252, 0.9308649897575378, 0.9317604899406433, 0.9327828884124756, 0.934076726436615, 0.9343517422676086, 0.9355927109718323, 0.9360756874084473], 'val_loss': [1.0274580717086792, 0.9310705661773682, 0.9275363683700562, 0.9114141464233398, 0.9216384291648865, 0.942703366279602, 0.9597593545913696, 0.9713475704193115, 0.9808105826377869, 0.9972490072250366, 1.0089031457901, 1.018241047859192, 1.032380223274231, 1.0437092781066895, 1.0668797492980957, 1.0902738571166992, 1.0935918092727661, 1.1145204305648804, 1.1326531171798706, 1.1396244764328003, 1.1460312604904175, 1.188302993774414, 1.186729073524475, 1.182966947555542, 1.20505952835083, 1.2166566848754883, 1.216659665107727, 1.2459849119186401, 1.2462135553359985], 'val_auc': [0.801892876625061, 0.8416906595230103, 0.8529003858566284, 0.8689085245132446, 0.8737862706184387, 0.8753856420516968, 0.8776049613952637, 0.8791897892951965, 0.8803598880767822, 0.8815566301345825, 0.8827390670776367, 0.8846757411956787, 0.8861585855484009, 0.8872883319854736, 0.8866197466850281, 0.8866145610809326, 0.8874036073684692, 0.8875197768211365, 0.8868153691291809, 0.8874967694282532, 0.8882904052734375, 0.886729896068573, 0.8870400190353394, 0.8891631364822388, 0.8873547315597534, 0.8882191777229309, 0.8885400295257568, 0.8868645429611206, 0.8868061900138855], 'val_accuracy': [0.5343868136405945, 0.576171875, 0.5987511873245239, 0.6256214380264282, 0.6353574991226196, 0.6428740620613098, 0.6518702507019043, 0.6537641882896423, 0.6573153138160706, 0.6639441251754761, 0.6666666865348816, 0.670957624912262, 0.6877663135528564, 0.6790364384651184, 0.6784446239471436, 0.6803977489471436, 0.6970880627632141, 0.6825284361839294, 0.6985973119735718, 0.6985381245613098, 0.7000473737716675, 0.6888612508773804, 0.6893051862716675, 0.6930930614471436, 0.6892459988594055, 0.6922348737716675, 0.6936848759651184, 0.6922052502632141, 0.6943655014038086]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 4, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 16:23:32 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 4, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 16:23:32 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 16:23:33 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 16:23:34 I deeptables.m.preprocessor.py 249 - transform_X taken 0.9589643478393555s\n",
      "04-25 16:23:34 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 16:23:34 I deeptables.m.preprocessor.py 236 - transform_y taken 0.007978677749633789s\n",
      "04-25 16:23:34 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 16:23:34 I deeptables.m.preprocessor.py 249 - transform_X taken 0.14461350440979004s\n",
      "04-25 16:23:34 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 16:23:34 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009975433349609375s\n",
      "04-25 16:23:34 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 16:23:34 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 16:23:34 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:23:34 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:23:34 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 16:23:35 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 16:23:35 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9435 - auc: 0.7188 - accuracy: 0.4583 - val_loss: 1.0296 - val_auc: 0.8003 - val_accuracy: 0.5281\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.4234 - auc: 0.8591 - accuracy: 0.6147 - val_loss: 0.9411 - val_auc: 0.8400 - val_accuracy: 0.5772\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.0511 - auc: 0.9198 - accuracy: 0.7269 - val_loss: 0.9329 - val_auc: 0.8567 - val_accuracy: 0.6067\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.7900 - auc: 0.9508 - accuracy: 0.7871 - val_loss: 0.9610 - val_auc: 0.8647 - val_accuracy: 0.6380\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.6457 - auc: 0.9654 - accuracy: 0.8196 - val_loss: 0.9965 - val_auc: 0.8689 - val_accuracy: 0.6475\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5587 - auc: 0.9730 - accuracy: 0.8413 - val_loss: 1.0177 - val_auc: 0.8721 - val_accuracy: 0.6590\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5001 - auc: 0.9779 - accuracy: 0.8573 - val_loss: 1.0351 - val_auc: 0.8745 - val_accuracy: 0.6669\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4547 - auc: 0.9814 - accuracy: 0.8695 - val_loss: 1.0549 - val_auc: 0.8762 - val_accuracy: 0.6699\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4183 - auc: 0.9842 - accuracy: 0.8796 - val_loss: 1.0684 - val_auc: 0.8787 - val_accuracy: 0.6795\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3881 - auc: 0.9864 - accuracy: 0.8885 - val_loss: 1.0937 - val_auc: 0.8790 - val_accuracy: 0.6826\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3636 - auc: 0.9881 - accuracy: 0.8953 - val_loss: 1.1215 - val_auc: 0.8791 - val_accuracy: 0.6867\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3390 - auc: 0.9896 - accuracy: 0.9022 - val_loss: 1.1178 - val_auc: 0.8819 - val_accuracy: 0.6904\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3200 - auc: 0.9908 - accuracy: 0.9070 - val_loss: 1.1341 - val_auc: 0.8826 - val_accuracy: 0.6923\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3032 - auc: 0.9916 - accuracy: 0.9109 - val_loss: 1.1669 - val_auc: 0.8822 - val_accuracy: 0.6941\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2902 - auc: 0.9925 - accuracy: 0.9149 - val_loss: 1.1825 - val_auc: 0.8826 - val_accuracy: 0.6993\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2754 - auc: 0.9932 - accuracy: 0.9184 - val_loss: 1.2009 - val_auc: 0.8826 - val_accuracy: 0.6995\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2623 - auc: 0.9938 - accuracy: 0.9212 - val_loss: 1.2342 - val_auc: 0.8826 - val_accuracy: 0.7005\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2545 - auc: 0.9942 - accuracy: 0.9237 - val_loss: 1.2120 - val_auc: 0.8845 - val_accuracy: 0.7028\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2457 - auc: 0.9947 - accuracy: 0.9254 - val_loss: 1.2335 - val_auc: 0.8845 - val_accuracy: 0.7053\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2378 - auc: 0.9950 - accuracy: 0.9275 - val_loss: 1.2625 - val_auc: 0.8837 - val_accuracy: 0.7044\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2318 - auc: 0.9952 - accuracy: 0.9285 - val_loss: 1.2434 - val_auc: 0.8860 - val_accuracy: 0.7071\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2253 - auc: 0.9955 - accuracy: 0.9301 - val_loss: 1.2995 - val_auc: 0.8834 - val_accuracy: 0.7037\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2198 - auc: 0.9957 - accuracy: 0.9315 - val_loss: 1.3011 - val_auc: 0.8838 - val_accuracy: 0.7048\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2163 - auc: 0.9958 - accuracy: 0.9320 - val_loss: 1.3017 - val_auc: 0.8845 - val_accuracy: 0.7056\n",
      "Epoch 25/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2101 - auc: 0.9961 - accuracy: 0.9331 - val_loss: 1.3157 - val_auc: 0.8851 - val_accuracy: 0.7075\n",
      "Epoch 26/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2081 - auc: 0.9962 - accuracy: 0.9338 - val_loss: 1.3419 - val_auc: 0.8838 - val_accuracy: 0.7063\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00026: early stopping\n",
      "04-25 16:28:46 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 16:28:46 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 16:28:47 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425162332_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9434741735458374, 1.4234007596969604, 1.0511033535003662, 0.7899739742279053, 0.6456621885299683, 0.5586524605751038, 0.5001219511032104, 0.4546773433685303, 0.41830208897590637, 0.38810449838638306, 0.36359158158302307, 0.3389711081981659, 0.31998080015182495, 0.30321064591407776, 0.29024165868759155, 0.27536413073539734, 0.26234570145606995, 0.25445377826690674, 0.24568165838718414, 0.23778434097766876, 0.23178455233573914, 0.22532187402248383, 0.21980932354927063, 0.21634386479854584, 0.21011875569820404, 0.20811276137828827], 'auc': [0.7187938094139099, 0.8590976595878601, 0.9198405742645264, 0.9507505297660828, 0.9653516411781311, 0.9729983806610107, 0.9779142737388611, 0.9813975691795349, 0.9841500520706177, 0.9863879680633545, 0.9880880117416382, 0.9895753264427185, 0.9908084273338318, 0.9916390180587769, 0.9924680590629578, 0.9932214021682739, 0.9937944412231445, 0.9942032694816589, 0.9946657419204712, 0.9949914813041687, 0.9951918721199036, 0.9954557418823242, 0.9956672191619873, 0.9958135485649109, 0.9960811138153076, 0.9961541295051575], 'accuracy': [0.45834624767303467, 0.6146844029426575, 0.7269291281700134, 0.7870988249778748, 0.8196285367012024, 0.8412610292434692, 0.857323169708252, 0.8694896697998047, 0.8796430826187134, 0.8885308504104614, 0.8953491449356079, 0.9021850824356079, 0.9069938659667969, 0.9109318852424622, 0.9149086475372314, 0.9183741807937622, 0.9212157130241394, 0.9236553907394409, 0.9254463315010071, 0.9275334477424622, 0.9285170435905457, 0.9300929307937622, 0.9315348863601685, 0.9320143461227417, 0.933086097240448, 0.9338440895080566], 'val_loss': [1.029625415802002, 0.9411142468452454, 0.9328773617744446, 0.9610385894775391, 0.996526300907135, 1.017716646194458, 1.0351452827453613, 1.0548791885375977, 1.0684142112731934, 1.0937389135360718, 1.1214599609375, 1.1178277730941772, 1.1340593099594116, 1.1668870449066162, 1.1824891567230225, 1.200886607170105, 1.2341995239257812, 1.2119622230529785, 1.2335079908370972, 1.2624996900558472, 1.2434133291244507, 1.2994885444641113, 1.3010832071304321, 1.3017092943191528, 1.3157340288162231, 1.3419129848480225], 'val_auc': [0.8003169298171997, 0.8400059342384338, 0.8566592335700989, 0.8647138476371765, 0.8688957095146179, 0.8721367716789246, 0.8745423555374146, 0.8762407302856445, 0.8786808252334595, 0.8790052533149719, 0.8791409134864807, 0.8818976283073425, 0.8825674057006836, 0.8821626901626587, 0.8825541734695435, 0.8825925588607788, 0.8826025128364563, 0.8845327496528625, 0.8844719529151917, 0.8837099075317383, 0.8860402703285217, 0.8834084868431091, 0.8838468790054321, 0.8844947218894958, 0.885099470615387, 0.8837754130363464], 'val_accuracy': [0.5280835628509521, 0.577207624912262, 0.6066820621490479, 0.6380208134651184, 0.647520124912262, 0.6590021252632141, 0.6669034361839294, 0.6698923110961914, 0.6795099377632141, 0.682558000087738, 0.6867305636405945, 0.690370500087738, 0.692323625087738, 0.6941288113594055, 0.699277937412262, 0.6995146870613098, 0.7005208134651184, 0.7027698755264282, 0.7052852511405945, 0.7043678760528564, 0.707090437412262, 0.7036872506141663, 0.7048118114471436, 0.7056108117103577, 0.7074751257896423, 0.7062914371490479]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 2, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 16:28:47 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 2, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 16:28:47 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 16:28:47 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-25 16:28:47 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 16:28:48 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.39345335960388184s\n",
      "04-25 16:28:48 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-25 16:28:48 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5076658725738525s\n",
      "04-25 16:28:48 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 16:28:49 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.2822456359863281s\n",
      "04-25 16:28:49 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.4062752723693848s\n",
      "04-25 16:28:49 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 16:28:49 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13364315032958984s\n",
      "04-25 16:28:49 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 16:28:49 I deeptables.m.preprocessor.py 236 - transform_y taken 0.000997781753540039s\n",
      "04-25 16:28:49 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 16:28:49 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 16:28:49 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:28:49 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:28:49 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 16:28:49 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 16:28:49 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9377 - auc: 0.7214 - accuracy: 0.4691 - val_loss: 1.0441 - val_auc: 0.7944 - val_accuracy: 0.5248\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.4280 - auc: 0.8575 - accuracy: 0.6085 - val_loss: 0.9479 - val_auc: 0.8354 - val_accuracy: 0.5682\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.0877 - auc: 0.9139 - accuracy: 0.7146 - val_loss: 0.9176 - val_auc: 0.8539 - val_accuracy: 0.5998\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.8046 - auc: 0.9492 - accuracy: 0.7826 - val_loss: 0.9343 - val_auc: 0.8643 - val_accuracy: 0.6238\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.6145 - auc: 0.9685 - accuracy: 0.8288 - val_loss: 0.9579 - val_auc: 0.8722 - val_accuracy: 0.6451\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4907 - auc: 0.9792 - accuracy: 0.8626 - val_loss: 0.9731 - val_auc: 0.8788 - val_accuracy: 0.6609\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4098 - auc: 0.9852 - accuracy: 0.8843 - val_loss: 0.9901 - val_auc: 0.8841 - val_accuracy: 0.6742\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3523 - auc: 0.9889 - accuracy: 0.8999 - val_loss: 1.0075 - val_auc: 0.8878 - val_accuracy: 0.6797\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3152 - auc: 0.9911 - accuracy: 0.9094 - val_loss: 1.0445 - val_auc: 0.8871 - val_accuracy: 0.6818\n",
      "Epoch 10/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2850 - auc: 0.9926 - accuracy: 0.9168 - val_loss: 1.0480 - val_auc: 0.8899 - val_accuracy: 0.6876\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2681 - auc: 0.9935 - accuracy: 0.9214 - val_loss: 1.0688 - val_auc: 0.8896 - val_accuracy: 0.6889\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2527 - auc: 0.9942 - accuracy: 0.9245 - val_loss: 1.0802 - val_auc: 0.8900 - val_accuracy: 0.6898\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2437 - auc: 0.9946 - accuracy: 0.9269 - val_loss: 1.0839 - val_auc: 0.8903 - val_accuracy: 0.6913\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2331 - auc: 0.9950 - accuracy: 0.9291 - val_loss: 1.0923 - val_auc: 0.8911 - val_accuracy: 0.6922\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2250 - auc: 0.9954 - accuracy: 0.9310 - val_loss: 1.0996 - val_auc: 0.8913 - val_accuracy: 0.6935\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2206 - auc: 0.9956 - accuracy: 0.9319 - val_loss: 1.1082 - val_auc: 0.8903 - val_accuracy: 0.6922\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2138 - auc: 0.9958 - accuracy: 0.9337 - val_loss: 1.1226 - val_auc: 0.8899 - val_accuracy: 0.6903\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2098 - auc: 0.9960 - accuracy: 0.9345 - val_loss: 1.1159 - val_auc: 0.8910 - val_accuracy: 0.6940\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2046 - auc: 0.9962 - accuracy: 0.9352 - val_loss: 1.1218 - val_auc: 0.8910 - val_accuracy: 0.6948\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2004 - auc: 0.9964 - accuracy: 0.9361 - val_loss: 1.1413 - val_auc: 0.8890 - val_accuracy: 0.6942\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "04-25 16:32:49 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 16:32:49 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 16:32:50 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425162847_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.937740445137024, 1.4279885292053223, 1.0876569747924805, 0.804588258266449, 0.6145394444465637, 0.4907037615776062, 0.4098086357116699, 0.3522568643093109, 0.31524646282196045, 0.28502240777015686, 0.2681199610233307, 0.2526930868625641, 0.24369490146636963, 0.23307351768016815, 0.2250044345855713, 0.2205909937620163, 0.2137787938117981, 0.209793359041214, 0.20460407435894012, 0.20039881765842438], 'auc': [0.7214305996894836, 0.857520580291748, 0.913935661315918, 0.9491629600524902, 0.968506932258606, 0.979247510433197, 0.9852049350738525, 0.988893449306488, 0.9910727143287659, 0.9925804138183594, 0.9935409426689148, 0.9941909313201904, 0.9946466684341431, 0.9950470924377441, 0.9954159259796143, 0.9955729842185974, 0.9958174228668213, 0.9959582090377808, 0.9962145090103149, 0.9963518977165222], 'accuracy': [0.46912723779678345, 0.6085429787635803, 0.7146110534667969, 0.7825791239738464, 0.8288054466247559, 0.8626008033752441, 0.8843496441841125, 0.8998512029647827, 0.9094123840332031, 0.9168441295623779, 0.9213743805885315, 0.9245049953460693, 0.9269129633903503, 0.9290599822998047, 0.9309707880020142, 0.9319085478782654, 0.9336607456207275, 0.934485673904419, 0.9351837635040283, 0.9361391663551331], 'val_loss': [1.044122576713562, 0.9478757381439209, 0.9176045656204224, 0.9343340992927551, 0.9578689336776733, 0.9731267094612122, 0.9901309013366699, 1.0074779987335205, 1.0445367097854614, 1.0480008125305176, 1.0687744617462158, 1.0801857709884644, 1.083920955657959, 1.092322826385498, 1.0996252298355103, 1.1081730127334595, 1.1226407289505005, 1.1158698797225952, 1.1217657327651978, 1.1413336992263794], 'val_auc': [0.7943997979164124, 0.8353907465934753, 0.853945255279541, 0.864301860332489, 0.8721816539764404, 0.8787938356399536, 0.8840677738189697, 0.8877809643745422, 0.8870590925216675, 0.8899343609809875, 0.8896199464797974, 0.8900245428085327, 0.8903194665908813, 0.8911499977111816, 0.8912876844406128, 0.8902906179428101, 0.8899252414703369, 0.8909989595413208, 0.8909595012664795, 0.8889922499656677], 'val_accuracy': [0.5248283743858337, 0.5681522488594055, 0.5998165011405945, 0.6238163113594055, 0.6450639367103577, 0.6608960628509521, 0.6742424368858337, 0.6796875, 0.6818477511405945, 0.687559187412262, 0.6888612508773804, 0.6897786259651184, 0.6913174986839294, 0.6922052502632141, 0.6934777498245239, 0.6922348737716675, 0.6902817487716675, 0.6940104365348816, 0.6947502493858337, 0.6942471861839294]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 2, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 16:32:50 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 2, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 16:32:50 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 16:32:50 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 16:32:51 I deeptables.m.preprocessor.py 249 - transform_X taken 0.9315192699432373s\n",
      "04-25 16:32:51 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 16:32:51 I deeptables.m.preprocessor.py 236 - transform_y taken 0.006981372833251953s\n",
      "04-25 16:32:52 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 16:32:52 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13563776016235352s\n",
      "04-25 16:32:52 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 16:32:52 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0019958019256591797s\n",
      "04-25 16:32:52 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 16:32:52 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 16:32:52 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:32:52 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:32:52 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 16:32:52 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 16:32:52 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9361 - auc: 0.7190 - accuracy: 0.4511 - val_loss: 1.0192 - val_auc: 0.8070 - val_accuracy: 0.5450\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.4135 - auc: 0.8599 - accuracy: 0.6149 - val_loss: 0.9496 - val_auc: 0.8345 - val_accuracy: 0.5645\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.0792 - auc: 0.9152 - accuracy: 0.7156 - val_loss: 0.9442 - val_auc: 0.8486 - val_accuracy: 0.5906\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.8425 - auc: 0.9445 - accuracy: 0.7704 - val_loss: 0.9712 - val_auc: 0.8543 - val_accuracy: 0.6034\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.7046 - auc: 0.9594 - accuracy: 0.8020 - val_loss: 1.0027 - val_auc: 0.8581 - val_accuracy: 0.6159\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.6102 - auc: 0.9686 - accuracy: 0.8259 - val_loss: 1.0100 - val_auc: 0.8646 - val_accuracy: 0.6307\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5332 - auc: 0.9754 - accuracy: 0.8471 - val_loss: 1.0257 - val_auc: 0.8695 - val_accuracy: 0.6442\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4742 - auc: 0.9803 - accuracy: 0.8649 - val_loss: 1.0475 - val_auc: 0.8713 - val_accuracy: 0.6504\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4248 - auc: 0.9839 - accuracy: 0.8786 - val_loss: 1.0540 - val_auc: 0.8766 - val_accuracy: 0.6612\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3866 - auc: 0.9866 - accuracy: 0.8893 - val_loss: 1.0749 - val_auc: 0.8770 - val_accuracy: 0.6648\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3596 - auc: 0.9884 - accuracy: 0.8961 - val_loss: 1.0986 - val_auc: 0.8779 - val_accuracy: 0.6683\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3350 - auc: 0.9899 - accuracy: 0.9034 - val_loss: 1.1053 - val_auc: 0.8803 - val_accuracy: 0.6737\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3139 - auc: 0.9911 - accuracy: 0.9086 - val_loss: 1.1295 - val_auc: 0.8801 - val_accuracy: 0.6779\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2962 - auc: 0.9920 - accuracy: 0.9128 - val_loss: 1.1449 - val_auc: 0.8811 - val_accuracy: 0.6812\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2829 - auc: 0.9928 - accuracy: 0.9162 - val_loss: 1.1587 - val_auc: 0.8806 - val_accuracy: 0.6791\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2699 - auc: 0.9935 - accuracy: 0.9193 - val_loss: 1.1758 - val_auc: 0.8799 - val_accuracy: 0.6794\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2596 - auc: 0.9939 - accuracy: 0.9220 - val_loss: 1.1943 - val_auc: 0.8808 - val_accuracy: 0.6840\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2504 - auc: 0.9944 - accuracy: 0.9241 - val_loss: 1.2079 - val_auc: 0.8804 - val_accuracy: 0.6823\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2425 - auc: 0.9947 - accuracy: 0.9260 - val_loss: 1.2018 - val_auc: 0.8829 - val_accuracy: 0.6860\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2358 - auc: 0.9950 - accuracy: 0.9278 - val_loss: 1.2242 - val_auc: 0.8821 - val_accuracy: 0.6872\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2290 - auc: 0.9953 - accuracy: 0.9296 - val_loss: 1.2360 - val_auc: 0.8814 - val_accuracy: 0.6884\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2228 - auc: 0.9955 - accuracy: 0.9309 - val_loss: 1.2587 - val_auc: 0.8821 - val_accuracy: 0.6876\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2181 - auc: 0.9957 - accuracy: 0.9316 - val_loss: 1.2710 - val_auc: 0.8806 - val_accuracy: 0.6858\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2141 - auc: 0.9959 - accuracy: 0.9325 - val_loss: 1.2711 - val_auc: 0.8816 - val_accuracy: 0.6877\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "04-25 16:37:40 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 16:37:40 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 16:37:41 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425163250_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9360593557357788, 1.4134693145751953, 1.0791959762573242, 0.8425304293632507, 0.7045806050300598, 0.6102045774459839, 0.533195436000824, 0.4741588234901428, 0.42480194568634033, 0.3866322934627533, 0.35956114530563354, 0.3349746763706207, 0.31388840079307556, 0.29623109102249146, 0.2828912138938904, 0.26992642879486084, 0.2595598101615906, 0.25035256147384644, 0.24252507090568542, 0.2358258217573166, 0.22898361086845398, 0.22280128300189972, 0.2180532068014145, 0.21407151222229004], 'auc': [0.7190119028091431, 0.8599076867103577, 0.9151991009712219, 0.9444704651832581, 0.9594446420669556, 0.9685630202293396, 0.9753796458244324, 0.9802870154380798, 0.9838818311691284, 0.9865533709526062, 0.9883507490158081, 0.9898697733879089, 0.9910982847213745, 0.9920403957366943, 0.9928092956542969, 0.9934505224227905, 0.9939425587654114, 0.9943618774414062, 0.9947370290756226, 0.9949615597724915, 0.995317816734314, 0.9955499172210693, 0.9956917762756348, 0.9958965182304382], 'accuracy': [0.4511331021785736, 0.6149382591247559, 0.7155770659446716, 0.7704020738601685, 0.8020398616790771, 0.8258792757987976, 0.8471344709396362, 0.8649417757987976, 0.8785572052001953, 0.8892782330513, 0.8961247801780701, 0.9033978581428528, 0.908598005771637, 0.9128144979476929, 0.9162377119064331, 0.9193119406700134, 0.9220018982887268, 0.9240713715553284, 0.9259857535362244, 0.927808403968811, 0.9295852780342102, 0.9308932423591614, 0.9316441416740417, 0.9324550032615662], 'val_loss': [1.0191898345947266, 0.9495552182197571, 0.944173276424408, 0.9712004065513611, 1.0027124881744385, 1.0100189447402954, 1.0256755352020264, 1.0475032329559326, 1.0539928674697876, 1.0749372243881226, 1.098602533340454, 1.1053466796875, 1.1295279264450073, 1.1448819637298584, 1.1587241888046265, 1.1757946014404297, 1.1942827701568604, 1.2078791856765747, 1.2018115520477295, 1.2241538763046265, 1.2359651327133179, 1.2586612701416016, 1.2709585428237915, 1.2710639238357544], 'val_auc': [0.8069775700569153, 0.8345373868942261, 0.8485839366912842, 0.8543145060539246, 0.8581206202507019, 0.8645514249801636, 0.8694840669631958, 0.8713211417198181, 0.8765523433685303, 0.8770028948783875, 0.87788325548172, 0.8803049325942993, 0.8800820112228394, 0.881134569644928, 0.8805910348892212, 0.879871666431427, 0.880793571472168, 0.8803572058677673, 0.8829050064086914, 0.8821251392364502, 0.8814147710800171, 0.8820781707763672, 0.8806403279304504, 0.8815626502037048], 'val_accuracy': [0.544981062412262, 0.5645418763160706, 0.5905539989471436, 0.6033676862716675, 0.6158854365348816, 0.6307410001754761, 0.6442353129386902, 0.6503610610961914, 0.6611623764038086, 0.6648319363594055, 0.6682942509651184, 0.6736801862716675, 0.6779119372367859, 0.6811967492103577, 0.6790660619735718, 0.6793619990348816, 0.6840080618858337, 0.6822916865348816, 0.6860499382019043, 0.6872040629386902, 0.6883878111839294, 0.6875887513160706, 0.6858131885528564, 0.6877071261405945]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 16:37:41 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 16:37:41 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 16:37:42 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 16:37:43 I deeptables.m.preprocessor.py 249 - transform_X taken 0.9584591388702393s\n",
      "04-25 16:37:43 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 16:37:43 I deeptables.m.preprocessor.py 236 - transform_y taken 0.007977962493896484s\n",
      "04-25 16:37:43 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 16:37:43 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13463997840881348s\n",
      "04-25 16:37:43 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 16:37:43 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009980201721191406s\n",
      "04-25 16:37:43 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 16:37:43 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 16:37:43 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:37:43 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:37:43 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 16:37:43 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 16:37:43 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9455 - auc: 0.7207 - accuracy: 0.4806 - val_loss: 1.0216 - val_auc: 0.8039 - val_accuracy: 0.5313\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.4152 - auc: 0.8597 - accuracy: 0.6131 - val_loss: 0.9342 - val_auc: 0.8409 - val_accuracy: 0.5753\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.0434 - auc: 0.9200 - accuracy: 0.7284 - val_loss: 0.9339 - val_auc: 0.8554 - val_accuracy: 0.5992\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.8121 - auc: 0.9475 - accuracy: 0.7799 - val_loss: 0.9824 - val_auc: 0.8574 - val_accuracy: 0.6091\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.6963 - auc: 0.9599 - accuracy: 0.8039 - val_loss: 1.0178 - val_auc: 0.8604 - val_accuracy: 0.6214\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.6076 - auc: 0.9686 - accuracy: 0.8263 - val_loss: 1.0353 - val_auc: 0.8661 - val_accuracy: 0.6351\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5411 - auc: 0.9748 - accuracy: 0.8462 - val_loss: 1.0628 - val_auc: 0.8679 - val_accuracy: 0.6424\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4876 - auc: 0.9791 - accuracy: 0.8603 - val_loss: 1.0851 - val_auc: 0.8704 - val_accuracy: 0.6562\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4478 - auc: 0.9820 - accuracy: 0.8718 - val_loss: 1.0965 - val_auc: 0.8725 - val_accuracy: 0.6626\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4184 - auc: 0.9843 - accuracy: 0.8804 - val_loss: 1.1181 - val_auc: 0.8722 - val_accuracy: 0.6673\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3867 - auc: 0.9865 - accuracy: 0.8887 - val_loss: 1.1416 - val_auc: 0.8734 - val_accuracy: 0.6672\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3635 - auc: 0.9880 - accuracy: 0.8946 - val_loss: 1.1483 - val_auc: 0.8758 - val_accuracy: 0.6764\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3407 - auc: 0.9895 - accuracy: 0.9020 - val_loss: 1.1678 - val_auc: 0.8760 - val_accuracy: 0.6737\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3220 - auc: 0.9906 - accuracy: 0.9059 - val_loss: 1.1836 - val_auc: 0.8776 - val_accuracy: 0.6833\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3060 - auc: 0.9915 - accuracy: 0.9103 - val_loss: 1.2020 - val_auc: 0.8770 - val_accuracy: 0.6792\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2911 - auc: 0.9924 - accuracy: 0.9144 - val_loss: 1.2096 - val_auc: 0.8786 - val_accuracy: 0.6882\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2788 - auc: 0.9930 - accuracy: 0.9169 - val_loss: 1.2223 - val_auc: 0.8792 - val_accuracy: 0.6900\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2677 - auc: 0.9936 - accuracy: 0.9200 - val_loss: 1.2620 - val_auc: 0.8768 - val_accuracy: 0.6828\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2577 - auc: 0.9940 - accuracy: 0.9224 - val_loss: 1.2540 - val_auc: 0.8790 - val_accuracy: 0.6919\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2481 - auc: 0.9945 - accuracy: 0.9246 - val_loss: 1.2765 - val_auc: 0.8792 - val_accuracy: 0.6916\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2402 - auc: 0.9948 - accuracy: 0.9271 - val_loss: 1.2824 - val_auc: 0.8799 - val_accuracy: 0.6879\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2350 - auc: 0.9950 - accuracy: 0.9278 - val_loss: 1.2901 - val_auc: 0.8799 - val_accuracy: 0.6882\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2278 - auc: 0.9954 - accuracy: 0.9297 - val_loss: 1.3071 - val_auc: 0.8803 - val_accuracy: 0.6959\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2222 - auc: 0.9956 - accuracy: 0.9308 - val_loss: 1.3112 - val_auc: 0.8800 - val_accuracy: 0.6956\n",
      "Epoch 25/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2161 - auc: 0.9958 - accuracy: 0.9322 - val_loss: 1.3345 - val_auc: 0.8799 - val_accuracy: 0.6951\n",
      "Epoch 26/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2130 - auc: 0.9959 - accuracy: 0.9324 - val_loss: 1.3411 - val_auc: 0.8805 - val_accuracy: 0.6959\n",
      "Epoch 27/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2082 - auc: 0.9961 - accuracy: 0.9338 - val_loss: 1.3596 - val_auc: 0.8802 - val_accuracy: 0.6965\n",
      "Epoch 28/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2065 - auc: 0.9962 - accuracy: 0.9339 - val_loss: 1.3430 - val_auc: 0.8822 - val_accuracy: 0.6982\n",
      "Epoch 29/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2029 - auc: 0.9963 - accuracy: 0.9349 - val_loss: 1.3674 - val_auc: 0.8808 - val_accuracy: 0.6975\n",
      "Epoch 30/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1994 - auc: 0.9964 - accuracy: 0.9360 - val_loss: 1.3686 - val_auc: 0.8813 - val_accuracy: 0.6984\n",
      "Epoch 31/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1967 - auc: 0.9965 - accuracy: 0.9367 - val_loss: 1.3915 - val_auc: 0.8800 - val_accuracy: 0.6945\n",
      "Epoch 32/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1933 - auc: 0.9966 - accuracy: 0.9370 - val_loss: 1.4004 - val_auc: 0.8803 - val_accuracy: 0.6963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1934 - auc: 0.9966 - accuracy: 0.9373 - val_loss: 1.3990 - val_auc: 0.8808 - val_accuracy: 0.6977\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00033: early stopping\n",
      "04-25 16:44:16 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 16:44:16 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 16:44:16 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425163741_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.945472240447998, 1.4152278900146484, 1.0433560609817505, 0.8121213912963867, 0.6962800025939941, 0.6076311469078064, 0.5410841703414917, 0.4875892102718353, 0.4478030204772949, 0.41838717460632324, 0.38670945167541504, 0.363466739654541, 0.34073859453201294, 0.3219829797744751, 0.3060454726219177, 0.2910665273666382, 0.2787904441356659, 0.26769524812698364, 0.2577117085456848, 0.24814651906490326, 0.24019573628902435, 0.23500895500183105, 0.2277771532535553, 0.2221905142068863, 0.2160601019859314, 0.21296682953834534, 0.2082444727420807, 0.20649512112140656, 0.2028852254152298, 0.19936902821063995, 0.19668394327163696, 0.19327150285243988, 0.19336125254631042], 'auc': [0.7206534743309021, 0.8597438931465149, 0.9199650883674622, 0.9474545121192932, 0.9598698019981384, 0.9686458110809326, 0.9747798442840576, 0.9790794253349304, 0.9820492267608643, 0.9843403100967407, 0.9865158200263977, 0.9880266785621643, 0.9894617795944214, 0.9906166195869446, 0.9915347099304199, 0.9923538565635681, 0.9930073022842407, 0.993559718132019, 0.9940170645713806, 0.994498610496521, 0.9948405623435974, 0.9950273036956787, 0.9953804612159729, 0.9955736398696899, 0.9958179593086243, 0.9959224462509155, 0.9961211085319519, 0.99619060754776, 0.9963204860687256, 0.9964189529418945, 0.9965251684188843, 0.9966092109680176, 0.996627151966095], 'accuracy': [0.48059213161468506, 0.6130661964416504, 0.7283534407615662, 0.7799455523490906, 0.8039154410362244, 0.8262529373168945, 0.8462249040603638, 0.860266923904419, 0.8717600703239441, 0.8803587555885315, 0.8887388706207275, 0.8946405649185181, 0.9019629955291748, 0.9059327244758606, 0.9103219509124756, 0.9143586158752441, 0.9168546795845032, 0.9199994206428528, 0.9223614931106567, 0.9245614409446716, 0.9270715713500977, 0.9277766942977905, 0.9297263026237488, 0.9308192133903503, 0.9321659207344055, 0.9324021339416504, 0.9338334798812866, 0.9339145421981812, 0.9349263906478882, 0.936029851436615, 0.9366538524627686, 0.9370099306106567, 0.9372532367706299], 'val_loss': [1.021613359451294, 0.9342242479324341, 0.9338956475257874, 0.9824084639549255, 1.0177863836288452, 1.0353310108184814, 1.0628414154052734, 1.085133671760559, 1.0965018272399902, 1.1180988550186157, 1.1415860652923584, 1.1483008861541748, 1.1677905321121216, 1.1835938692092896, 1.2019836902618408, 1.2096080780029297, 1.2222650051116943, 1.261980652809143, 1.2540425062179565, 1.2764551639556885, 1.2824078798294067, 1.290075421333313, 1.3071422576904297, 1.3112406730651855, 1.3344764709472656, 1.341073989868164, 1.3595739603042603, 1.3429961204528809, 1.3674044609069824, 1.3685667514801025, 1.3915226459503174, 1.4004223346710205, 1.3990192413330078], 'val_auc': [0.8038526177406311, 0.8409422039985657, 0.8554274439811707, 0.8574415445327759, 0.8603879809379578, 0.8660731911659241, 0.8678725957870483, 0.8704445362091064, 0.8724541664123535, 0.8722484111785889, 0.8733945488929749, 0.8757702112197876, 0.8760137557983398, 0.8776305913925171, 0.8770492076873779, 0.8785691857337952, 0.8791505098342896, 0.8768331408500671, 0.8790024518966675, 0.8792144060134888, 0.8799444437026978, 0.8799343109130859, 0.8803484439849854, 0.8800232410430908, 0.879856526851654, 0.8804793357849121, 0.8801583647727966, 0.8821774125099182, 0.8808374404907227, 0.8812856078147888, 0.8799529075622559, 0.8802900910377502, 0.8807584047317505], 'val_accuracy': [0.531309187412262, 0.5753432512283325, 0.5992246866226196, 0.6090790629386902, 0.6213896870613098, 0.6350615620613098, 0.6423709988594055, 0.656190812587738, 0.6625828742980957, 0.6672881245613098, 0.6671993136405945, 0.6763731241226196, 0.6737097501754761, 0.6832978129386902, 0.6791548132896423, 0.6882102489471436, 0.6899858117103577, 0.6828243136405945, 0.6919389367103577, 0.6915838122367859, 0.6879143118858337, 0.6882398128509521, 0.6958747506141663, 0.6956380009651184, 0.6951349377632141, 0.6959043741226196, 0.6964961886405945, 0.698183000087738, 0.6974727511405945, 0.6983605623245239, 0.6945134997367859, 0.6963186264038086, 0.6976503133773804]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 2, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 16:44:17 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 2, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 16:44:17 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 16:44:17 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 16:44:18 I deeptables.m.preprocessor.py 249 - transform_X taken 0.9440135955810547s\n",
      "04-25 16:44:18 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 16:44:18 I deeptables.m.preprocessor.py 236 - transform_y taken 0.007485628128051758s\n",
      "04-25 16:44:18 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 16:44:18 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13514494895935059s\n",
      "04-25 16:44:18 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 16:44:18 I deeptables.m.preprocessor.py 236 - transform_y taken 0.001995563507080078s\n",
      "04-25 16:44:18 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 16:44:18 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 16:44:18 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:44:19 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:44:19 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 16:44:19 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 16:44:19 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9366 - auc: 0.7227 - accuracy: 0.4848 - val_loss: 1.0250 - val_auc: 0.8038 - val_accuracy: 0.5560\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.3919 - auc: 0.8655 - accuracy: 0.6235 - val_loss: 0.9318 - val_auc: 0.8406 - val_accuracy: 0.5779\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.9743 - auc: 0.9302 - accuracy: 0.7480 - val_loss: 0.8968 - val_auc: 0.8577 - val_accuracy: 0.6029\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.7106 - auc: 0.9593 - accuracy: 0.8075 - val_loss: 0.8826 - val_auc: 0.8704 - val_accuracy: 0.6291\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5668 - auc: 0.9727 - accuracy: 0.8417 - val_loss: 0.8770 - val_auc: 0.8792 - val_accuracy: 0.6495\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4755 - auc: 0.9803 - accuracy: 0.8670 - val_loss: 0.8630 - val_auc: 0.8868 - val_accuracy: 0.6619\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4110 - auc: 0.9851 - accuracy: 0.8842 - val_loss: 0.8740 - val_auc: 0.8903 - val_accuracy: 0.6717\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3654 - auc: 0.9881 - accuracy: 0.8965 - val_loss: 0.8786 - val_auc: 0.8949 - val_accuracy: 0.6786\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3251 - auc: 0.9906 - accuracy: 0.9067 - val_loss: 0.9038 - val_auc: 0.8951 - val_accuracy: 0.6820\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2970 - auc: 0.9920 - accuracy: 0.9138 - val_loss: 0.8981 - val_auc: 0.8985 - val_accuracy: 0.6890\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2741 - auc: 0.9932 - accuracy: 0.9199 - val_loss: 0.9212 - val_auc: 0.8996 - val_accuracy: 0.6906\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2581 - auc: 0.9940 - accuracy: 0.9235 - val_loss: 0.9181 - val_auc: 0.9003 - val_accuracy: 0.6920\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2446 - auc: 0.9946 - accuracy: 0.9264 - val_loss: 0.9269 - val_auc: 0.9013 - val_accuracy: 0.6937\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2345 - auc: 0.9950 - accuracy: 0.9286 - val_loss: 0.9377 - val_auc: 0.9015 - val_accuracy: 0.7064\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2257 - auc: 0.9954 - accuracy: 0.9305 - val_loss: 0.9536 - val_auc: 0.9017 - val_accuracy: 0.6932\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2187 - auc: 0.9956 - accuracy: 0.9318 - val_loss: 0.9776 - val_auc: 0.9009 - val_accuracy: 0.7009\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2111 - auc: 0.9959 - accuracy: 0.9335 - val_loss: 0.9736 - val_auc: 0.9017 - val_accuracy: 0.6949\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2085 - auc: 0.9961 - accuracy: 0.9341 - val_loss: 0.9776 - val_auc: 0.9015 - val_accuracy: 0.6952\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2040 - auc: 0.9963 - accuracy: 0.9351 - val_loss: 0.9914 - val_auc: 0.9006 - val_accuracy: 0.6924\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2013 - auc: 0.9964 - accuracy: 0.9357 - val_loss: 1.0040 - val_auc: 0.8995 - val_accuracy: 0.6931\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1966 - auc: 0.9965 - accuracy: 0.9366 - val_loss: 1.0029 - val_auc: 0.9013 - val_accuracy: 0.6966\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1935 - auc: 0.9966 - accuracy: 0.9370 - val_loss: 0.9912 - val_auc: 0.9012 - val_accuracy: 0.7006\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "04-25 16:48:40 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 16:48:40 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 16:48:41 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425164417_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9366241693496704, 1.3918522596359253, 0.974348247051239, 0.7105551362037659, 0.5668297410011292, 0.4754641354084015, 0.4109655022621155, 0.3653758764266968, 0.3251417279243469, 0.2969522774219513, 0.2740679085254669, 0.2581160068511963, 0.24456971883773804, 0.2345367670059204, 0.22566433250904083, 0.21867069602012634, 0.21105162799358368, 0.20849239826202393, 0.2039872705936432, 0.20131783187389374, 0.1966216117143631, 0.19347447156906128], 'auc': [0.7227192521095276, 0.8654539585113525, 0.9301767349243164, 0.959263801574707, 0.9726659655570984, 0.9802731871604919, 0.9851140975952148, 0.988140344619751, 0.9905557036399841, 0.992030680179596, 0.9932406544685364, 0.9940069913864136, 0.9946097731590271, 0.9950351119041443, 0.9953708052635193, 0.9956361055374146, 0.9959482550621033, 0.9960920214653015, 0.9962731003761292, 0.9963633418083191, 0.996461033821106, 0.9965739250183105], 'accuracy': [0.48480862379074097, 0.6235263347625732, 0.7480257153511047, 0.8075078725814819, 0.8416770100593567, 0.8670041561126709, 0.8842262029647827, 0.896484375, 0.9066977500915527, 0.9137945771217346, 0.9198583960533142, 0.9235495924949646, 0.9264264106750488, 0.9285945892333984, 0.9304560422897339, 0.9318028092384338, 0.933544397354126, 0.934052050113678, 0.935137927532196, 0.9356984496116638, 0.9365551471710205, 0.937041699886322], 'val_loss': [1.0250064134597778, 0.931837260723114, 0.896762490272522, 0.8826022744178772, 0.8769721984863281, 0.8630015254020691, 0.8739769458770752, 0.8785619735717773, 0.9037631154060364, 0.8980879783630371, 0.9212117791175842, 0.9180964827537537, 0.9269468188285828, 0.9377174377441406, 0.9535722732543945, 0.977620005607605, 0.9736025333404541, 0.9776118993759155, 0.9914132356643677, 1.0039746761322021, 1.0028984546661377, 0.991221010684967], 'val_auc': [0.8037624359130859, 0.8405647873878479, 0.8577069640159607, 0.8704254627227783, 0.8791841864585876, 0.8867560625076294, 0.8903281092643738, 0.8948963284492493, 0.8951111435890198, 0.8985169529914856, 0.8995512127876282, 0.9002798795700073, 0.90128093957901, 0.9015390276908875, 0.9017255306243896, 0.900904655456543, 0.9017285704612732, 0.9014932513237, 0.9006465673446655, 0.8994979858398438, 0.9012820720672607, 0.9011696577072144], 'val_accuracy': [0.5559895634651184, 0.5778586864471436, 0.6028645634651184, 0.6291133761405945, 0.6495028138160706, 0.6619022488594055, 0.6717270612716675, 0.6785629987716675, 0.6819661259651184, 0.6889500617980957, 0.6906368136405945, 0.6920276880264282, 0.6936553120613098, 0.7064393758773804, 0.6931818127632141, 0.7008759379386902, 0.6948686242103577, 0.6952237486839294, 0.6923532485961914, 0.6930634379386902, 0.6965849995613098, 0.7006096243858337]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 6, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 16:48:41 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 6, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 16:48:41 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 16:48:42 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-25 16:48:42 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 16:48:42 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.38947463035583496s\n",
      "04-25 16:48:42 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-25 16:48:42 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5410583019256592s\n",
      "04-25 16:48:42 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 16:48:43 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.2882418632507324s\n",
      "04-25 16:48:43 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.4441723823547363s\n",
      "04-25 16:48:43 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 16:48:43 I deeptables.m.preprocessor.py 249 - transform_X taken 0.1301565170288086s\n",
      "04-25 16:48:43 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 16:48:43 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009975433349609375s\n",
      "04-25 16:48:43 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 16:48:43 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 16:48:43 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:48:43 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:48:43 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 16:48:44 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 16:48:44 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9423 - auc: 0.7198 - accuracy: 0.4753 - val_loss: 1.0298 - val_auc: 0.7998 - val_accuracy: 0.5212\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.4267 - auc: 0.8584 - accuracy: 0.6111 - val_loss: 0.9420 - val_auc: 0.8385 - val_accuracy: 0.5742\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.0771 - auc: 0.9159 - accuracy: 0.7186 - val_loss: 0.9481 - val_auc: 0.8522 - val_accuracy: 0.5995\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.8313 - auc: 0.9459 - accuracy: 0.7760 - val_loss: 1.0166 - val_auc: 0.8519 - val_accuracy: 0.6088\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.7049 - auc: 0.9587 - accuracy: 0.8002 - val_loss: 1.0143 - val_auc: 0.8618 - val_accuracy: 0.6252\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.6147 - auc: 0.9676 - accuracy: 0.8222 - val_loss: 1.0354 - val_auc: 0.8658 - val_accuracy: 0.6378\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5411 - auc: 0.9744 - accuracy: 0.8436 - val_loss: 1.0596 - val_auc: 0.8692 - val_accuracy: 0.6469\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4824 - auc: 0.9793 - accuracy: 0.8610 - val_loss: 1.0697 - val_auc: 0.8727 - val_accuracy: 0.6545\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4392 - auc: 0.9828 - accuracy: 0.8737 - val_loss: 1.0862 - val_auc: 0.8754 - val_accuracy: 0.6635\n",
      "Epoch 10/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4025 - auc: 0.9854 - accuracy: 0.8843 - val_loss: 1.1130 - val_auc: 0.8758 - val_accuracy: 0.6675\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3733 - auc: 0.9874 - accuracy: 0.8916 - val_loss: 1.1370 - val_auc: 0.8765 - val_accuracy: 0.6705\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3498 - auc: 0.9889 - accuracy: 0.8989 - val_loss: 1.1518 - val_auc: 0.8780 - val_accuracy: 0.6774\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3287 - auc: 0.9902 - accuracy: 0.9046 - val_loss: 1.1639 - val_auc: 0.8786 - val_accuracy: 0.6789\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3093 - auc: 0.9914 - accuracy: 0.9096 - val_loss: 1.1921 - val_auc: 0.8784 - val_accuracy: 0.6816\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2958 - auc: 0.9921 - accuracy: 0.9124 - val_loss: 1.2023 - val_auc: 0.8797 - val_accuracy: 0.6847\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2830 - auc: 0.9928 - accuracy: 0.9159 - val_loss: 1.2106 - val_auc: 0.8806 - val_accuracy: 0.6844\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2697 - auc: 0.9934 - accuracy: 0.9193 - val_loss: 1.2308 - val_auc: 0.8802 - val_accuracy: 0.6867\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2591 - auc: 0.9939 - accuracy: 0.9221 - val_loss: 1.2426 - val_auc: 0.8808 - val_accuracy: 0.6892\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2501 - auc: 0.9944 - accuracy: 0.9242 - val_loss: 1.2574 - val_auc: 0.8807 - val_accuracy: 0.6882\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2427 - auc: 0.9947 - accuracy: 0.9259 - val_loss: 1.2756 - val_auc: 0.8810 - val_accuracy: 0.6955\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2344 - auc: 0.9950 - accuracy: 0.9278 - val_loss: 1.2843 - val_auc: 0.8809 - val_accuracy: 0.6904\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2276 - auc: 0.9953 - accuracy: 0.9297 - val_loss: 1.3016 - val_auc: 0.8816 - val_accuracy: 0.6928\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2232 - auc: 0.9955 - accuracy: 0.9306 - val_loss: 1.3128 - val_auc: 0.8812 - val_accuracy: 0.6920\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2181 - auc: 0.9957 - accuracy: 0.9317 - val_loss: 1.3265 - val_auc: 0.8814 - val_accuracy: 0.6921\n",
      "Epoch 25/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2142 - auc: 0.9959 - accuracy: 0.9328 - val_loss: 1.3357 - val_auc: 0.8808 - val_accuracy: 0.6926\n",
      "Epoch 26/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2094 - auc: 0.9961 - accuracy: 0.9337 - val_loss: 1.3302 - val_auc: 0.8825 - val_accuracy: 0.6945\n",
      "Epoch 27/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2059 - auc: 0.9962 - accuracy: 0.9343 - val_loss: 1.3756 - val_auc: 0.8802 - val_accuracy: 0.6913\n",
      "Epoch 28/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2037 - auc: 0.9963 - accuracy: 0.9349 - val_loss: 1.3715 - val_auc: 0.8814 - val_accuracy: 0.6952\n",
      "Epoch 29/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1999 - auc: 0.9964 - accuracy: 0.9355 - val_loss: 1.3710 - val_auc: 0.8821 - val_accuracy: 0.6943\n",
      "Epoch 30/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1960 - auc: 0.9965 - accuracy: 0.9365 - val_loss: 1.3836 - val_auc: 0.8814 - val_accuracy: 0.6939\n",
      "Epoch 31/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1942 - auc: 0.9966 - accuracy: 0.9370 - val_loss: 1.3811 - val_auc: 0.8823 - val_accuracy: 0.6928\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00031: early stopping\n",
      "04-25 16:54:52 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 16:54:52 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 16:54:52 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425164841_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9423383474349976, 1.4267240762710571, 1.0770776271820068, 0.8312780857086182, 0.7049486637115479, 0.6147489547729492, 0.5411200523376465, 0.48239952325820923, 0.43920397758483887, 0.4024837613105774, 0.37332624197006226, 0.34979790449142456, 0.3286808729171753, 0.3093411922454834, 0.2957703471183777, 0.2830268144607544, 0.26969122886657715, 0.25907474756240845, 0.2501221299171448, 0.24265672266483307, 0.23437833786010742, 0.2276175171136856, 0.22324047982692719, 0.21810337901115417, 0.21424612402915955, 0.20942555367946625, 0.20586644113063812, 0.2036803960800171, 0.19989317655563354, 0.19604535400867462, 0.19422686100006104], 'auc': [0.719795286655426, 0.8583810925483704, 0.915885865688324, 0.9459383487701416, 0.958721935749054, 0.9676156044006348, 0.9743730425834656, 0.9792775511741638, 0.9828070402145386, 0.9854187965393066, 0.9874069690704346, 0.9889320135116577, 0.9901991486549377, 0.9913728833198547, 0.9920598268508911, 0.9927545785903931, 0.9934245347976685, 0.9939338564872742, 0.9943520426750183, 0.9947027564048767, 0.9950306415557861, 0.9953383803367615, 0.9955137372016907, 0.9957219958305359, 0.9958711862564087, 0.9960816502571106, 0.9961645007133484, 0.9962990283966064, 0.9964035153388977, 0.9965443015098572, 0.9966267943382263], 'accuracy': [0.47525453567504883, 0.6110531091690063, 0.7186089754104614, 0.775958240032196, 0.8002418279647827, 0.8221527934074402, 0.8435842990875244, 0.8610284328460693, 0.8737484216690063, 0.884272038936615, 0.8915804028511047, 0.8989381194114685, 0.9045894742012024, 0.9095921516418457, 0.9123984575271606, 0.9159345626831055, 0.9192625880241394, 0.9220970869064331, 0.9242088794708252, 0.9259434342384338, 0.9278365969657898, 0.9297403693199158, 0.930565357208252, 0.9316688179969788, 0.9328392744064331, 0.9337241649627686, 0.9343482255935669, 0.9348805546760559, 0.9355257153511047, 0.9365199208259583, 0.9369605779647827], 'val_loss': [1.029762864112854, 0.942049503326416, 0.948107898235321, 1.0165760517120361, 1.0142810344696045, 1.035390853881836, 1.0595641136169434, 1.0697144269943237, 1.0861661434173584, 1.11296808719635, 1.1369656324386597, 1.1518383026123047, 1.163909912109375, 1.1920632123947144, 1.2023169994354248, 1.2105610370635986, 1.2307738065719604, 1.2425905466079712, 1.257424235343933, 1.275601863861084, 1.2842730283737183, 1.301580548286438, 1.3127994537353516, 1.3265173435211182, 1.3357059955596924, 1.3301751613616943, 1.375602126121521, 1.3714957237243652, 1.3710286617279053, 1.383587121963501, 1.3811439275741577], 'val_auc': [0.7998390197753906, 0.8384672999382019, 0.8522403836250305, 0.8519469499588013, 0.8617632985115051, 0.8657528758049011, 0.8692221641540527, 0.8726518750190735, 0.8754286766052246, 0.8757880330085754, 0.8764773607254028, 0.8779579997062683, 0.878551185131073, 0.8783814311027527, 0.8796854615211487, 0.8806138038635254, 0.8802235722541809, 0.880793571472168, 0.8806530237197876, 0.880962610244751, 0.8808991312980652, 0.8816195130348206, 0.8811740279197693, 0.8813706040382385, 0.880752682685852, 0.8824722170829773, 0.8801978826522827, 0.8814297914505005, 0.8820630311965942, 0.8814446926116943, 0.8822560906410217], 'val_accuracy': [0.5212180614471436, 0.5742483139038086, 0.5995206236839294, 0.6088127493858337, 0.6252071261405945, 0.6378136873245239, 0.6469282507896423, 0.6544744372367859, 0.6634706258773804, 0.6674952507019043, 0.6705137491226196, 0.6774384379386902, 0.6789180636405945, 0.6816110610961914, 0.6847182512283325, 0.6844223737716675, 0.6867009997367859, 0.6892163753509521, 0.6882398128509521, 0.6954604387283325, 0.6904001235961914, 0.6928266882896423, 0.6919981241226196, 0.6921164989471436, 0.6925603747367859, 0.6944838762283325, 0.6912878751754761, 0.6951941251754761, 0.6943359375, 0.6939216256141663, 0.6927675008773804]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 5, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 16:54:52 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 5, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 16:54:52 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 16:54:53 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-25 16:54:53 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 16:54:53 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.38947439193725586s\n",
      "04-25 16:54:53 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-25 16:54:54 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5445539951324463s\n",
      "04-25 16:54:54 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 16:54:54 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.28025078773498535s\n",
      "04-25 16:54:54 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.4386789798736572s\n",
      "04-25 16:54:54 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 16:54:54 I deeptables.m.preprocessor.py 249 - transform_X taken 0.12765884399414062s\n",
      "04-25 16:54:54 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 16:54:54 I deeptables.m.preprocessor.py 236 - transform_y taken 0.000997781753540039s\n",
      "04-25 16:54:54 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 16:54:54 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 16:54:54 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:54:54 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 16:54:54 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 16:54:55 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 16:54:55 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9477 - auc: 0.7161 - accuracy: 0.4451 - val_loss: 1.0335 - val_auc: 0.7982 - val_accuracy: 0.5231\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.4361 - auc: 0.8548 - accuracy: 0.6047 - val_loss: 0.9421 - val_auc: 0.8390 - val_accuracy: 0.5847\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.0939 - auc: 0.9132 - accuracy: 0.7136 - val_loss: 0.9464 - val_auc: 0.8511 - val_accuracy: 0.5934\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.8305 - auc: 0.9461 - accuracy: 0.7764 - val_loss: 0.9836 - val_auc: 0.8580 - val_accuracy: 0.6122\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.6750 - auc: 0.9623 - accuracy: 0.8112 - val_loss: 1.0166 - val_auc: 0.8641 - val_accuracy: 0.6410\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5843 - auc: 0.9708 - accuracy: 0.8335 - val_loss: 1.0399 - val_auc: 0.8680 - val_accuracy: 0.6521\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5224 - auc: 0.9760 - accuracy: 0.8499 - val_loss: 1.0802 - val_auc: 0.8688 - val_accuracy: 0.6584\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4777 - auc: 0.9797 - accuracy: 0.8627 - val_loss: 1.0873 - val_auc: 0.8714 - val_accuracy: 0.6642\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4380 - auc: 0.9828 - accuracy: 0.8736 - val_loss: 1.1150 - val_auc: 0.8716 - val_accuracy: 0.6701\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4050 - auc: 0.9851 - accuracy: 0.8833 - val_loss: 1.1260 - val_auc: 0.8740 - val_accuracy: 0.6764\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3751 - auc: 0.9872 - accuracy: 0.8913 - val_loss: 1.1483 - val_auc: 0.8751 - val_accuracy: 0.6823\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3521 - auc: 0.9888 - accuracy: 0.8987 - val_loss: 1.1553 - val_auc: 0.8776 - val_accuracy: 0.6864\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3283 - auc: 0.9902 - accuracy: 0.9050 - val_loss: 1.1758 - val_auc: 0.8776 - val_accuracy: 0.6882\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3126 - auc: 0.9912 - accuracy: 0.9094 - val_loss: 1.1912 - val_auc: 0.8787 - val_accuracy: 0.6902\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2944 - auc: 0.9922 - accuracy: 0.9135 - val_loss: 1.2120 - val_auc: 0.8784 - val_accuracy: 0.6934\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2841 - auc: 0.9927 - accuracy: 0.9163 - val_loss: 1.2325 - val_auc: 0.8782 - val_accuracy: 0.6937\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2696 - auc: 0.9935 - accuracy: 0.9196 - val_loss: 1.2410 - val_auc: 0.8795 - val_accuracy: 0.6966\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2598 - auc: 0.9940 - accuracy: 0.9220 - val_loss: 1.2631 - val_auc: 0.8791 - val_accuracy: 0.6974\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2511 - auc: 0.9944 - accuracy: 0.9240 - val_loss: 1.2791 - val_auc: 0.8792 - val_accuracy: 0.7001\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2427 - auc: 0.9947 - accuracy: 0.9261 - val_loss: 1.2972 - val_auc: 0.8790 - val_accuracy: 0.6992\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2373 - auc: 0.9950 - accuracy: 0.9276 - val_loss: 1.3120 - val_auc: 0.8792 - val_accuracy: 0.7010\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2315 - auc: 0.9952 - accuracy: 0.9288 - val_loss: 1.3160 - val_auc: 0.8795 - val_accuracy: 0.7002\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2238 - auc: 0.9955 - accuracy: 0.9307 - val_loss: 1.3219 - val_auc: 0.8806 - val_accuracy: 0.6878\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2188 - auc: 0.9957 - accuracy: 0.9316 - val_loss: 1.3451 - val_auc: 0.8793 - val_accuracy: 0.7006\n",
      "Epoch 25/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2168 - auc: 0.9958 - accuracy: 0.9323 - val_loss: 1.3513 - val_auc: 0.8796 - val_accuracy: 0.6892\n",
      "Epoch 26/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2108 - auc: 0.9960 - accuracy: 0.9331 - val_loss: 1.3582 - val_auc: 0.8801 - val_accuracy: 0.7032\n",
      "Epoch 27/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2077 - auc: 0.9961 - accuracy: 0.9340 - val_loss: 1.3489 - val_auc: 0.8816 - val_accuracy: 0.7052\n",
      "Epoch 28/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2048 - auc: 0.9962 - accuracy: 0.9351 - val_loss: 1.3642 - val_auc: 0.8806 - val_accuracy: 0.6891\n",
      "Epoch 29/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2023 - auc: 0.9963 - accuracy: 0.9353 - val_loss: 1.3747 - val_auc: 0.8803 - val_accuracy: 0.7036\n",
      "Epoch 30/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1991 - auc: 0.9964 - accuracy: 0.9358 - val_loss: 1.3884 - val_auc: 0.8795 - val_accuracy: 0.6898\n",
      "Epoch 31/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1967 - auc: 0.9965 - accuracy: 0.9365 - val_loss: 1.3927 - val_auc: 0.8805 - val_accuracy: 0.7040\n",
      "Epoch 32/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1935 - auc: 0.9966 - accuracy: 0.9370 - val_loss: 1.4052 - val_auc: 0.8804 - val_accuracy: 0.7038\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00032: early stopping\n",
      "04-25 17:01:14 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 17:01:14 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 17:01:14 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425165452_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9476763010025024, 1.4361366033554077, 1.093949556350708, 0.8304980397224426, 0.6749874353408813, 0.5843291878700256, 0.5223841667175293, 0.4777385890483856, 0.43799248337745667, 0.4049866795539856, 0.37508806586265564, 0.3520849645137787, 0.328319251537323, 0.3125820457935333, 0.29442521929740906, 0.2840718626976013, 0.2696347236633301, 0.25980544090270996, 0.2510583996772766, 0.2426695078611374, 0.23731796443462372, 0.231479212641716, 0.22377072274684906, 0.21881407499313354, 0.21681825816631317, 0.21082940697669983, 0.20765821635723114, 0.204757958650589, 0.20227324962615967, 0.19906435906887054, 0.1966504156589508, 0.1934967041015625], 'auc': [0.7161130309104919, 0.8548207879066467, 0.9131789207458496, 0.9461108446121216, 0.9623324871063232, 0.9707517623901367, 0.9760482311248779, 0.9796733856201172, 0.9827653169631958, 0.9851284027099609, 0.9872276186943054, 0.9887723326683044, 0.990203320980072, 0.9911767840385437, 0.9922197461128235, 0.9927424192428589, 0.9934816360473633, 0.9939559102058411, 0.9943601489067078, 0.9947320818901062, 0.9950061440467834, 0.9952250123023987, 0.995486855506897, 0.9957284927368164, 0.9957814812660217, 0.9960383176803589, 0.9961491823196411, 0.9962422251701355, 0.9963423609733582, 0.9964296817779541, 0.9965208768844604, 0.996632993221283], 'accuracy': [0.44514328241348267, 0.6047213673591614, 0.7136415839195251, 0.7763566374778748, 0.8111955523490906, 0.833466112613678, 0.8498561382293701, 0.8626819252967834, 0.8735968470573425, 0.8832778930664062, 0.8912807703018188, 0.8986737132072449, 0.9050055146217346, 0.9094440937042236, 0.9134984016418457, 0.9163082242012024, 0.9196468591690063, 0.9219843149185181, 0.9240396618843079, 0.9261373281478882, 0.927635669708252, 0.9288308024406433, 0.9306746125221252, 0.9316124320030212, 0.9322646260261536, 0.9331283569335938, 0.9340450167655945, 0.935088574886322, 0.9352577924728394, 0.9358112812042236, 0.9364599585533142, 0.936967670917511], 'val_loss': [1.033549189567566, 0.9421488642692566, 0.9463995099067688, 0.9836424589157104, 1.0166481733322144, 1.0399036407470703, 1.080187201499939, 1.0872712135314941, 1.114990472793579, 1.1259746551513672, 1.1483157873153687, 1.1552894115447998, 1.1757564544677734, 1.1911576986312866, 1.211992621421814, 1.232463002204895, 1.2410045862197876, 1.2630618810653687, 1.279114842414856, 1.2972136735916138, 1.3119792938232422, 1.3160171508789062, 1.3219326734542847, 1.3450794219970703, 1.3513054847717285, 1.3582311868667603, 1.348888635635376, 1.3642398118972778, 1.3747118711471558, 1.3883624076843262, 1.3927361965179443, 1.4051673412322998], 'val_auc': [0.7981928586959839, 0.8389804363250732, 0.8510545492172241, 0.8580082654953003, 0.8640859127044678, 0.8679755330085754, 0.8688457012176514, 0.8713530898094177, 0.8716321587562561, 0.8739821314811707, 0.8751369714736938, 0.8776468634605408, 0.8776052594184875, 0.878743588924408, 0.8784044981002808, 0.8782116174697876, 0.8794528245925903, 0.8791258335113525, 0.8791889548301697, 0.878993034362793, 0.8791564702987671, 0.879497766494751, 0.8806119561195374, 0.8793113827705383, 0.8795749545097351, 0.8801174163818359, 0.8816288709640503, 0.8806134462356567, 0.8803150653839111, 0.8795005679130554, 0.8805290460586548, 0.8803819417953491], 'val_accuracy': [0.5231119990348816, 0.5846650004386902, 0.5934244990348816, 0.612245500087738, 0.6410393118858337, 0.6520774364471436, 0.6584398746490479, 0.6642104387283325, 0.6700698137283325, 0.6763731241226196, 0.6823212504386902, 0.6863754987716675, 0.6882398128509521, 0.6901929378509521, 0.693359375, 0.6936848759651184, 0.6965849995613098, 0.6974135637283325, 0.7000769376754761, 0.6991891860961914, 0.7010239362716675, 0.7001657485961914, 0.6877663135528564, 0.7005800008773804, 0.6892459988594055, 0.7032137513160706, 0.7051668763160706, 0.6890684366226196, 0.7035688757896423, 0.6898378133773804, 0.704042375087738, 0.7038352489471436]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 4, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 17:01:14 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 4, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 17:01:14 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 17:01:15 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-25 17:01:15 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 17:01:15 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.3909952640533447s\n",
      "04-25 17:01:15 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-25 17:01:16 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5026679039001465s\n",
      "04-25 17:01:16 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 17:01:16 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.2772696018218994s\n",
      "04-25 17:01:16 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.3918468952178955s\n",
      "04-25 17:01:16 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 17:01:16 I deeptables.m.preprocessor.py 249 - transform_X taken 0.129652738571167s\n",
      "04-25 17:01:16 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 17:01:16 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009980201721191406s\n",
      "04-25 17:01:16 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 17:01:16 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 17:01:16 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:01:16 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:01:16 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 17:01:17 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 17:01:17 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9376 - auc: 0.7199 - accuracy: 0.4642 - val_loss: 1.0217 - val_auc: 0.8049 - val_accuracy: 0.5462\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.3734 - auc: 0.8696 - accuracy: 0.6313 - val_loss: 0.9230 - val_auc: 0.8443 - val_accuracy: 0.5865\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.9720 - auc: 0.9300 - accuracy: 0.7481 - val_loss: 0.8938 - val_auc: 0.8599 - val_accuracy: 0.6076\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.7314 - auc: 0.9564 - accuracy: 0.7993 - val_loss: 0.8862 - val_auc: 0.8689 - val_accuracy: 0.6261\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.6056 - auc: 0.9686 - accuracy: 0.8294 - val_loss: 0.8926 - val_auc: 0.8740 - val_accuracy: 0.6385\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5304 - auc: 0.9752 - accuracy: 0.8494 - val_loss: 0.9028 - val_auc: 0.8774 - val_accuracy: 0.6537\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4820 - auc: 0.9792 - accuracy: 0.8619 - val_loss: 0.9001 - val_auc: 0.8807 - val_accuracy: 0.6548\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4468 - auc: 0.9819 - accuracy: 0.8718 - val_loss: 0.9094 - val_auc: 0.8823 - val_accuracy: 0.6590\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4151 - auc: 0.9842 - accuracy: 0.8796 - val_loss: 0.9338 - val_auc: 0.8832 - val_accuracy: 0.6648\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3903 - auc: 0.9861 - accuracy: 0.8870 - val_loss: 0.9475 - val_auc: 0.8842 - val_accuracy: 0.6726\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3670 - auc: 0.9877 - accuracy: 0.8935 - val_loss: 0.9421 - val_auc: 0.8872 - val_accuracy: 0.6733\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3443 - auc: 0.9892 - accuracy: 0.8998 - val_loss: 0.9679 - val_auc: 0.8871 - val_accuracy: 0.6768\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3260 - auc: 0.9904 - accuracy: 0.9050 - val_loss: 0.9757 - val_auc: 0.8886 - val_accuracy: 0.6780\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3095 - auc: 0.9913 - accuracy: 0.9091 - val_loss: 0.9798 - val_auc: 0.8911 - val_accuracy: 0.6835\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2958 - auc: 0.9921 - accuracy: 0.9126 - val_loss: 1.0019 - val_auc: 0.8906 - val_accuracy: 0.6866\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2816 - auc: 0.9929 - accuracy: 0.9162 - val_loss: 1.0167 - val_auc: 0.8905 - val_accuracy: 0.6857\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2690 - auc: 0.9935 - accuracy: 0.9190 - val_loss: 1.0363 - val_auc: 0.8915 - val_accuracy: 0.6860\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2589 - auc: 0.9940 - accuracy: 0.9216 - val_loss: 1.0546 - val_auc: 0.8914 - val_accuracy: 0.6874\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2500 - auc: 0.9944 - accuracy: 0.9244 - val_loss: 1.0749 - val_auc: 0.8906 - val_accuracy: 0.6908\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2415 - auc: 0.9948 - accuracy: 0.9262 - val_loss: 1.0673 - val_auc: 0.8936 - val_accuracy: 0.6920\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2333 - auc: 0.9952 - accuracy: 0.9284 - val_loss: 1.1049 - val_auc: 0.8906 - val_accuracy: 0.6905\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2263 - auc: 0.9954 - accuracy: 0.9301 - val_loss: 1.1307 - val_auc: 0.8902 - val_accuracy: 0.6913\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2220 - auc: 0.9956 - accuracy: 0.9305 - val_loss: 1.1261 - val_auc: 0.8917 - val_accuracy: 0.6928\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2166 - auc: 0.9958 - accuracy: 0.9317 - val_loss: 1.1369 - val_auc: 0.8911 - val_accuracy: 0.6930\n",
      "Epoch 25/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2123 - auc: 0.9960 - accuracy: 0.9330 - val_loss: 1.1576 - val_auc: 0.8904 - val_accuracy: 0.6925\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "04-25 17:06:14 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 17:06:14 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 17:06:15 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425170114_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9375630617141724, 1.3733593225479126, 0.9719583988189697, 0.7314187288284302, 0.6055524945259094, 0.5304328203201294, 0.48200684785842896, 0.4468475878238678, 0.4150773882865906, 0.39032047986984253, 0.3669613301753998, 0.34427815675735474, 0.326002836227417, 0.3095210790634155, 0.29575201869010925, 0.28158608078956604, 0.2690299153327942, 0.2588615119457245, 0.2500217854976654, 0.24150550365447998, 0.2332894206047058, 0.22630518674850464, 0.22202804684638977, 0.21660307049751282, 0.21225686371326447], 'auc': [0.7198763489723206, 0.8695646524429321, 0.9300394058227539, 0.9563812613487244, 0.9685992002487183, 0.9751718044281006, 0.9791696071624756, 0.9819319248199463, 0.9842005372047424, 0.9860880374908447, 0.987679660320282, 0.9891557693481445, 0.990384042263031, 0.9913266897201538, 0.9920950531959534, 0.9928637146949768, 0.993515133857727, 0.9939815402030945, 0.9943988919258118, 0.9947760701179504, 0.9951839447021484, 0.99542236328125, 0.9955829977989197, 0.9958193898200989, 0.9960117936134338], 'accuracy': [0.4642232656478882, 0.6313106417655945, 0.7480539083480835, 0.7993287444114685, 0.8293624520301819, 0.8493590354919434, 0.8619415760040283, 0.8718482255935669, 0.8796430826187134, 0.8870078325271606, 0.8935476541519165, 0.8998265266418457, 0.9049843549728394, 0.9091268181800842, 0.9126241207122803, 0.9161601662635803, 0.9189629554748535, 0.9215964674949646, 0.924409806728363, 0.926225483417511, 0.9283936619758606, 0.930082380771637, 0.9305195212364197, 0.931739330291748, 0.9329909086227417], 'val_loss': [1.0217410326004028, 0.9229773879051208, 0.8937801718711853, 0.886195957660675, 0.8925681114196777, 0.9028343558311462, 0.9001127481460571, 0.9093783497810364, 0.9338120222091675, 0.9475034475326538, 0.9420918226242065, 0.9679246544837952, 0.975746750831604, 0.9798453450202942, 1.0019211769104004, 1.0167182683944702, 1.0362542867660522, 1.0546047687530518, 1.0748502016067505, 1.0673044919967651, 1.104884147644043, 1.1307178735733032, 1.1260998249053955, 1.136940836906433, 1.1576019525527954], 'val_auc': [0.8049383759498596, 0.8443037867546082, 0.8599193096160889, 0.8689064383506775, 0.8739996552467346, 0.8773506283760071, 0.8807007074356079, 0.8822943568229675, 0.8832145929336548, 0.8842244744300842, 0.8872132301330566, 0.8870778679847717, 0.888648509979248, 0.8910749554634094, 0.890609860420227, 0.8904752135276794, 0.8914654850959778, 0.8913580179214478, 0.8906255960464478, 0.8935967683792114, 0.8905981183052063, 0.8901856541633606, 0.8916699886322021, 0.8911274075508118, 0.8903532028198242], 'val_accuracy': [0.5461943745613098, 0.5864997506141663, 0.6075994372367859, 0.6260949373245239, 0.6385239362716675, 0.6537346243858337, 0.6547999382019043, 0.6589725613594055, 0.6647727489471436, 0.6726148128509521, 0.6732658743858337, 0.676816999912262, 0.6779711246490479, 0.6835049986839294, 0.6865530014038086, 0.6856652498245239, 0.6860203742980957, 0.6873816251754761, 0.6908143758773804, 0.6919981241226196, 0.6905480623245239, 0.6912878751754761, 0.6928266882896423, 0.6930042505264282, 0.6925307512283325]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 4, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 17:06:15 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 4, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 17:06:15 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 17:06:15 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 17:06:16 I deeptables.m.preprocessor.py 249 - transform_X taken 0.958942174911499s\n",
      "04-25 17:06:16 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 17:06:16 I deeptables.m.preprocessor.py 236 - transform_y taken 0.007978439331054688s\n",
      "04-25 17:06:17 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 17:06:17 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13763189315795898s\n",
      "04-25 17:06:17 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 17:06:17 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0019953250885009766s\n",
      "04-25 17:06:17 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 17:06:17 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 17:06:17 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:06:17 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:06:17 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 17:06:17 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 17:06:17 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9457 - auc: 0.7191 - accuracy: 0.4727 - val_loss: 1.0344 - val_auc: 0.7964 - val_accuracy: 0.5084\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.4241 - auc: 0.8585 - accuracy: 0.6118 - val_loss: 0.9386 - val_auc: 0.8404 - val_accuracy: 0.5793\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.0609 - auc: 0.9180 - accuracy: 0.7250 - val_loss: 0.9368 - val_auc: 0.8541 - val_accuracy: 0.5987\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.8181 - auc: 0.9472 - accuracy: 0.7785 - val_loss: 0.9953 - val_auc: 0.8556 - val_accuracy: 0.6080\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.6881 - auc: 0.9606 - accuracy: 0.8042 - val_loss: 1.0138 - val_auc: 0.8624 - val_accuracy: 0.6229\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5906 - auc: 0.9703 - accuracy: 0.8325 - val_loss: 1.0423 - val_auc: 0.8657 - val_accuracy: 0.6359\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5171 - auc: 0.9769 - accuracy: 0.8538 - val_loss: 1.0567 - val_auc: 0.8708 - val_accuracy: 0.6554\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4641 - auc: 0.9811 - accuracy: 0.8683 - val_loss: 1.0822 - val_auc: 0.8724 - val_accuracy: 0.6635\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4239 - auc: 0.9841 - accuracy: 0.8798 - val_loss: 1.1067 - val_auc: 0.8740 - val_accuracy: 0.6681\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3906 - auc: 0.9864 - accuracy: 0.8886 - val_loss: 1.1181 - val_auc: 0.8768 - val_accuracy: 0.6759\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3654 - auc: 0.9881 - accuracy: 0.8957 - val_loss: 1.1367 - val_auc: 0.8774 - val_accuracy: 0.6713\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3432 - auc: 0.9895 - accuracy: 0.9005 - val_loss: 1.1540 - val_auc: 0.8773 - val_accuracy: 0.6739\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3226 - auc: 0.9906 - accuracy: 0.9065 - val_loss: 1.1581 - val_auc: 0.8806 - val_accuracy: 0.6811\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3061 - auc: 0.9917 - accuracy: 0.9107 - val_loss: 1.1918 - val_auc: 0.8788 - val_accuracy: 0.6865\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2912 - auc: 0.9924 - accuracy: 0.9148 - val_loss: 1.2062 - val_auc: 0.8799 - val_accuracy: 0.6883\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2784 - auc: 0.9930 - accuracy: 0.9177 - val_loss: 1.2110 - val_auc: 0.8805 - val_accuracy: 0.6910\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2685 - auc: 0.9936 - accuracy: 0.9201 - val_loss: 1.2312 - val_auc: 0.8808 - val_accuracy: 0.6940\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2587 - auc: 0.9940 - accuracy: 0.9227 - val_loss: 1.2487 - val_auc: 0.8794 - val_accuracy: 0.6863\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2490 - auc: 0.9945 - accuracy: 0.9248 - val_loss: 1.2638 - val_auc: 0.8803 - val_accuracy: 0.6865\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2413 - auc: 0.9948 - accuracy: 0.9263 - val_loss: 1.2770 - val_auc: 0.8806 - val_accuracy: 0.6900\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2362 - auc: 0.9950 - accuracy: 0.9271 - val_loss: 1.2901 - val_auc: 0.8803 - val_accuracy: 0.6945\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2278 - auc: 0.9953 - accuracy: 0.9289 - val_loss: 1.3144 - val_auc: 0.8804 - val_accuracy: 0.6894\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "04-25 17:10:37 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 17:10:37 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 17:10:38 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425170615_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9456547498703003, 1.4241160154342651, 1.0609413385391235, 0.8181253671646118, 0.6880820393562317, 0.5905689001083374, 0.5170800685882568, 0.4641386568546295, 0.42389437556266785, 0.3905625641345978, 0.36543866991996765, 0.34319812059402466, 0.32261720299720764, 0.3060871660709381, 0.29122456908226013, 0.2783924341201782, 0.2684986889362335, 0.2587262690067291, 0.24898570775985718, 0.24129973351955414, 0.23618316650390625, 0.22781245410442352], 'auc': [0.7191495299339294, 0.8584545254707336, 0.9180260300636292, 0.9471921920776367, 0.9606062769889832, 0.9702591896057129, 0.9768600463867188, 0.9810710549354553, 0.9840616583824158, 0.9863764047622681, 0.9880625605583191, 0.9894765615463257, 0.9906206727027893, 0.991653561592102, 0.9923987984657288, 0.9930378794670105, 0.99361252784729, 0.9939954280853271, 0.9944543242454529, 0.9947987198829651, 0.9950196743011475, 0.9953339695930481], 'accuracy': [0.4726879894733429, 0.611821711063385, 0.7249689698219299, 0.7785036563873291, 0.8042150735855103, 0.8324719667434692, 0.8537800312042236, 0.8682909607887268, 0.8797664642333984, 0.8886295557022095, 0.8956699967384338, 0.9005140066146851, 0.9065320491790771, 0.910737931728363, 0.9147534966468811, 0.9177255034446716, 0.9200664162635803, 0.9226682186126709, 0.924755334854126, 0.9262783527374268, 0.9270750880241394, 0.9288589954376221], 'val_loss': [1.0344281196594238, 0.9385675191879272, 0.9367660880088806, 0.9953352212905884, 1.0137914419174194, 1.0423223972320557, 1.0567177534103394, 1.0821806192398071, 1.106668472290039, 1.1180860996246338, 1.136746883392334, 1.1539937257766724, 1.1580655574798584, 1.1917651891708374, 1.2061506509780884, 1.2110038995742798, 1.2312355041503906, 1.2486562728881836, 1.2638188600540161, 1.276951789855957, 1.2901345491409302, 1.314446210861206], 'val_auc': [0.7964096069335938, 0.8404451012611389, 0.8541078567504883, 0.8556492924690247, 0.8623608350753784, 0.8657488226890564, 0.8708019852638245, 0.8724223375320435, 0.8740060925483704, 0.8767610192298889, 0.877433717250824, 0.8773377537727356, 0.880640983581543, 0.878765881061554, 0.8798509836196899, 0.8805152177810669, 0.8807988166809082, 0.8794431686401367, 0.8802578449249268, 0.8805935978889465, 0.8803371787071228, 0.8803882002830505], 'val_accuracy': [0.5083747506141663, 0.5793383121490479, 0.5987215638160706, 0.6079545617103577, 0.6228989362716675, 0.6358901262283325, 0.6554213762283325, 0.6635002493858337, 0.6681166887283325, 0.6758700013160706, 0.6712535619735718, 0.6738576889038086, 0.6811079382896423, 0.686464250087738, 0.6883286237716675, 0.6909623742103577, 0.6939808130264282, 0.6862866878509521, 0.6864938735961914, 0.6899858117103577, 0.6944838762283325, 0.689393937587738]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 2, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 17:10:38 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 2, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 17:10:38 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 17:10:38 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 17:10:39 I deeptables.m.preprocessor.py 249 - transform_X taken 0.9699316024780273s\n",
      "04-25 17:10:39 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 17:10:39 I deeptables.m.preprocessor.py 236 - transform_y taken 0.007979154586791992s\n",
      "04-25 17:10:39 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 17:10:40 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13763189315795898s\n",
      "04-25 17:10:40 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 17:10:40 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009975433349609375s\n",
      "04-25 17:10:40 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 17:10:40 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 17:10:40 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:10:40 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:10:40 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 17:10:40 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 17:10:40 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9410 - auc: 0.7209 - accuracy: 0.4809 - val_loss: 1.0229 - val_auc: 0.8045 - val_accuracy: 0.5450\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.4313 - auc: 0.8563 - accuracy: 0.6071 - val_loss: 0.9394 - val_auc: 0.8395 - val_accuracy: 0.5885\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.1108 - auc: 0.9104 - accuracy: 0.7070 - val_loss: 0.9480 - val_auc: 0.8496 - val_accuracy: 0.5898\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.8567 - auc: 0.9427 - accuracy: 0.7681 - val_loss: 0.9738 - val_auc: 0.8584 - val_accuracy: 0.6102\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.7066 - auc: 0.9589 - accuracy: 0.8012 - val_loss: 0.9923 - val_auc: 0.8651 - val_accuracy: 0.6368\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.6051 - auc: 0.9689 - accuracy: 0.8272 - val_loss: 1.0162 - val_auc: 0.8697 - val_accuracy: 0.6523\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5329 - auc: 0.9752 - accuracy: 0.8475 - val_loss: 1.0320 - val_auc: 0.8745 - val_accuracy: 0.6703\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4761 - auc: 0.9798 - accuracy: 0.8635 - val_loss: 1.0629 - val_auc: 0.8753 - val_accuracy: 0.6744\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4350 - auc: 0.9830 - accuracy: 0.8747 - val_loss: 1.0877 - val_auc: 0.8761 - val_accuracy: 0.6797\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3999 - auc: 0.9856 - accuracy: 0.8848 - val_loss: 1.0943 - val_auc: 0.8794 - val_accuracy: 0.6882\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3713 - auc: 0.9876 - accuracy: 0.8939 - val_loss: 1.1207 - val_auc: 0.8789 - val_accuracy: 0.6905\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3480 - auc: 0.9890 - accuracy: 0.8999 - val_loss: 1.1486 - val_auc: 0.8789 - val_accuracy: 0.6957\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3266 - auc: 0.9903 - accuracy: 0.9059 - val_loss: 1.1636 - val_auc: 0.8796 - val_accuracy: 0.6961\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3099 - auc: 0.9913 - accuracy: 0.9093 - val_loss: 1.1804 - val_auc: 0.8806 - val_accuracy: 0.7032\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2958 - auc: 0.9921 - accuracy: 0.9135 - val_loss: 1.1927 - val_auc: 0.8814 - val_accuracy: 0.7027\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2822 - auc: 0.9929 - accuracy: 0.9165 - val_loss: 1.2079 - val_auc: 0.8814 - val_accuracy: 0.7044\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2688 - auc: 0.9935 - accuracy: 0.9193 - val_loss: 1.2356 - val_auc: 0.8804 - val_accuracy: 0.7043\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2595 - auc: 0.9940 - accuracy: 0.9223 - val_loss: 1.2529 - val_auc: 0.8807 - val_accuracy: 0.7068\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2508 - auc: 0.9943 - accuracy: 0.9247 - val_loss: 1.2512 - val_auc: 0.8815 - val_accuracy: 0.7073\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.2423 - auc: 0.9947 - accuracy: 0.9262 - val_loss: 1.2821 - val_auc: 0.8808 - val_accuracy: 0.7073\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2358 - auc: 0.9950 - accuracy: 0.9280 - val_loss: 1.2940 - val_auc: 0.8809 - val_accuracy: 0.7075\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2277 - auc: 0.9954 - accuracy: 0.9295 - val_loss: 1.2992 - val_auc: 0.8808 - val_accuracy: 0.7023\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2228 - auc: 0.9955 - accuracy: 0.9309 - val_loss: 1.3190 - val_auc: 0.8808 - val_accuracy: 0.7088\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2166 - auc: 0.9958 - accuracy: 0.9323 - val_loss: 1.3283 - val_auc: 0.8810 - val_accuracy: 0.7102\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "04-25 17:15:26 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 17:15:26 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 17:15:27 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425171038_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.941037654876709, 1.431312084197998, 1.1107577085494995, 0.8566846251487732, 0.7065895199775696, 0.6051333546638489, 0.5329453945159912, 0.4760949909687042, 0.4349825382232666, 0.3998974859714508, 0.3713347315788269, 0.34800222516059875, 0.32655084133148193, 0.3098955452442169, 0.29576539993286133, 0.2821602523326874, 0.2688257098197937, 0.25947505235671997, 0.25084173679351807, 0.24231970310211182, 0.23583555221557617, 0.22765618562698364, 0.22280016541481018, 0.21659480035305023], 'auc': [0.7208607792854309, 0.8562970161437988, 0.9103969931602478, 0.9426673650741577, 0.9589080810546875, 0.9688819646835327, 0.9751711487770081, 0.9798113107681274, 0.9829624891281128, 0.9855718016624451, 0.9875771403312683, 0.9890400171279907, 0.9903338551521301, 0.9912834763526917, 0.9920694231987, 0.9928728342056274, 0.9935043454170227, 0.9939656257629395, 0.9943313598632812, 0.994701087474823, 0.9950300455093384, 0.9953566789627075, 0.9955471754074097, 0.9958001971244812], 'accuracy': [0.4808918237686157, 0.6071081161499023, 0.706981897354126, 0.768135130405426, 0.8012219667434692, 0.8271766304969788, 0.8474588394165039, 0.8634716272354126, 0.8747391104698181, 0.8847832679748535, 0.8939072489738464, 0.899872362613678, 0.9059080481529236, 0.9093312621116638, 0.9134631752967834, 0.9165127277374268, 0.9192767143249512, 0.9222592711448669, 0.9246566295623779, 0.9261796474456787, 0.9279599785804749, 0.9294689297676086, 0.9309073090553284, 0.9323316216468811], 'val_loss': [1.0228908061981201, 0.9393998980522156, 0.9480469226837158, 0.9737839102745056, 0.992343544960022, 1.0162169933319092, 1.031986951828003, 1.062906265258789, 1.0876784324645996, 1.0942994356155396, 1.1207271814346313, 1.148620843887329, 1.1635723114013672, 1.1803680658340454, 1.1927149295806885, 1.2079356908798218, 1.2356414794921875, 1.2529488801956177, 1.2511943578720093, 1.2821046113967896, 1.2940189838409424, 1.2991740703582764, 1.318981647491455, 1.3282514810562134], 'val_auc': [0.8045415282249451, 0.8395320177078247, 0.8496103286743164, 0.858359158039093, 0.8651339411735535, 0.8697472214698792, 0.8744565844535828, 0.8752846121788025, 0.8760743737220764, 0.8794012665748596, 0.8789019584655762, 0.8788838386535645, 0.8795796036720276, 0.8806366920471191, 0.8814180493354797, 0.8814264535903931, 0.8804091215133667, 0.8806722164154053, 0.8814573884010315, 0.8807882070541382, 0.8809194564819336, 0.8808483481407166, 0.8808256983757019, 0.8810099959373474], 'val_accuracy': [0.544981062412262, 0.5884528756141663, 0.5898141860961914, 0.6102036237716675, 0.6368075013160706, 0.65234375, 0.6703065633773804, 0.6743903756141663, 0.6796875, 0.6882102489471436, 0.6905184388160706, 0.6956971883773804, 0.6961411237716675, 0.703184187412262, 0.7026515007019043, 0.7043678760528564, 0.7042791247367859, 0.7067649364471436, 0.7072975635528564, 0.7072679996490479, 0.7075343132019043, 0.7023259997367859, 0.7088364362716675, 0.7102272510528564]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 4, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 17:15:27 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 4, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 17:15:27 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 17:15:27 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 17:15:28 I deeptables.m.preprocessor.py 249 - transform_X taken 0.9524745941162109s\n",
      "04-25 17:15:28 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 17:15:28 I deeptables.m.preprocessor.py 236 - transform_y taken 0.007978439331054688s\n",
      "04-25 17:15:29 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 17:15:29 I deeptables.m.preprocessor.py 249 - transform_X taken 0.1356334686279297s\n",
      "04-25 17:15:29 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 17:15:29 I deeptables.m.preprocessor.py 236 - transform_y taken 0.000997781753540039s\n",
      "04-25 17:15:29 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 17:15:29 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 17:15:29 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:15:29 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:15:29 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 17:15:29 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 17:15:29 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9465 - auc: 0.7202 - accuracy: 0.4829 - val_loss: 1.0368 - val_auc: 0.7979 - val_accuracy: 0.5278\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.4253 - auc: 0.8586 - accuracy: 0.6129 - val_loss: 0.9381 - val_auc: 0.8400 - val_accuracy: 0.5769\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.0556 - auc: 0.9190 - accuracy: 0.7266 - val_loss: 0.9362 - val_auc: 0.8553 - val_accuracy: 0.5996\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.7867 - auc: 0.9511 - accuracy: 0.7892 - val_loss: 0.9768 - val_auc: 0.8621 - val_accuracy: 0.6220\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.6407 - auc: 0.9658 - accuracy: 0.8211 - val_loss: 1.0045 - val_auc: 0.8673 - val_accuracy: 0.6342\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5615 - auc: 0.9729 - accuracy: 0.8402 - val_loss: 1.0245 - val_auc: 0.8710 - val_accuracy: 0.6557\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5068 - auc: 0.9773 - accuracy: 0.8551 - val_loss: 1.0413 - val_auc: 0.8742 - val_accuracy: 0.6624\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4656 - auc: 0.9805 - accuracy: 0.8658 - val_loss: 1.0725 - val_auc: 0.8731 - val_accuracy: 0.6681\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4318 - auc: 0.9832 - accuracy: 0.8753 - val_loss: 1.0839 - val_auc: 0.8757 - val_accuracy: 0.6673\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4012 - auc: 0.9853 - accuracy: 0.8838 - val_loss: 1.1134 - val_auc: 0.8746 - val_accuracy: 0.6686\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3763 - auc: 0.9871 - accuracy: 0.8909 - val_loss: 1.1176 - val_auc: 0.8774 - val_accuracy: 0.6824\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3518 - auc: 0.9887 - accuracy: 0.8984 - val_loss: 1.1441 - val_auc: 0.8768 - val_accuracy: 0.6845\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3309 - auc: 0.9901 - accuracy: 0.9035 - val_loss: 1.1633 - val_auc: 0.8777 - val_accuracy: 0.6971\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3157 - auc: 0.9909 - accuracy: 0.9081 - val_loss: 1.1689 - val_auc: 0.8788 - val_accuracy: 0.6982\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2998 - auc: 0.9919 - accuracy: 0.9120 - val_loss: 1.1907 - val_auc: 0.8790 - val_accuracy: 0.6997\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2867 - auc: 0.9926 - accuracy: 0.9156 - val_loss: 1.2136 - val_auc: 0.8784 - val_accuracy: 0.6889\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2758 - auc: 0.9932 - accuracy: 0.9182 - val_loss: 1.2270 - val_auc: 0.8788 - val_accuracy: 0.6977\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2645 - auc: 0.9937 - accuracy: 0.9205 - val_loss: 1.2393 - val_auc: 0.8794 - val_accuracy: 0.6971\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2563 - auc: 0.9941 - accuracy: 0.9229 - val_loss: 1.2473 - val_auc: 0.8796 - val_accuracy: 0.6903\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2457 - auc: 0.9946 - accuracy: 0.9248 - val_loss: 1.2672 - val_auc: 0.8802 - val_accuracy: 0.6950\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2416 - auc: 0.9948 - accuracy: 0.9260 - val_loss: 1.2758 - val_auc: 0.8805 - val_accuracy: 0.7033\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2331 - auc: 0.9951 - accuracy: 0.9280 - val_loss: 1.2976 - val_auc: 0.8802 - val_accuracy: 0.6960\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2285 - auc: 0.9953 - accuracy: 0.9289 - val_loss: 1.2849 - val_auc: 0.8819 - val_accuracy: 0.7044\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2229 - auc: 0.9956 - accuracy: 0.9307 - val_loss: 1.3162 - val_auc: 0.8805 - val_accuracy: 0.7041\n",
      "Epoch 25/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2191 - auc: 0.9957 - accuracy: 0.9313 - val_loss: 1.3241 - val_auc: 0.8812 - val_accuracy: 0.7062\n",
      "Epoch 26/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2127 - auc: 0.9959 - accuracy: 0.9329 - val_loss: 1.3418 - val_auc: 0.8805 - val_accuracy: 0.7061\n",
      "Epoch 27/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2101 - auc: 0.9961 - accuracy: 0.9331 - val_loss: 1.3194 - val_auc: 0.8826 - val_accuracy: 0.7121\n",
      "Epoch 28/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2059 - auc: 0.9962 - accuracy: 0.9345 - val_loss: 1.3531 - val_auc: 0.8807 - val_accuracy: 0.7119\n",
      "Epoch 29/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2020 - auc: 0.9963 - accuracy: 0.9350 - val_loss: 1.3607 - val_auc: 0.8814 - val_accuracy: 0.7057\n",
      "Epoch 30/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1990 - auc: 0.9964 - accuracy: 0.9357 - val_loss: 1.3694 - val_auc: 0.8810 - val_accuracy: 0.7051\n",
      "Epoch 31/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1963 - auc: 0.9966 - accuracy: 0.9365 - val_loss: 1.3864 - val_auc: 0.8800 - val_accuracy: 0.7037\n",
      "Epoch 32/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1946 - auc: 0.9966 - accuracy: 0.9362 - val_loss: 1.3958 - val_auc: 0.8806 - val_accuracy: 0.7062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00032: early stopping\n",
      "04-25 17:21:50 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 17:21:50 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 17:21:51 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425171527_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9464948177337646, 1.4252547025680542, 1.0556484460830688, 0.7867283225059509, 0.6407276391983032, 0.5614786148071289, 0.5068111419677734, 0.46562519669532776, 0.43183866143226624, 0.4012473225593567, 0.3762720227241516, 0.3518279194831848, 0.33091476559638977, 0.31573328375816345, 0.29975104331970215, 0.28673291206359863, 0.2757706344127655, 0.2645232379436493, 0.2562812566757202, 0.2457144558429718, 0.24161013960838318, 0.23312821984291077, 0.2284858226776123, 0.22294402122497559, 0.21906225383281708, 0.21265128254890442, 0.2100553959608078, 0.20586960017681122, 0.20198215544223785, 0.19902490079402924, 0.1962917149066925, 0.19456373155117035], 'auc': [0.7202046513557434, 0.8585799336433411, 0.9189770221710205, 0.9511024951934814, 0.965831458568573, 0.9728559255599976, 0.9772902131080627, 0.9805371761322021, 0.9831566214561462, 0.9853363037109375, 0.9871116876602173, 0.9887228608131409, 0.9900745749473572, 0.9909167885780334, 0.9918769598007202, 0.992569625377655, 0.9931799173355103, 0.9937101602554321, 0.9941051602363586, 0.9945842623710632, 0.9947721362113953, 0.995128870010376, 0.9953150153160095, 0.9955699443817139, 0.9957327246665955, 0.9959099888801575, 0.9960800409317017, 0.9961876273155212, 0.9963470697402954, 0.9964455962181091, 0.9965667128562927, 0.9965939521789551], 'accuracy': [0.4829083979129791, 0.6128828525543213, 0.7265554666519165, 0.7892070412635803, 0.8211163282394409, 0.840238630771637, 0.855059802532196, 0.8658336997032166, 0.8753067255020142, 0.8838278651237488, 0.8908823728561401, 0.8983986973762512, 0.9034543037414551, 0.908143162727356, 0.912024736404419, 0.915596067905426, 0.9182190895080566, 0.9205423593521118, 0.9228832721710205, 0.9247588515281677, 0.9260244965553284, 0.9280199408531189, 0.9289224743843079, 0.9306992888450623, 0.9313092231750488, 0.9329450726509094, 0.933135449886322, 0.9344821572303772, 0.9349510669708252, 0.9356914162635803, 0.9365375638008118, 0.9361673593521118], 'val_loss': [1.0368022918701172, 0.938062310218811, 0.9362088441848755, 0.976787805557251, 1.0044857263565063, 1.0244640111923218, 1.0413005352020264, 1.0724961757659912, 1.0838953256607056, 1.1133841276168823, 1.1175503730773926, 1.144084095954895, 1.1633453369140625, 1.1689345836639404, 1.190683126449585, 1.2135742902755737, 1.2269842624664307, 1.23926842212677, 1.2473374605178833, 1.267236590385437, 1.2758175134658813, 1.297648310661316, 1.2849397659301758, 1.3161996603012085, 1.3240821361541748, 1.3417539596557617, 1.3194329738616943, 1.3531103134155273, 1.3606688976287842, 1.3694326877593994, 1.3863558769226074, 1.3957637548446655], 'val_auc': [0.7979084253311157, 0.8400363326072693, 0.8552728295326233, 0.8620547652244568, 0.8672809600830078, 0.8710241317749023, 0.8741938471794128, 0.8731157779693604, 0.8756517171859741, 0.8746224641799927, 0.8774335384368896, 0.8767988681793213, 0.8777108788490295, 0.8788469433784485, 0.8789739012718201, 0.8783606290817261, 0.8788063526153564, 0.879424512386322, 0.8796199560165405, 0.8802385926246643, 0.8805406093597412, 0.8801918625831604, 0.8819394707679749, 0.8804519176483154, 0.8812233805656433, 0.8805113434791565, 0.8826000094413757, 0.8806518912315369, 0.8814319372177124, 0.8809777498245239, 0.8799958229064941, 0.8805960416793823], 'val_accuracy': [0.5278468132019043, 0.5769116878509521, 0.599550187587738, 0.6219815611839294, 0.6341737508773804, 0.6557173132896423, 0.6623756885528564, 0.6680575013160706, 0.6673473119735718, 0.6685606241226196, 0.6824100613594055, 0.6844519376754761, 0.6970880627632141, 0.698183000087738, 0.6996922492980957, 0.6888908743858337, 0.6977095007896423, 0.6970880627632141, 0.6902521252632141, 0.6950165629386902, 0.7033321261405945, 0.6959931254386902, 0.7044270634651184, 0.7041311264038086, 0.7062026262283325, 0.706113874912262, 0.7120620012283325, 0.7119140625, 0.7056995630264282, 0.7051076889038086, 0.7037464380264282, 0.7061730623245239]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 17:21:51 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 17:21:51 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 17:21:51 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 17:21:52 I deeptables.m.preprocessor.py 249 - transform_X taken 0.9669203758239746s\n",
      "04-25 17:21:52 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 17:21:52 I deeptables.m.preprocessor.py 236 - transform_y taken 0.00797891616821289s\n",
      "04-25 17:21:53 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 17:21:53 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13514947891235352s\n",
      "04-25 17:21:53 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 17:21:53 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009975433349609375s\n",
      "04-25 17:21:53 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 17:21:53 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 17:21:53 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:21:53 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:21:53 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 17:21:53 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 17:21:53 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9463 - auc: 0.7200 - accuracy: 0.4814 - val_loss: 1.0437 - val_auc: 0.7929 - val_accuracy: 0.5081\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.4248 - auc: 0.8581 - accuracy: 0.6107 - val_loss: 0.9498 - val_auc: 0.8359 - val_accuracy: 0.5702\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.0486 - auc: 0.9197 - accuracy: 0.7266 - val_loss: 0.9426 - val_auc: 0.8548 - val_accuracy: 0.6033\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.7791 - auc: 0.9516 - accuracy: 0.7889 - val_loss: 0.9569 - val_auc: 0.8643 - val_accuracy: 0.6268\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.6290 - auc: 0.9665 - accuracy: 0.8241 - val_loss: 0.9870 - val_auc: 0.8704 - val_accuracy: 0.6398\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5465 - auc: 0.9738 - accuracy: 0.8445 - val_loss: 1.0289 - val_auc: 0.8707 - val_accuracy: 0.6451\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4939 - auc: 0.9782 - accuracy: 0.8586 - val_loss: 1.0410 - val_auc: 0.8731 - val_accuracy: 0.6526\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4548 - auc: 0.9813 - accuracy: 0.8692 - val_loss: 1.0471 - val_auc: 0.8758 - val_accuracy: 0.6641\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4209 - auc: 0.9839 - accuracy: 0.8783 - val_loss: 1.0801 - val_auc: 0.8758 - val_accuracy: 0.6613\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3885 - auc: 0.9862 - accuracy: 0.8869 - val_loss: 1.1025 - val_auc: 0.8769 - val_accuracy: 0.6694\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3626 - auc: 0.9880 - accuracy: 0.8950 - val_loss: 1.1157 - val_auc: 0.8787 - val_accuracy: 0.6737\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3416 - auc: 0.9895 - accuracy: 0.9009 - val_loss: 1.1306 - val_auc: 0.8803 - val_accuracy: 0.6760\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3208 - auc: 0.9907 - accuracy: 0.9066 - val_loss: 1.1654 - val_auc: 0.8791 - val_accuracy: 0.6797\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3045 - auc: 0.9916 - accuracy: 0.9109 - val_loss: 1.1936 - val_auc: 0.8796 - val_accuracy: 0.6827\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2911 - auc: 0.9924 - accuracy: 0.9145 - val_loss: 1.1940 - val_auc: 0.8811 - val_accuracy: 0.6845\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2771 - auc: 0.9931 - accuracy: 0.9175 - val_loss: 1.2068 - val_auc: 0.8822 - val_accuracy: 0.6895\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2663 - auc: 0.9937 - accuracy: 0.9208 - val_loss: 1.2077 - val_auc: 0.8831 - val_accuracy: 0.6861\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2563 - auc: 0.9942 - accuracy: 0.9226 - val_loss: 1.2395 - val_auc: 0.8817 - val_accuracy: 0.6875\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2469 - auc: 0.9945 - accuracy: 0.9249 - val_loss: 1.2569 - val_auc: 0.8820 - val_accuracy: 0.6892\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2371 - auc: 0.9950 - accuracy: 0.9273 - val_loss: 1.2933 - val_auc: 0.8813 - val_accuracy: 0.6894\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2305 - auc: 0.9952 - accuracy: 0.9287 - val_loss: 1.2912 - val_auc: 0.8823 - val_accuracy: 0.6927\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2254 - auc: 0.9954 - accuracy: 0.9298 - val_loss: 1.2863 - val_auc: 0.8828 - val_accuracy: 0.6908\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "04-25 17:26:15 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 17:26:15 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 17:26:16 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425172151_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9462716579437256, 1.424768090248108, 1.0485584735870361, 0.779137134552002, 0.629043698310852, 0.5464690327644348, 0.4939242899417877, 0.45483535528182983, 0.42093604803085327, 0.3885282278060913, 0.3625677525997162, 0.3415563106536865, 0.3208147883415222, 0.30446314811706543, 0.2911379933357239, 0.27707913517951965, 0.2662794291973114, 0.2562801241874695, 0.24686335027217865, 0.2371378391981125, 0.23048114776611328, 0.22539664804935455], 'auc': [0.7200007438659668, 0.8580883145332336, 0.9196597337722778, 0.9516434669494629, 0.9665096998214722, 0.9737873673439026, 0.9782273769378662, 0.9812700152397156, 0.9838991165161133, 0.9861653447151184, 0.9879962205886841, 0.9894554615020752, 0.990683376789093, 0.9915952682495117, 0.9923813343048096, 0.9930757880210876, 0.993655800819397, 0.9941686987876892, 0.9945440888404846, 0.9949514865875244, 0.9952350854873657, 0.9954368472099304], 'accuracy': [0.481395959854126, 0.6107252836227417, 0.7266153693199158, 0.7889249920845032, 0.8241376876831055, 0.8445397019386292, 0.8586276173591614, 0.869214653968811, 0.8783174753189087, 0.8868739008903503, 0.8949825167655945, 0.9009441137313843, 0.9065919518470764, 0.9109389185905457, 0.9145349264144897, 0.9175033569335938, 0.9208314418792725, 0.9225941896438599, 0.9248505234718323, 0.9272549152374268, 0.928735613822937, 0.9298038482666016], 'val_loss': [1.0437215566635132, 0.9497879147529602, 0.9426315426826477, 0.9569066166877747, 0.9870094656944275, 1.028907299041748, 1.041017770767212, 1.0471190214157104, 1.0800533294677734, 1.1024799346923828, 1.1156541109085083, 1.1305783987045288, 1.1654036045074463, 1.1935927867889404, 1.1940149068832397, 1.2067826986312866, 1.2077388763427734, 1.239461898803711, 1.2569299936294556, 1.2933164834976196, 1.2911964654922485, 1.2862945795059204], 'val_auc': [0.7928857803344727, 0.8359465599060059, 0.8548063039779663, 0.8642561435699463, 0.8703641891479492, 0.8707312345504761, 0.8730879426002502, 0.8757591843605042, 0.8757807612419128, 0.8769291043281555, 0.8787283301353455, 0.8802791237831116, 0.879063606262207, 0.8795571327209473, 0.8811197876930237, 0.8821895122528076, 0.883095383644104, 0.8816505074501038, 0.8819700479507446, 0.8812543749809265, 0.8822604417800903, 0.8828197717666626], 'val_accuracy': [0.5081084370613098, 0.5702237486839294, 0.6033084988594055, 0.6268051862716675, 0.6397963762283325, 0.6450935006141663, 0.6525508761405945, 0.6640920639038086, 0.6613103747367859, 0.6693596243858337, 0.6736505627632141, 0.6760179996490479, 0.6796875, 0.682676374912262, 0.6845407485961914, 0.6895418763160706, 0.6861387491226196, 0.6875295639038086, 0.6891868114471436, 0.6894235610961914, 0.6926787495613098, 0.6907848119735718]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 5, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 17:26:16 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 5, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 17:26:16 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 17:26:17 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-25 17:26:17 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 17:26:17 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.39196205139160156s\n",
      "04-25 17:26:17 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-25 17:26:18 I deeptables.m.preprocessor.py 383 - Imputation taken 0.54654860496521s\n",
      "04-25 17:26:18 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 17:26:18 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.285247802734375s\n",
      "04-25 17:26:18 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.4471724033355713s\n",
      "04-25 17:26:18 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 17:26:18 I deeptables.m.preprocessor.py 249 - transform_X taken 0.1301567554473877s\n",
      "04-25 17:26:18 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 17:26:18 I deeptables.m.preprocessor.py 236 - transform_y taken 0.000997304916381836s\n",
      "04-25 17:26:18 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 17:26:18 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 17:26:18 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:26:19 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:26:19 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 17:26:19 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 17:26:19 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9442 - auc: 0.7186 - accuracy: 0.4769 - val_loss: 1.0201 - val_auc: 0.8054 - val_accuracy: 0.5407\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.4229 - auc: 0.8592 - accuracy: 0.6132 - val_loss: 0.9422 - val_auc: 0.8397 - val_accuracy: 0.5853\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.0380 - auc: 0.9212 - accuracy: 0.7295 - val_loss: 0.9411 - val_auc: 0.8537 - val_accuracy: 0.6145\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.7819 - auc: 0.9513 - accuracy: 0.7892 - val_loss: 0.9704 - val_auc: 0.8621 - val_accuracy: 0.6329\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.6382 - auc: 0.9657 - accuracy: 0.8214 - val_loss: 0.9952 - val_auc: 0.8681 - val_accuracy: 0.6490\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5519 - auc: 0.9734 - accuracy: 0.8427 - val_loss: 1.0273 - val_auc: 0.8713 - val_accuracy: 0.6574\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4951 - auc: 0.9781 - accuracy: 0.8582 - val_loss: 1.0450 - val_auc: 0.8735 - val_accuracy: 0.6653\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4526 - auc: 0.9815 - accuracy: 0.8695 - val_loss: 1.0582 - val_auc: 0.8756 - val_accuracy: 0.6711\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4170 - auc: 0.9842 - accuracy: 0.8807 - val_loss: 1.0657 - val_auc: 0.8776 - val_accuracy: 0.6760\n",
      "Epoch 10/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3853 - auc: 0.9865 - accuracy: 0.8888 - val_loss: 1.0911 - val_auc: 0.8793 - val_accuracy: 0.6847\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3581 - auc: 0.9884 - accuracy: 0.8962 - val_loss: 1.1097 - val_auc: 0.8801 - val_accuracy: 0.6883\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3378 - auc: 0.9897 - accuracy: 0.9026 - val_loss: 1.1253 - val_auc: 0.8807 - val_accuracy: 0.6900\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3165 - auc: 0.9909 - accuracy: 0.9075 - val_loss: 1.1467 - val_auc: 0.8817 - val_accuracy: 0.6937\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3003 - auc: 0.9918 - accuracy: 0.9116 - val_loss: 1.1581 - val_auc: 0.8824 - val_accuracy: 0.6945\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2875 - auc: 0.9926 - accuracy: 0.9150 - val_loss: 1.1803 - val_auc: 0.8826 - val_accuracy: 0.6982\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2751 - auc: 0.9932 - accuracy: 0.9178 - val_loss: 1.1995 - val_auc: 0.8819 - val_accuracy: 0.6981\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2626 - auc: 0.9938 - accuracy: 0.9211 - val_loss: 1.2085 - val_auc: 0.8833 - val_accuracy: 0.7001\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2534 - auc: 0.9942 - accuracy: 0.9235 - val_loss: 1.2443 - val_auc: 0.8819 - val_accuracy: 0.7001\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2440 - auc: 0.9947 - accuracy: 0.9252 - val_loss: 1.2530 - val_auc: 0.8813 - val_accuracy: 0.7013\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2362 - auc: 0.9950 - accuracy: 0.9271 - val_loss: 1.2538 - val_auc: 0.8835 - val_accuracy: 0.7040\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2296 - auc: 0.9952 - accuracy: 0.9289 - val_loss: 1.2690 - val_auc: 0.8838 - val_accuracy: 0.7039\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2244 - auc: 0.9955 - accuracy: 0.9300 - val_loss: 1.2800 - val_auc: 0.8836 - val_accuracy: 0.7046\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2180 - auc: 0.9957 - accuracy: 0.9313 - val_loss: 1.2991 - val_auc: 0.8841 - val_accuracy: 0.7062\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2130 - auc: 0.9959 - accuracy: 0.9327 - val_loss: 1.2981 - val_auc: 0.8844 - val_accuracy: 0.7062\n",
      "Epoch 25/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2087 - auc: 0.9961 - accuracy: 0.9335 - val_loss: 1.3445 - val_auc: 0.8822 - val_accuracy: 0.7060\n",
      "Epoch 26/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2061 - auc: 0.9962 - accuracy: 0.9342 - val_loss: 1.3455 - val_auc: 0.8826 - val_accuracy: 0.7073\n",
      "Epoch 27/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2021 - auc: 0.9963 - accuracy: 0.9352 - val_loss: 1.3668 - val_auc: 0.8821 - val_accuracy: 0.7061\n",
      "Epoch 28/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1996 - auc: 0.9964 - accuracy: 0.9352 - val_loss: 1.3697 - val_auc: 0.8829 - val_accuracy: 0.7079\n",
      "Epoch 29/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1959 - auc: 0.9965 - accuracy: 0.9365 - val_loss: 1.3880 - val_auc: 0.8827 - val_accuracy: 0.7076\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00029: early stopping\n",
      "04-25 17:32:05 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 17:32:05 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 17:32:05 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425172616_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9442164897918701, 1.4229130744934082, 1.0380234718322754, 0.7819082140922546, 0.6381621956825256, 0.5519346594810486, 0.4950999915599823, 0.45262953639030457, 0.4170263409614563, 0.38527777791023254, 0.35814234614372253, 0.3378499448299408, 0.3165060877799988, 0.30030426383018494, 0.2874841094017029, 0.27508220076560974, 0.2625873386859894, 0.25344303250312805, 0.2439500242471695, 0.2361663579940796, 0.22961688041687012, 0.2243613302707672, 0.21797369420528412, 0.21299739181995392, 0.20874309539794922, 0.20614616572856903, 0.20212695002555847, 0.1995728760957718, 0.1959429532289505], 'auc': [0.7185506820678711, 0.8592389822006226, 0.9211565852165222, 0.9512916207313538, 0.9657467603683472, 0.9734083414077759, 0.9780741930007935, 0.9814820885658264, 0.9841839671134949, 0.9865416288375854, 0.9883785247802734, 0.9896868467330933, 0.9909060001373291, 0.9918274283409119, 0.9925631880760193, 0.9931821227073669, 0.9937583208084106, 0.9941992163658142, 0.9946502447128296, 0.994988203048706, 0.9952327609062195, 0.995452880859375, 0.9956992864608765, 0.9959133863449097, 0.9960899949073792, 0.9961628317832947, 0.9963270425796509, 0.9963981509208679, 0.9965382814407349], 'accuracy': [0.4768621623516083, 0.613182544708252, 0.7294780611991882, 0.7891823649406433, 0.8214336037635803, 0.8427382111549377, 0.8582186102867126, 0.8694720268249512, 0.8806689977645874, 0.8888481259346008, 0.8961776494979858, 0.9025869965553284, 0.9075403213500977, 0.9115735292434692, 0.9150214195251465, 0.9178242087364197, 0.9210888147354126, 0.9234650135040283, 0.9252347946166992, 0.9271280169487, 0.9289189577102661, 0.9300470948219299, 0.9313480257987976, 0.9327088594436646, 0.9334774017333984, 0.9342459440231323, 0.9351802468299866, 0.9352260828018188, 0.936463475227356], 'val_loss': [1.0201393365859985, 0.9421838521957397, 0.9411110281944275, 0.9703710079193115, 0.9951651692390442, 1.0272972583770752, 1.04500150680542, 1.058189034461975, 1.0656635761260986, 1.0911377668380737, 1.1096975803375244, 1.1252765655517578, 1.1466621160507202, 1.158082365989685, 1.1802915334701538, 1.199549913406372, 1.2084765434265137, 1.2443032264709473, 1.253011703491211, 1.2538021802902222, 1.2690473794937134, 1.2800052165985107, 1.2991034984588623, 1.2980637550354004, 1.344545841217041, 1.3454903364181519, 1.3667875528335571, 1.3697452545166016, 1.38800048828125], 'val_auc': [0.8054379224777222, 0.8397324681282043, 0.8537127375602722, 0.8621059060096741, 0.8681359887123108, 0.8712584376335144, 0.8735402822494507, 0.8756161332130432, 0.8775653839111328, 0.8793490529060364, 0.880069375038147, 0.8807148933410645, 0.881673276424408, 0.882358968257904, 0.882552444934845, 0.8819286823272705, 0.8833081126213074, 0.8819499015808105, 0.8813098669052124, 0.8834603428840637, 0.8837981820106506, 0.8836232423782349, 0.8840776085853577, 0.8843607902526855, 0.8822125196456909, 0.8825971484184265, 0.8820669054985046, 0.8829075694084167, 0.882703959941864], 'val_accuracy': [0.5406901240348816, 0.5853456258773804, 0.6144649386405945, 0.6329308748245239, 0.6490293741226196, 0.6574336886405945, 0.6652758121490479, 0.6711055636405945, 0.6759588122367859, 0.6847182512283325, 0.6882694363594055, 0.6899561882019043, 0.6937144994735718, 0.6945430636405945, 0.6982421875, 0.6980942487716675, 0.700136125087738, 0.700136125087738, 0.7013494372367859, 0.7040128111839294, 0.7038944363594055, 0.7046342492103577, 0.7062322497367859, 0.7062322497367859, 0.705995500087738, 0.7072679996490479, 0.7060842514038086, 0.7079190611839294, 0.7075639367103577]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 2, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 17:32:05 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 2, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 17:32:05 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 17:32:06 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-25 17:32:06 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 17:32:06 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.39294958114624023s\n",
      "04-25 17:32:06 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-25 17:32:07 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5510420799255371s\n",
      "04-25 17:32:07 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 17:32:07 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.2847447395324707s\n",
      "04-25 17:32:07 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.450648546218872s\n",
      "04-25 17:32:07 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 17:32:07 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13862943649291992s\n",
      "04-25 17:32:07 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 17:32:07 I deeptables.m.preprocessor.py 236 - transform_y taken 0.001995086669921875s\n",
      "04-25 17:32:07 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 17:32:07 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 17:32:07 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:32:07 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:32:07 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 17:32:08 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 17:32:08 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9421 - auc: 0.7179 - accuracy: 0.4397 - val_loss: 1.0253 - val_auc: 0.8028 - val_accuracy: 0.5380\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.4317 - auc: 0.8567 - accuracy: 0.6098 - val_loss: 0.9632 - val_auc: 0.8306 - val_accuracy: 0.5606\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.0828 - auc: 0.9151 - accuracy: 0.7179 - val_loss: 0.9508 - val_auc: 0.8491 - val_accuracy: 0.5964\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.8171 - auc: 0.9483 - accuracy: 0.7807 - val_loss: 0.9745 - val_auc: 0.8578 - val_accuracy: 0.6141\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.6636 - auc: 0.9639 - accuracy: 0.8142 - val_loss: 0.9876 - val_auc: 0.8663 - val_accuracy: 0.6303\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.5705 - auc: 0.9723 - accuracy: 0.8369 - val_loss: 1.0183 - val_auc: 0.8692 - val_accuracy: 0.6412\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5085 - auc: 0.9776 - accuracy: 0.8533 - val_loss: 1.0323 - val_auc: 0.8727 - val_accuracy: 0.6504\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4583 - auc: 0.9814 - accuracy: 0.8679 - val_loss: 1.0522 - val_auc: 0.8752 - val_accuracy: 0.6588\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4196 - auc: 0.9843 - accuracy: 0.8787 - val_loss: 1.0593 - val_auc: 0.8786 - val_accuracy: 0.6664\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3859 - auc: 0.9866 - accuracy: 0.8879 - val_loss: 1.0871 - val_auc: 0.8781 - val_accuracy: 0.6705\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3602 - auc: 0.9883 - accuracy: 0.8952 - val_loss: 1.1057 - val_auc: 0.8789 - val_accuracy: 0.6709\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3369 - auc: 0.9898 - accuracy: 0.9021 - val_loss: 1.1315 - val_auc: 0.8776 - val_accuracy: 0.6726\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3171 - auc: 0.9910 - accuracy: 0.9072 - val_loss: 1.1313 - val_auc: 0.8814 - val_accuracy: 0.6810\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 45ms/step - loss: 0.3020 - auc: 0.9918 - accuracy: 0.9112 - val_loss: 1.1694 - val_auc: 0.8793 - val_accuracy: 0.6813\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 13s 46ms/step - loss: 0.2883 - auc: 0.9925 - accuracy: 0.9140 - val_loss: 1.1743 - val_auc: 0.8803 - val_accuracy: 0.6805\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 45ms/step - loss: 0.2768 - auc: 0.9931 - accuracy: 0.9175 - val_loss: 1.1874 - val_auc: 0.8818 - val_accuracy: 0.6837\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2673 - auc: 0.9936 - accuracy: 0.9197 - val_loss: 1.2053 - val_auc: 0.8811 - val_accuracy: 0.6837\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2568 - auc: 0.9941 - accuracy: 0.9224 - val_loss: 1.2130 - val_auc: 0.8817 - val_accuracy: 0.6862\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2487 - auc: 0.9945 - accuracy: 0.9239 - val_loss: 1.2298 - val_auc: 0.8817 - val_accuracy: 0.6882\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2409 - auc: 0.9947 - accuracy: 0.9257 - val_loss: 1.2430 - val_auc: 0.8811 - val_accuracy: 0.6882\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 13s 46ms/step - loss: 0.2355 - auc: 0.9950 - accuracy: 0.9276 - val_loss: 1.2676 - val_auc: 0.8803 - val_accuracy: 0.6862\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n",
      "04-25 17:36:23 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 17:36:23 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 17:36:24 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425173205_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.942138433456421, 1.43169367313385, 1.0828399658203125, 0.817134439945221, 0.6636044979095459, 0.5705187320709229, 0.5084580779075623, 0.45829886198043823, 0.4196421802043915, 0.38594725728034973, 0.3602498769760132, 0.3369480073451996, 0.3171458840370178, 0.30195102095603943, 0.28827807307243347, 0.27679890394210815, 0.2672945261001587, 0.25680506229400635, 0.24868543446063995, 0.2409198135137558, 0.2354501634836197], 'auc': [0.7178618311882019, 0.8567324280738831, 0.9150904417037964, 0.948275625705719, 0.9638579487800598, 0.9723019003868103, 0.9775736331939697, 0.9813877940177917, 0.9842868447303772, 0.9866214394569397, 0.9883403778076172, 0.9897977113723755, 0.9909737706184387, 0.9918291568756104, 0.9925468564033508, 0.9931477904319763, 0.9936032891273499, 0.9941120147705078, 0.9944717884063721, 0.9947378635406494, 0.9950181841850281], 'accuracy': [0.4396752417087555, 0.6098121404647827, 0.7179250121116638, 0.780682384967804, 0.8141710758209229, 0.836889386177063, 0.8533357977867126, 0.8679137229919434, 0.8787088394165039, 0.8878927230834961, 0.8952152132987976, 0.9021040201187134, 0.9072477221488953, 0.9112421274185181, 0.9140272736549377, 0.9175280332565308, 0.9196574687957764, 0.9223614931106567, 0.9239303469657898, 0.9257460236549377, 0.9276109933853149], 'val_loss': [1.0252586603164673, 0.9631866812705994, 0.9507917165756226, 0.9745274186134338, 0.987581193447113, 1.0183281898498535, 1.0322761535644531, 1.0521540641784668, 1.0592519044876099, 1.0871355533599854, 1.1057143211364746, 1.1315419673919678, 1.1312755346298218, 1.1694045066833496, 1.174282431602478, 1.1873582601547241, 1.205270767211914, 1.2129782438278198, 1.2297693490982056, 1.2429742813110352, 1.2676353454589844], 'val_auc': [0.8027597665786743, 0.8306439518928528, 0.8490611910820007, 0.8577820062637329, 0.8663243055343628, 0.8691927790641785, 0.8726925849914551, 0.8752142190933228, 0.8786455392837524, 0.8781236410140991, 0.8788681626319885, 0.8776342868804932, 0.8813931941986084, 0.8793383836746216, 0.8802745342254639, 0.8817909359931946, 0.8810649514198303, 0.8817048668861389, 0.8816937804222107, 0.8810800313949585, 0.8803372979164124], 'val_accuracy': [0.538026750087738, 0.5606356263160706, 0.5964429378509521, 0.6141394376754761, 0.6303266882896423, 0.6412464380264282, 0.6503610610961914, 0.6587653756141663, 0.6663707494735718, 0.6704841256141663, 0.6709280014038086, 0.6725852489471436, 0.6809895634651184, 0.6813151240348816, 0.6804865002632141, 0.6837416887283325, 0.6836825013160706, 0.6861979365348816, 0.6881806254386902, 0.6882102489471436, 0.6861979365348816]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 2, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 17:36:24 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 2, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 17:36:24 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 17:36:24 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-25 17:36:24 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 17:36:25 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.439835786819458s\n",
      "04-25 17:36:25 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-25 17:36:26 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5884609222412109s\n",
      "04-25 17:36:26 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 17:36:26 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.32514357566833496s\n",
      "04-25 17:36:26 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.6007790565490723s\n",
      "04-25 17:36:26 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 17:36:26 I deeptables.m.preprocessor.py 249 - transform_X taken 0.1436150074005127s\n",
      "04-25 17:36:26 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 17:36:26 I deeptables.m.preprocessor.py 236 - transform_y taken 0.001995086669921875s\n",
      "04-25 17:36:26 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 17:36:26 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 17:36:26 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:36:26 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:36:26 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 17:36:27 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 17:36:27 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9457 - auc: 0.7186 - accuracy: 0.4736 - val_loss: 1.0430 - val_auc: 0.7930 - val_accuracy: 0.5191\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 13s 48ms/step - loss: 1.4239 - auc: 0.8584 - accuracy: 0.6126 - val_loss: 0.9321 - val_auc: 0.8418 - val_accuracy: 0.5747\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 13s 48ms/step - loss: 1.0796 - auc: 0.9158 - accuracy: 0.7179 - val_loss: 0.9378 - val_auc: 0.8542 - val_accuracy: 0.5995\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.8167 - auc: 0.9480 - accuracy: 0.7814 - val_loss: 0.9616 - val_auc: 0.8632 - val_accuracy: 0.6212\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 13s 48ms/step - loss: 0.6656 - auc: 0.9636 - accuracy: 0.8148 - val_loss: 1.0013 - val_auc: 0.8680 - val_accuracy: 0.6342\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 13s 47ms/step - loss: 0.5716 - auc: 0.9720 - accuracy: 0.8379 - val_loss: 1.0162 - val_auc: 0.8722 - val_accuracy: 0.6571\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 13s 46ms/step - loss: 0.5117 - auc: 0.9771 - accuracy: 0.8537 - val_loss: 1.0514 - val_auc: 0.8728 - val_accuracy: 0.6642\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.4659 - auc: 0.9807 - accuracy: 0.8662 - val_loss: 1.0696 - val_auc: 0.8750 - val_accuracy: 0.6581\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 13s 46ms/step - loss: 0.4276 - auc: 0.9836 - accuracy: 0.8770 - val_loss: 1.1019 - val_auc: 0.8750 - val_accuracy: 0.6754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.3959 - auc: 0.9859 - accuracy: 0.8859 - val_loss: 1.1050 - val_auc: 0.8783 - val_accuracy: 0.6830\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3715 - auc: 0.9875 - accuracy: 0.8931 - val_loss: 1.1270 - val_auc: 0.8785 - val_accuracy: 0.6847\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.3492 - auc: 0.9890 - accuracy: 0.8994 - val_loss: 1.1486 - val_auc: 0.8788 - val_accuracy: 0.6871\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.3299 - auc: 0.9902 - accuracy: 0.9038 - val_loss: 1.1683 - val_auc: 0.8792 - val_accuracy: 0.6805\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3142 - auc: 0.9911 - accuracy: 0.9083 - val_loss: 1.1790 - val_auc: 0.8812 - val_accuracy: 0.6961\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 13s 47ms/step - loss: 0.2975 - auc: 0.9919 - accuracy: 0.9128 - val_loss: 1.2158 - val_auc: 0.8792 - val_accuracy: 0.6956\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2863 - auc: 0.9926 - accuracy: 0.9154 - val_loss: 1.2268 - val_auc: 0.8799 - val_accuracy: 0.6978\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2755 - auc: 0.9931 - accuracy: 0.9182 - val_loss: 1.2264 - val_auc: 0.8811 - val_accuracy: 0.6878\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.2633 - auc: 0.9938 - accuracy: 0.9213 - val_loss: 1.2451 - val_auc: 0.8812 - val_accuracy: 0.7026\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2551 - auc: 0.9941 - accuracy: 0.9227 - val_loss: 1.2740 - val_auc: 0.8795 - val_accuracy: 0.6876\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "04-25 17:40:27 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 17:40:27 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 17:40:27 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425173624_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9457306861877441, 1.4239405393600464, 1.0796034336090088, 0.8167303800582886, 0.6656413078308105, 0.5716043710708618, 0.5116632580757141, 0.4658733606338501, 0.4276120364665985, 0.39590850472450256, 0.3714900314807892, 0.34917640686035156, 0.32985904812812805, 0.31416869163513184, 0.29746848344802856, 0.2862658202648163, 0.27549147605895996, 0.26325997710227966, 0.2551422119140625], 'auc': [0.7185965180397034, 0.8584099411964417, 0.9158390164375305, 0.9479657411575317, 0.963558554649353, 0.9719637632369995, 0.9771268963813782, 0.9807422757148743, 0.9836371541023254, 0.9858866333961487, 0.9875253438949585, 0.9889518022537231, 0.9901586771011353, 0.9910804629325867, 0.9919094443321228, 0.9925646781921387, 0.9931269288063049, 0.993783175945282, 0.994121789932251], 'accuracy': [0.47355523705482483, 0.6126114130020142, 0.7179179787635803, 0.7813522219657898, 0.8148268461227417, 0.8379047513008118, 0.8536848425865173, 0.8662039041519165, 0.8770059943199158, 0.8859008550643921, 0.8930646181106567, 0.8994457721710205, 0.903827965259552, 0.9082701206207275, 0.9127756953239441, 0.9153669476509094, 0.9181556105613708, 0.9212968349456787, 0.9227140545845032], 'val_loss': [1.0430399179458618, 0.9321073889732361, 0.9377866983413696, 0.9616040587425232, 1.0012729167938232, 1.016186237335205, 1.051412582397461, 1.0695736408233643, 1.1019452810287476, 1.104972243309021, 1.12698233127594, 1.1485627889633179, 1.168338418006897, 1.1789755821228027, 1.2158395051956177, 1.226824402809143, 1.2264209985733032, 1.2450979948043823, 1.2740355730056763], 'val_auc': [0.7930188775062561, 0.8417797088623047, 0.854200005531311, 0.8632216453552246, 0.8679530024528503, 0.8722019791603088, 0.8728376030921936, 0.874994695186615, 0.8749722242355347, 0.8782662153244019, 0.8784857988357544, 0.8787598013877869, 0.8791607022285461, 0.8812010884284973, 0.8791934251785278, 0.8798876404762268, 0.8810755610466003, 0.8811962008476257, 0.8794708847999573], 'val_accuracy': [0.5190577507019043, 0.5746626257896423, 0.5995206236839294, 0.621152937412262, 0.6342033743858337, 0.6571378111839294, 0.6642104387283325, 0.6580551862716675, 0.6753965616226196, 0.6829723119735718, 0.6847478747367859, 0.6871153116226196, 0.6804569363594055, 0.6960819363594055, 0.6956084370613098, 0.6977686882019043, 0.6877959370613098, 0.7025627493858337, 0.6876479387283325]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 5, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 17:40:28 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 5, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 17:40:28 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 17:40:28 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 17:40:28 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 17:40:28 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.3874702453613281s\n",
      "04-25 17:40:28 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-25 17:40:29 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5131328105926514s\n",
      "04-25 17:40:29 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 17:40:29 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.3047027587890625s\n",
      "04-25 17:40:29 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.4376850128173828s\n",
      "04-25 17:40:29 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 17:40:30 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13464021682739258s\n",
      "04-25 17:40:30 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 17:40:30 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0010004043579101562s\n",
      "04-25 17:40:30 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 17:40:30 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 17:40:30 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:40:30 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:40:30 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 17:40:30 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 17:40:30 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9376 - auc: 0.7216 - accuracy: 0.4803 - val_loss: 1.0185 - val_auc: 0.8064 - val_accuracy: 0.5487\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.3624 - auc: 0.8718 - accuracy: 0.6352 - val_loss: 0.9285 - val_auc: 0.8422 - val_accuracy: 0.5880\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.9710 - auc: 0.9310 - accuracy: 0.7502 - val_loss: 0.8938 - val_auc: 0.8610 - val_accuracy: 0.6117\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.7164 - auc: 0.9589 - accuracy: 0.8067 - val_loss: 0.8887 - val_auc: 0.8713 - val_accuracy: 0.6323\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 13s 45ms/step - loss: 0.5734 - auc: 0.9722 - accuracy: 0.8394 - val_loss: 0.8964 - val_auc: 0.8773 - val_accuracy: 0.6608\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.4857 - auc: 0.9794 - accuracy: 0.8643 - val_loss: 0.9023 - val_auc: 0.8830 - val_accuracy: 0.6720\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 13s 45ms/step - loss: 0.4258 - auc: 0.9841 - accuracy: 0.8806 - val_loss: 0.9061 - val_auc: 0.8866 - val_accuracy: 0.6818\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 13s 47ms/step - loss: 0.3769 - auc: 0.9873 - accuracy: 0.8933 - val_loss: 0.9134 - val_auc: 0.8909 - val_accuracy: 0.6929\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 13s 48ms/step - loss: 0.3407 - auc: 0.9896 - accuracy: 0.9029 - val_loss: 0.9351 - val_auc: 0.8928 - val_accuracy: 0.6946\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3127 - auc: 0.9913 - accuracy: 0.9100 - val_loss: 0.9304 - val_auc: 0.8944 - val_accuracy: 0.6971\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2888 - auc: 0.9925 - accuracy: 0.9155 - val_loss: 0.9549 - val_auc: 0.8951 - val_accuracy: 0.6987\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2690 - auc: 0.9934 - accuracy: 0.9197 - val_loss: 0.9787 - val_auc: 0.8957 - val_accuracy: 0.7016\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2570 - auc: 0.9940 - accuracy: 0.9232 - val_loss: 0.9849 - val_auc: 0.8967 - val_accuracy: 0.7076\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2443 - auc: 0.9946 - accuracy: 0.9261 - val_loss: 1.0077 - val_auc: 0.8963 - val_accuracy: 0.7051\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2338 - auc: 0.9950 - accuracy: 0.9283 - val_loss: 1.0146 - val_auc: 0.8970 - val_accuracy: 0.7064\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.2260 - auc: 0.9954 - accuracy: 0.9300 - val_loss: 1.0324 - val_auc: 0.8969 - val_accuracy: 0.7061\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2204 - auc: 0.9955 - accuracy: 0.9315 - val_loss: 1.0513 - val_auc: 0.8959 - val_accuracy: 0.7131\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2141 - auc: 0.9959 - accuracy: 0.9330 - val_loss: 1.0666 - val_auc: 0.8949 - val_accuracy: 0.7111\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2082 - auc: 0.9961 - accuracy: 0.9340 - val_loss: 1.0607 - val_auc: 0.8967 - val_accuracy: 0.7129\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.2055 - auc: 0.9962 - accuracy: 0.9344 - val_loss: 1.0878 - val_auc: 0.8952 - val_accuracy: 0.7125\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "04-25 17:44:37 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 17:44:37 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 17:44:37 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425174028_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9376215934753418, 1.362436294555664, 0.9710330963134766, 0.7163934111595154, 0.5733588933944702, 0.4857149124145508, 0.4258415699005127, 0.37685760855674744, 0.34066957235336304, 0.3126586973667145, 0.28879931569099426, 0.2690398693084717, 0.25703123211860657, 0.2443312704563141, 0.23376889526844025, 0.2260332852602005, 0.22043061256408691, 0.21412453055381775, 0.20817983150482178, 0.20554454624652863], 'auc': [0.7216423153877258, 0.8717650771141052, 0.9310144186019897, 0.9588789939880371, 0.9722204804420471, 0.9794354438781738, 0.9840521812438965, 0.9872968196868896, 0.989623486995697, 0.9912517666816711, 0.9924526810646057, 0.9934139847755432, 0.994025707244873, 0.9945825934410095, 0.9950432181358337, 0.9953649044036865, 0.9955474734306335, 0.9958668947219849, 0.9960980415344238, 0.9961720108985901], 'accuracy': [0.48026779294013977, 0.6352310180664062, 0.7501938939094543, 0.8067041039466858, 0.839448869228363, 0.8642613291740417, 0.8805773258209229, 0.8933290839195251, 0.9029395580291748, 0.9100010991096497, 0.915522038936615, 0.9197491407394409, 0.9232181906700134, 0.9261020421981812, 0.9283125400543213, 0.9300082921981812, 0.9315242767333984, 0.9330155849456787, 0.9340485334396362, 0.9343517422676086], 'val_loss': [1.0185143947601318, 0.9285488128662109, 0.8938292264938354, 0.8886855840682983, 0.8964289426803589, 0.9022743701934814, 0.9061319828033447, 0.9134172201156616, 0.9350671172142029, 0.930408239364624, 0.9549064040184021, 0.9786555171012878, 0.9848507046699524, 1.007690668106079, 1.014573335647583, 1.0323702096939087, 1.0512770414352417, 1.0666307210922241, 1.0607450008392334, 1.0878223180770874], 'val_auc': [0.8063886761665344, 0.842249870300293, 0.8610295057296753, 0.8712857961654663, 0.8772955536842346, 0.8830427527427673, 0.8866442441940308, 0.8909260034561157, 0.8927746415138245, 0.894440233707428, 0.8950821161270142, 0.8956851363182068, 0.8966588377952576, 0.8963449597358704, 0.8970466256141663, 0.8969317674636841, 0.8958601951599121, 0.894923746585846, 0.8966556787490845, 0.8952421545982361], 'val_accuracy': [0.5486505627632141, 0.5880089998245239, 0.6116536259651184, 0.6323390007019043, 0.6607776880264282, 0.6719637513160706, 0.6818477511405945, 0.6929154992103577, 0.6945726871490479, 0.6971472501754761, 0.6986860632896423, 0.7016157507896423, 0.7076231241226196, 0.705078125, 0.7063506245613098, 0.7061434388160706, 0.7130681872367859, 0.7110558748245239, 0.7129201889038086, 0.7124763131141663]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 2, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 17:44:37 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 2, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 17:44:37 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 17:44:38 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 17:44:39 I deeptables.m.preprocessor.py 249 - transform_X taken 0.9524850845336914s\n",
      "04-25 17:44:39 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 17:44:39 I deeptables.m.preprocessor.py 236 - transform_y taken 0.007978677749633789s\n",
      "04-25 17:44:39 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 17:44:39 I deeptables.m.preprocessor.py 249 - transform_X taken 0.16158199310302734s\n",
      "04-25 17:44:39 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 17:44:39 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009968280792236328s\n",
      "04-25 17:44:39 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 17:44:39 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 17:44:39 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:44:39 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:44:39 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 17:44:40 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 17:44:40 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 16s 46ms/step - loss: 1.9283 - auc: 0.7247 - accuracy: 0.4828 - val_loss: 1.0300 - val_auc: 0.8021 - val_accuracy: 0.5415\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 1.4036 - auc: 0.8622 - accuracy: 0.6176 - val_loss: 0.9497 - val_auc: 0.8344 - val_accuracy: 0.5669\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 1.0046 - auc: 0.9256 - accuracy: 0.7385 - val_loss: 0.8899 - val_auc: 0.8617 - val_accuracy: 0.6130\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.7145 - auc: 0.9590 - accuracy: 0.8076 - val_loss: 0.8863 - val_auc: 0.8718 - val_accuracy: 0.6405\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.5612 - auc: 0.9733 - accuracy: 0.8448 - val_loss: 0.8960 - val_auc: 0.8787 - val_accuracy: 0.6567\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 11s 41ms/step - loss: 0.4696 - auc: 0.9809 - accuracy: 0.8698 - val_loss: 0.9064 - val_auc: 0.8828 - val_accuracy: 0.6684\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4052 - auc: 0.9856 - accuracy: 0.8869 - val_loss: 0.9128 - val_auc: 0.8871 - val_accuracy: 0.6759\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3566 - auc: 0.9886 - accuracy: 0.8993 - val_loss: 0.9175 - val_auc: 0.8908 - val_accuracy: 0.6845\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.3199 - auc: 0.9908 - accuracy: 0.9089 - val_loss: 0.9255 - val_auc: 0.8937 - val_accuracy: 0.6891\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.2943 - auc: 0.9922 - accuracy: 0.9151 - val_loss: 0.9509 - val_auc: 0.8941 - val_accuracy: 0.6934\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 11s 41ms/step - loss: 0.2725 - auc: 0.9933 - accuracy: 0.9208 - val_loss: 0.9509 - val_auc: 0.8958 - val_accuracy: 0.6979\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2557 - auc: 0.9941 - accuracy: 0.9245 - val_loss: 0.9656 - val_auc: 0.8973 - val_accuracy: 0.6978\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2426 - auc: 0.9947 - accuracy: 0.9278 - val_loss: 0.9927 - val_auc: 0.8952 - val_accuracy: 0.6953\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2346 - auc: 0.9951 - accuracy: 0.9291 - val_loss: 0.9904 - val_auc: 0.8962 - val_accuracy: 0.6977\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 11s 40ms/step - loss: 0.2283 - auc: 0.9953 - accuracy: 0.9308 - val_loss: 1.0087 - val_auc: 0.8967 - val_accuracy: 0.6984\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 11s 40ms/step - loss: 0.2196 - auc: 0.9956 - accuracy: 0.9327 - val_loss: 1.0040 - val_auc: 0.8969 - val_accuracy: 0.6990\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 11s 41ms/step - loss: 0.2131 - auc: 0.9958 - accuracy: 0.9337 - val_loss: 1.0150 - val_auc: 0.8975 - val_accuracy: 0.7022\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 11s 41ms/step - loss: 0.2109 - auc: 0.9960 - accuracy: 0.9340 - val_loss: 1.0105 - val_auc: 0.8985 - val_accuracy: 0.7021\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2056 - auc: 0.9961 - accuracy: 0.9354 - val_loss: 1.0163 - val_auc: 0.8983 - val_accuracy: 0.7033\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2023 - auc: 0.9963 - accuracy: 0.9364 - val_loss: 1.0396 - val_auc: 0.8967 - val_accuracy: 0.7018\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 11s 40ms/step - loss: 0.2001 - auc: 0.9964 - accuracy: 0.9365 - val_loss: 1.0354 - val_auc: 0.8970 - val_accuracy: 0.7019\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 11s 41ms/step - loss: 0.1964 - auc: 0.9965 - accuracy: 0.9369 - val_loss: 1.0431 - val_auc: 0.8963 - val_accuracy: 0.7012\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1933 - auc: 0.9966 - accuracy: 0.9377 - val_loss: 1.0504 - val_auc: 0.8961 - val_accuracy: 0.7029\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "04-25 17:49:12 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 17:49:12 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 17:49:13 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425174437_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9283299446105957, 1.4035913944244385, 1.004620909690857, 0.7144791483879089, 0.5611598491668701, 0.46958184242248535, 0.40518486499786377, 0.35664132237434387, 0.31991487741470337, 0.2943463623523712, 0.2724589705467224, 0.25566554069519043, 0.242550328373909, 0.23455768823623657, 0.22831952571868896, 0.21957479417324066, 0.21314139664173126, 0.2108602672815323, 0.20561468601226807, 0.20232068002223969, 0.20010797679424286, 0.19638755917549133, 0.1932658553123474], 'auc': [0.7246879935264587, 0.8621730208396912, 0.9256259202957153, 0.9590386748313904, 0.9733457565307617, 0.9808820486068726, 0.9855557084083557, 0.988645613193512, 0.9908338785171509, 0.9922094345092773, 0.993291974067688, 0.9940786957740784, 0.9946757555007935, 0.9950655698776245, 0.995319664478302, 0.9956293702125549, 0.9958295226097107, 0.9959673285484314, 0.9961339235305786, 0.9963099360466003, 0.9963908791542053, 0.9964683055877686, 0.9966049194335938], 'accuracy': [0.4828449487686157, 0.617582380771637, 0.7384998202323914, 0.8076453804969788, 0.8448464274406433, 0.8697999119758606, 0.8868562579154968, 0.8992977142333984, 0.9089223146438599, 0.9151412844657898, 0.9208208918571472, 0.9244838953018188, 0.9277520179748535, 0.9291093349456787, 0.9308297634124756, 0.9326841831207275, 0.9336959719657898, 0.9340344667434692, 0.9354305267333984, 0.9364000558853149, 0.9365375638008118, 0.9369394183158875, 0.9377432465553284], 'val_loss': [1.0300184488296509, 0.9497451782226562, 0.8898757100105286, 0.8863360285758972, 0.8960143327713013, 0.9064291715621948, 0.9127727150917053, 0.9174952507019043, 0.925482451915741, 0.9509122967720032, 0.9508516192436218, 0.9655975103378296, 0.9927269816398621, 0.9903591275215149, 1.0087090730667114, 1.004045844078064, 1.0149697065353394, 1.0104525089263916, 1.0162882804870605, 1.0395524501800537, 1.0354145765304565, 1.0431411266326904, 1.0504491329193115], 'val_auc': [0.8021288514137268, 0.834433913230896, 0.8617444634437561, 0.871753990650177, 0.8787137269973755, 0.8828386068344116, 0.8871415853500366, 0.8908438682556152, 0.8937109112739563, 0.8940587639808655, 0.8957952260971069, 0.8972756862640381, 0.895163893699646, 0.8962099552154541, 0.8967439532279968, 0.8969035744667053, 0.8975374102592468, 0.8984904885292053, 0.8983259201049805, 0.8967011570930481, 0.8970463871955872, 0.8963098526000977, 0.8961450457572937], 'val_accuracy': [0.5415186882019043, 0.5668501257896423, 0.6130445003509521, 0.6405066251754761, 0.6566938757896423, 0.6684126257896423, 0.6759291887283325, 0.6844815611839294, 0.6890684366226196, 0.6934481263160706, 0.6979462504386902, 0.6978278756141663, 0.6952829360961914, 0.6976503133773804, 0.6983901262283325, 0.6990116238594055, 0.7021780014038086, 0.7020596861839294, 0.7033321261405945, 0.7018229365348816, 0.7018525004386902, 0.701231062412262, 0.7029474377632141]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 6, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 17:49:13 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 6, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 17:49:13 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 17:49:13 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-25 17:49:13 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 17:49:14 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.38099122047424316s\n",
      "04-25 17:49:14 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-25 17:49:14 I deeptables.m.preprocessor.py 383 - Imputation taken 0.5315902233123779s\n",
      "04-25 17:49:14 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 17:49:15 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.2787599563598633s\n",
      "04-25 17:49:15 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.4262356758117676s\n",
      "04-25 17:49:15 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 17:49:15 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13514494895935059s\n",
      "04-25 17:49:15 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 17:49:15 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009970664978027344s\n",
      "04-25 17:49:15 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 17:49:15 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 17:49:15 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:49:15 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:49:15 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 17:49:15 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 17:49:15 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 42ms/step - loss: 1.9449 - auc: 0.7196 - accuracy: 0.4768 - val_loss: 1.0378 - val_auc: 0.7944 - val_accuracy: 0.5054\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 1.4400 - auc: 0.8552 - accuracy: 0.6068 - val_loss: 0.9541 - val_auc: 0.8342 - val_accuracy: 0.5682\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 11s 41ms/step - loss: 1.1086 - auc: 0.9111 - accuracy: 0.7081 - val_loss: 0.9528 - val_auc: 0.8487 - val_accuracy: 0.5942\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 11s 41ms/step - loss: 0.8327 - auc: 0.9461 - accuracy: 0.7762 - val_loss: 0.9794 - val_auc: 0.8596 - val_accuracy: 0.6200\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.6564 - auc: 0.9650 - accuracy: 0.8186 - val_loss: 0.9911 - val_auc: 0.8709 - val_accuracy: 0.6408\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.5322 - auc: 0.9762 - accuracy: 0.8509 - val_loss: 1.0248 - val_auc: 0.8771 - val_accuracy: 0.6550\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4478 - auc: 0.9826 - accuracy: 0.8739 - val_loss: 1.0468 - val_auc: 0.8820 - val_accuracy: 0.6680\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3880 - auc: 0.9867 - accuracy: 0.8891 - val_loss: 1.0777 - val_auc: 0.8849 - val_accuracy: 0.6768\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.3451 - auc: 0.9893 - accuracy: 0.9013 - val_loss: 1.1103 - val_auc: 0.8867 - val_accuracy: 0.6834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3138 - auc: 0.9911 - accuracy: 0.9085 - val_loss: 1.1216 - val_auc: 0.8883 - val_accuracy: 0.7013\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.2874 - auc: 0.9925 - accuracy: 0.9153 - val_loss: 1.1574 - val_auc: 0.8884 - val_accuracy: 0.6886\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.2697 - auc: 0.9934 - accuracy: 0.9198 - val_loss: 1.1668 - val_auc: 0.8886 - val_accuracy: 0.7031\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2537 - auc: 0.9942 - accuracy: 0.9238 - val_loss: 1.1975 - val_auc: 0.8878 - val_accuracy: 0.7039\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2451 - auc: 0.9945 - accuracy: 0.9260 - val_loss: 1.2055 - val_auc: 0.8884 - val_accuracy: 0.6927\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2343 - auc: 0.9950 - accuracy: 0.9289 - val_loss: 1.2234 - val_auc: 0.8881 - val_accuracy: 0.7058\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 11s 40ms/step - loss: 0.2273 - auc: 0.9953 - accuracy: 0.9306 - val_loss: 1.2229 - val_auc: 0.8889 - val_accuracy: 0.6940\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2203 - auc: 0.9956 - accuracy: 0.9316 - val_loss: 1.2450 - val_auc: 0.8880 - val_accuracy: 0.6919\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2139 - auc: 0.9959 - accuracy: 0.9332 - val_loss: 1.2393 - val_auc: 0.8899 - val_accuracy: 0.7054\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 11s 41ms/step - loss: 0.2089 - auc: 0.9960 - accuracy: 0.9344 - val_loss: 1.2566 - val_auc: 0.8886 - val_accuracy: 0.6937\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2058 - auc: 0.9961 - accuracy: 0.9352 - val_loss: 1.2589 - val_auc: 0.8893 - val_accuracy: 0.6966\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 11s 41ms/step - loss: 0.2011 - auc: 0.9964 - accuracy: 0.9364 - val_loss: 1.2707 - val_auc: 0.8877 - val_accuracy: 0.6940\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 11s 41ms/step - loss: 0.1988 - auc: 0.9964 - accuracy: 0.9366 - val_loss: 1.2810 - val_auc: 0.8875 - val_accuracy: 0.6911\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 11s 41ms/step - loss: 0.1954 - auc: 0.9965 - accuracy: 0.9372 - val_loss: 1.2847 - val_auc: 0.8877 - val_accuracy: 0.6924\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00023: early stopping\n",
      "04-25 17:53:48 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 17:53:48 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 17:53:49 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425174913_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9448903799057007, 1.440016269683838, 1.1086435317993164, 0.8326554894447327, 0.6563522219657898, 0.5321531295776367, 0.4478127360343933, 0.387961745262146, 0.3450700640678406, 0.31378936767578125, 0.28736433386802673, 0.2696692645549774, 0.25369158387184143, 0.2450907975435257, 0.2342735081911087, 0.22733543813228607, 0.2203458994626999, 0.21394230425357819, 0.20886312425136566, 0.20577430725097656, 0.20114564895629883, 0.19881954789161682, 0.1953515112400055], 'auc': [0.7195907831192017, 0.8552151918411255, 0.9111006259918213, 0.9461124539375305, 0.9650100469589233, 0.9762483239173889, 0.9826472997665405, 0.9867441654205322, 0.9893410801887512, 0.9910929799079895, 0.9924819469451904, 0.9934171438217163, 0.9941600561141968, 0.9945141673088074, 0.9950254559516907, 0.9952907562255859, 0.9955659508705139, 0.9958522915840149, 0.9960142970085144, 0.9961234331130981, 0.9963520765304565, 0.9963874816894531, 0.9965323805809021], 'accuracy': [0.4767987132072449, 0.6068225502967834, 0.7081276774406433, 0.7762367725372314, 0.8186132311820984, 0.8508538603782654, 0.8739070892333984, 0.8891231417655945, 0.901317834854126, 0.9084957242012024, 0.9152576327323914, 0.9197667241096497, 0.9237752556800842, 0.9259786605834961, 0.9289295077323914, 0.9306217432022095, 0.9316089153289795, 0.9332200288772583, 0.9344469308853149, 0.9352049231529236, 0.9364070892333984, 0.9366009831428528, 0.9372285604476929], 'val_loss': [1.0377768278121948, 0.9540895819664001, 0.9527849555015564, 0.979449987411499, 0.9910877346992493, 1.024753212928772, 1.0468465089797974, 1.0776515007019043, 1.1103414297103882, 1.1215519905090332, 1.1574146747589111, 1.1667957305908203, 1.197500467300415, 1.2055084705352783, 1.2233858108520508, 1.2229056358337402, 1.2450034618377686, 1.2392634153366089, 1.2565597295761108, 1.2588878870010376, 1.2707226276397705, 1.281044602394104, 1.2847247123718262], 'val_auc': [0.7944353222846985, 0.8342454433441162, 0.8487317562103271, 0.8596471548080444, 0.8709226250648499, 0.877134382724762, 0.8820318579673767, 0.8849129676818848, 0.8866647481918335, 0.8882829546928406, 0.8884149789810181, 0.8886093497276306, 0.887782633304596, 0.888403058052063, 0.8880809545516968, 0.8889392614364624, 0.8880211710929871, 0.8899095058441162, 0.8886412978172302, 0.8892897963523865, 0.8877198100090027, 0.8875329494476318, 0.8876779675483704], 'val_accuracy': [0.5054154992103577, 0.5682410001754761, 0.5942234992980957, 0.6199988126754761, 0.6408321261405945, 0.6549775004386902, 0.66796875, 0.6767578125, 0.6833866238594055, 0.7013494372367859, 0.6886245012283325, 0.7030954360961914, 0.7039240002632141, 0.6927379369735718, 0.7057587504386902, 0.6940400004386902, 0.6919389367103577, 0.7054036259651184, 0.6937440633773804, 0.6965553760528564, 0.6939808130264282, 0.6910511255264282, 0.6923532485961914]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 6, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 17:53:49 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 6, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 17:53:49 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 17:53:50 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 17:53:51 I deeptables.m.preprocessor.py 249 - transform_X taken 0.9828774929046631s\n",
      "04-25 17:53:51 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 17:53:51 I deeptables.m.preprocessor.py 236 - transform_y taken 0.006981372833251953s\n",
      "04-25 17:53:51 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 17:53:51 I deeptables.m.preprocessor.py 249 - transform_X taken 0.14960169792175293s\n",
      "04-25 17:53:51 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 17:53:51 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0019953250885009766s\n",
      "04-25 17:53:51 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 17:53:51 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 17:53:51 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:53:51 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 17:53:51 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 17:53:51 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 17:53:51 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 14s 43ms/step - loss: 1.9461 - auc: 0.7197 - accuracy: 0.4811 - val_loss: 1.0257 - val_auc: 0.8038 - val_accuracy: 0.5536\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 11s 41ms/step - loss: 1.4346 - auc: 0.8570 - accuracy: 0.6104 - val_loss: 0.9621 - val_auc: 0.8315 - val_accuracy: 0.5603\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 11s 40ms/step - loss: 1.0886 - auc: 0.9143 - accuracy: 0.7160 - val_loss: 0.9355 - val_auc: 0.8542 - val_accuracy: 0.5967\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 11s 40ms/step - loss: 0.8224 - auc: 0.9474 - accuracy: 0.7810 - val_loss: 0.9769 - val_auc: 0.8602 - val_accuracy: 0.6179\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 11s 41ms/step - loss: 0.6744 - auc: 0.9629 - accuracy: 0.8130 - val_loss: 0.9969 - val_auc: 0.8679 - val_accuracy: 0.6323\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5821 - auc: 0.9715 - accuracy: 0.8351 - val_loss: 1.0212 - val_auc: 0.8716 - val_accuracy: 0.6448\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5207 - auc: 0.9767 - accuracy: 0.8524 - val_loss: 1.0401 - val_auc: 0.8735 - val_accuracy: 0.6507\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4712 - auc: 0.9805 - accuracy: 0.8661 - val_loss: 1.0726 - val_auc: 0.8745 - val_accuracy: 0.6550\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4288 - auc: 0.9837 - accuracy: 0.8776 - val_loss: 1.0881 - val_auc: 0.8763 - val_accuracy: 0.6633\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3953 - auc: 0.9860 - accuracy: 0.8869 - val_loss: 1.0995 - val_auc: 0.8788 - val_accuracy: 0.6699\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3674 - auc: 0.9879 - accuracy: 0.8947 - val_loss: 1.1290 - val_auc: 0.8789 - val_accuracy: 0.6849\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3438 - auc: 0.9894 - accuracy: 0.9008 - val_loss: 1.1496 - val_auc: 0.8790 - val_accuracy: 0.6896\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3279 - auc: 0.9904 - accuracy: 0.9053 - val_loss: 1.1546 - val_auc: 0.8804 - val_accuracy: 0.6800\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3088 - auc: 0.9915 - accuracy: 0.9100 - val_loss: 1.1825 - val_auc: 0.8798 - val_accuracy: 0.6833\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2956 - auc: 0.9921 - accuracy: 0.9139 - val_loss: 1.1913 - val_auc: 0.8807 - val_accuracy: 0.6905\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2821 - auc: 0.9929 - accuracy: 0.9168 - val_loss: 1.2064 - val_auc: 0.8818 - val_accuracy: 0.7050\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2707 - auc: 0.9935 - accuracy: 0.9196 - val_loss: 1.2216 - val_auc: 0.8815 - val_accuracy: 0.6951\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2603 - auc: 0.9939 - accuracy: 0.9221 - val_loss: 1.2374 - val_auc: 0.8819 - val_accuracy: 0.7090\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2509 - auc: 0.9944 - accuracy: 0.9241 - val_loss: 1.2494 - val_auc: 0.8825 - val_accuracy: 0.7096\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2439 - auc: 0.9947 - accuracy: 0.9258 - val_loss: 1.2568 - val_auc: 0.8819 - val_accuracy: 0.6972\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2353 - auc: 0.9951 - accuracy: 0.9277 - val_loss: 1.2876 - val_auc: 0.8815 - val_accuracy: 0.7057\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2306 - auc: 0.9952 - accuracy: 0.9289 - val_loss: 1.2894 - val_auc: 0.8829 - val_accuracy: 0.7126\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2244 - auc: 0.9955 - accuracy: 0.9302 - val_loss: 1.3134 - val_auc: 0.8816 - val_accuracy: 0.6990\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2202 - auc: 0.9957 - accuracy: 0.9312 - val_loss: 1.3180 - val_auc: 0.8826 - val_accuracy: 0.7067\n",
      "Epoch 25/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2175 - auc: 0.9958 - accuracy: 0.9320 - val_loss: 1.3277 - val_auc: 0.8819 - val_accuracy: 0.6988\n",
      "Epoch 26/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2109 - auc: 0.9960 - accuracy: 0.9330 - val_loss: 1.3246 - val_auc: 0.8831 - val_accuracy: 0.7116\n",
      "Epoch 27/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2082 - auc: 0.9961 - accuracy: 0.9335 - val_loss: 1.3360 - val_auc: 0.8831 - val_accuracy: 0.7141\n",
      "Epoch 28/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2053 - auc: 0.9963 - accuracy: 0.9349 - val_loss: 1.3475 - val_auc: 0.8828 - val_accuracy: 0.7003\n",
      "Epoch 29/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2011 - auc: 0.9964 - accuracy: 0.9353 - val_loss: 1.3532 - val_auc: 0.8828 - val_accuracy: 0.7084\n",
      "Epoch 30/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1990 - auc: 0.9965 - accuracy: 0.9360 - val_loss: 1.3663 - val_auc: 0.8827 - val_accuracy: 0.7138\n",
      "Epoch 31/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1957 - auc: 0.9965 - accuracy: 0.9365 - val_loss: 1.3627 - val_auc: 0.8833 - val_accuracy: 0.7062\n",
      "Epoch 32/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1926 - auc: 0.9966 - accuracy: 0.9368 - val_loss: 1.3887 - val_auc: 0.8825 - val_accuracy: 0.7127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1909 - auc: 0.9967 - accuracy: 0.9373 - val_loss: 1.3966 - val_auc: 0.8821 - val_accuracy: 0.7008\n",
      "Epoch 34/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1889 - auc: 0.9967 - accuracy: 0.9383 - val_loss: 1.3898 - val_auc: 0.8827 - val_accuracy: 0.7003\n",
      "Epoch 35/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1873 - auc: 0.9968 - accuracy: 0.9380 - val_loss: 1.4142 - val_auc: 0.8815 - val_accuracy: 0.6999\n",
      "Epoch 36/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1866 - auc: 0.9968 - accuracy: 0.9379 - val_loss: 1.3969 - val_auc: 0.8827 - val_accuracy: 0.7144\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00036: early stopping\n",
      "04-25 18:00:59 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 18:00:59 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 18:00:59 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425175349_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9461153745651245, 1.4345886707305908, 1.0885848999023438, 0.8224438428878784, 0.6744476556777954, 0.5820678472518921, 0.5206812620162964, 0.4712357521057129, 0.4287910759449005, 0.3953447937965393, 0.36736971139907837, 0.3437637388706207, 0.32789239287376404, 0.3088068664073944, 0.29558250308036804, 0.2820650041103363, 0.270704060792923, 0.2602723240852356, 0.2509377598762512, 0.2438988834619522, 0.2353431135416031, 0.23056328296661377, 0.22444690763950348, 0.22019533812999725, 0.21747413277626038, 0.21093438565731049, 0.20815490186214447, 0.2052837312221527, 0.20107147097587585, 0.19899798929691315, 0.1957404762506485, 0.19261957705020905, 0.1909172683954239, 0.18891721963882446, 0.18728120625019073, 0.1866036057472229], 'auc': [0.7197173237800598, 0.8570467829704285, 0.9142829775810242, 0.9474012851715088, 0.9629075527191162, 0.9714574217796326, 0.9766526818275452, 0.9805310368537903, 0.9836650490760803, 0.9860140681266785, 0.9879060983657837, 0.9893680214881897, 0.9903592467308044, 0.9914880394935608, 0.9921230673789978, 0.9928634166717529, 0.9934678077697754, 0.993925929069519, 0.994381308555603, 0.9946774244308472, 0.9950666427612305, 0.9952417612075806, 0.9955357909202576, 0.9957169890403748, 0.9957959055900574, 0.995992124080658, 0.996117889881134, 0.9962531328201294, 0.9963785409927368, 0.9964562654495239, 0.9965422749519348, 0.9966240525245667, 0.9967229962348938, 0.996748685836792, 0.9968128204345703, 0.996841311454773], 'accuracy': [0.48108571767807007, 0.6104397177696228, 0.7159824967384338, 0.7810243964195251, 0.8130006194114685, 0.8351336717605591, 0.852440357208252, 0.8660522699356079, 0.8775559663772583, 0.8868844509124756, 0.8946934342384338, 0.900810182094574, 0.9052910804748535, 0.9099588394165039, 0.9138932824134827, 0.9167665839195251, 0.9195763468742371, 0.9220830202102661, 0.9240713715553284, 0.9258482456207275, 0.9277202486991882, 0.9288836717605591, 0.9302480816841125, 0.9312422275543213, 0.9319720268249512, 0.9330226182937622, 0.9334668517112732, 0.9348911046981812, 0.9352754354476929, 0.9360122680664062, 0.9364529252052307, 0.9367772936820984, 0.9372919797897339, 0.9383214116096497, 0.938007652759552, 0.9378525614738464], 'val_loss': [1.0257045030593872, 0.9620804190635681, 0.9355029463768005, 0.9769039750099182, 0.9968891739845276, 1.0212293863296509, 1.0401146411895752, 1.0726155042648315, 1.0880978107452393, 1.0994828939437866, 1.1290334463119507, 1.1495710611343384, 1.1546155214309692, 1.1824965476989746, 1.1912630796432495, 1.2064200639724731, 1.2216100692749023, 1.2374060153961182, 1.2493643760681152, 1.256826400756836, 1.2875663042068481, 1.2894028425216675, 1.313425898551941, 1.318037986755371, 1.3276795148849487, 1.3246088027954102, 1.3360227346420288, 1.3475043773651123, 1.353239893913269, 1.3663171529769897, 1.3627408742904663, 1.3886680603027344, 1.3966100215911865, 1.3898104429244995, 1.4142476320266724, 1.3969483375549316], 'val_auc': [0.8038102388381958, 0.8315132856369019, 0.8542243838310242, 0.8601832389831543, 0.8678625226020813, 0.8715943694114685, 0.8735396265983582, 0.8744920492172241, 0.8762669563293457, 0.8788127899169922, 0.8788517117500305, 0.8790461421012878, 0.8804125189781189, 0.8797863721847534, 0.8807281255722046, 0.881841778755188, 0.8814573884010315, 0.8818569183349609, 0.8825076222419739, 0.8819173574447632, 0.8814979791641235, 0.8828651905059814, 0.881598174571991, 0.882634699344635, 0.8819149136543274, 0.8831030130386353, 0.8830735683441162, 0.8827762603759766, 0.8827803134918213, 0.8826737403869629, 0.8833393454551697, 0.8824649453163147, 0.8820902109146118, 0.8826608657836914, 0.8815091848373413, 0.8826932311058044], 'val_accuracy': [0.5535629987716675, 0.5603397488594055, 0.596738874912262, 0.6179273128509521, 0.6322502493858337, 0.6447679996490479, 0.6506865620613098, 0.6550071239471436, 0.6632930636405945, 0.669921875, 0.6849254369735718, 0.6896010637283325, 0.6799538135528564, 0.6833274364471436, 0.690488874912262, 0.7050485610961914, 0.6951349377632141, 0.709043562412262, 0.7096058130264282, 0.697206437587738, 0.7056995630264282, 0.7126243114471436, 0.6989820003509521, 0.7067353129386902, 0.6988044381141663, 0.7115885615348816, 0.7141039371490479, 0.700254499912262, 0.7084221243858337, 0.713808000087738, 0.7062322497367859, 0.7127426862716675, 0.7008167505264282, 0.7003432512283325, 0.6999289989471436, 0.7143702507019043]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 6, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 18:00:59 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 6, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 18:00:59 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 18:01:00 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 18:01:01 I deeptables.m.preprocessor.py 249 - transform_X taken 0.9803919792175293s\n",
      "04-25 18:01:01 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 18:01:01 I deeptables.m.preprocessor.py 236 - transform_y taken 0.007978677749633789s\n",
      "04-25 18:01:01 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 18:01:01 I deeptables.m.preprocessor.py 249 - transform_X taken 0.15010619163513184s\n",
      "04-25 18:01:01 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 18:01:01 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009968280792236328s\n",
      "04-25 18:01:01 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 18:01:01 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 18:01:01 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 18:01:01 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 18:01:01 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 18:01:02 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 18:01:02 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 45ms/step - loss: 1.9454 - auc: 0.7191 - accuracy: 0.4672 - val_loss: 1.0163 - val_auc: 0.8076 - val_accuracy: 0.5454\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.4393 - auc: 0.8544 - accuracy: 0.6039 - val_loss: 0.9500 - val_auc: 0.8357 - val_accuracy: 0.5664\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.1077 - auc: 0.9111 - accuracy: 0.7078 - val_loss: 0.9434 - val_auc: 0.8522 - val_accuracy: 0.5988\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.8393 - auc: 0.9456 - accuracy: 0.7769 - val_loss: 0.9711 - val_auc: 0.8606 - val_accuracy: 0.6161\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.6911 - auc: 0.9614 - accuracy: 0.8098 - val_loss: 1.0058 - val_auc: 0.8649 - val_accuracy: 0.6298\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.6032 - auc: 0.9697 - accuracy: 0.8312 - val_loss: 1.0265 - val_auc: 0.8685 - val_accuracy: 0.6395\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5358 - auc: 0.9755 - accuracy: 0.8475 - val_loss: 1.0510 - val_auc: 0.8708 - val_accuracy: 0.6483\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4827 - auc: 0.9796 - accuracy: 0.8619 - val_loss: 1.0661 - val_auc: 0.8733 - val_accuracy: 0.6679\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4397 - auc: 0.9827 - accuracy: 0.8727 - val_loss: 1.0883 - val_auc: 0.8752 - val_accuracy: 0.6758\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.4066 - auc: 0.9851 - accuracy: 0.8826 - val_loss: 1.1011 - val_auc: 0.8771 - val_accuracy: 0.6805\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3762 - auc: 0.9872 - accuracy: 0.8912 - val_loss: 1.1180 - val_auc: 0.8775 - val_accuracy: 0.6834\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3511 - auc: 0.9888 - accuracy: 0.8974 - val_loss: 1.1460 - val_auc: 0.8780 - val_accuracy: 0.6876\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3324 - auc: 0.9899 - accuracy: 0.9035 - val_loss: 1.1633 - val_auc: 0.8792 - val_accuracy: 0.6901\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3131 - auc: 0.9911 - accuracy: 0.9082 - val_loss: 1.1797 - val_auc: 0.8797 - val_accuracy: 0.6929\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2970 - auc: 0.9920 - accuracy: 0.9124 - val_loss: 1.1961 - val_auc: 0.8800 - val_accuracy: 0.6949\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2844 - auc: 0.9927 - accuracy: 0.9158 - val_loss: 1.1952 - val_auc: 0.8819 - val_accuracy: 0.6988\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2718 - auc: 0.9934 - accuracy: 0.9188 - val_loss: 1.2236 - val_auc: 0.8806 - val_accuracy: 0.6996\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2610 - auc: 0.9938 - accuracy: 0.9222 - val_loss: 1.2306 - val_auc: 0.8824 - val_accuracy: 0.7029\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2513 - auc: 0.9943 - accuracy: 0.9234 - val_loss: 1.2563 - val_auc: 0.8816 - val_accuracy: 0.7014\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2429 - auc: 0.9947 - accuracy: 0.9259 - val_loss: 1.2770 - val_auc: 0.8810 - val_accuracy: 0.7022\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2348 - auc: 0.9950 - accuracy: 0.9280 - val_loss: 1.2713 - val_auc: 0.8826 - val_accuracy: 0.7046\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2281 - auc: 0.9953 - accuracy: 0.9292 - val_loss: 1.2811 - val_auc: 0.8838 - val_accuracy: 0.7050\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2225 - auc: 0.9956 - accuracy: 0.9302 - val_loss: 1.2976 - val_auc: 0.8823 - val_accuracy: 0.7051\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2195 - auc: 0.9956 - accuracy: 0.9312 - val_loss: 1.3164 - val_auc: 0.8818 - val_accuracy: 0.7048\n",
      "Epoch 25/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2156 - auc: 0.9958 - accuracy: 0.9323 - val_loss: 1.3273 - val_auc: 0.8817 - val_accuracy: 0.7048\n",
      "Epoch 26/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2111 - auc: 0.9960 - accuracy: 0.9330 - val_loss: 1.3277 - val_auc: 0.8828 - val_accuracy: 0.7091\n",
      "Epoch 27/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2072 - auc: 0.9961 - accuracy: 0.9337 - val_loss: 1.3314 - val_auc: 0.8837 - val_accuracy: 0.7081\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "04-25 18:06:26 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 18:06:26 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 18:06:26 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425180059_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.945447325706482, 1.4392606019973755, 1.1077195405960083, 0.8392563462257385, 0.6911115646362305, 0.6031787395477295, 0.5358471870422363, 0.482730507850647, 0.4396873712539673, 0.4066024422645569, 0.3761535882949829, 0.3510994017124176, 0.33239811658859253, 0.31310439109802246, 0.2970080077648163, 0.2843775451183319, 0.27179592847824097, 0.2609846293926239, 0.25127512216567993, 0.24288934469223022, 0.234805628657341, 0.22814001142978668, 0.22254598140716553, 0.2194790095090866, 0.2155984342098236, 0.21107950806617737, 0.20715658366680145], 'auc': [0.7191196084022522, 0.8543881177902222, 0.9110656976699829, 0.9455624222755432, 0.9614449143409729, 0.969734251499176, 0.975464403629303, 0.9795931577682495, 0.9826654195785522, 0.9851436614990234, 0.9871835708618164, 0.9887881875038147, 0.989927351474762, 0.9910984635353088, 0.9920035600662231, 0.9926697611808777, 0.9933527112007141, 0.9938413500785828, 0.9943118095397949, 0.994705855846405, 0.9950395226478577, 0.9952587485313416, 0.9955530166625977, 0.9956445097923279, 0.9958412051200867, 0.99599289894104, 0.9961221814155579], 'accuracy': [0.4672093689441681, 0.6038681864738464, 0.7078244686126709, 0.7768924832344055, 0.8097748160362244, 0.8311957120895386, 0.8474693894386292, 0.8619204163551331, 0.8726978302001953, 0.8826467990875244, 0.8911714553833008, 0.897436261177063, 0.9034754633903503, 0.908227801322937, 0.9123702645301819, 0.9158287644386292, 0.9188324809074402, 0.9221500158309937, 0.9234050512313843, 0.9258940815925598, 0.9280481338500977, 0.9292362332344055, 0.9302339553833008, 0.9311858415603638, 0.9323210716247559, 0.9329556226730347, 0.9336819052696228], 'val_loss': [1.0163381099700928, 0.9500375390052795, 0.9433906674385071, 0.9711006879806519, 1.005839467048645, 1.0265412330627441, 1.0509577989578247, 1.0660701990127563, 1.0882587432861328, 1.1010549068450928, 1.118028163909912, 1.1460248231887817, 1.1633143424987793, 1.1796753406524658, 1.196062445640564, 1.1952261924743652, 1.2236027717590332, 1.2305964231491089, 1.2563077211380005, 1.2769696712493896, 1.2712702751159668, 1.2810546159744263, 1.2976348400115967, 1.316441535949707, 1.3273462057113647, 1.3277454376220703, 1.3314313888549805], 'val_auc': [0.8075821995735168, 0.8356754779815674, 0.8522176742553711, 0.8605530261993408, 0.8648772239685059, 0.868471086025238, 0.870799720287323, 0.8733488321304321, 0.8751778602600098, 0.8770570158958435, 0.8775392174720764, 0.8780326843261719, 0.8791530728340149, 0.8796809911727905, 0.8799778819084167, 0.8819458484649658, 0.8805932998657227, 0.8823590278625488, 0.881557822227478, 0.8809703588485718, 0.8825914263725281, 0.8838312029838562, 0.8823497891426086, 0.8817983865737915, 0.8817357420921326, 0.8827520608901978, 0.8836575150489807], 'val_accuracy': [0.5453953742980957, 0.5663766860961914, 0.5987807512283325, 0.6160629987716675, 0.629823625087738, 0.6395004987716675, 0.6483191251754761, 0.667909562587738, 0.675840437412262, 0.6805161237716675, 0.6833866238594055, 0.6876183748245239, 0.6901337504386902, 0.6929154992103577, 0.6948981881141663, 0.6988044381141663, 0.6996034383773804, 0.7029474377632141, 0.7014381885528564, 0.702207624912262, 0.7046342492103577, 0.7049893736839294, 0.705078125, 0.7048413753509521, 0.7047526240348816, 0.7091027498245239, 0.708066999912262]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 6, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 18:06:27 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 6, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 18:06:27 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n",
      "04-25 18:06:27 I hypernets.t.toolbox.py 346 - 4 class detected, inferred as a [multiclass classification] task\n",
      "04-25 18:06:27 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "04-25 18:06:27 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.392467737197876s\n",
      "04-25 18:06:27 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "04-25 18:06:28 I deeptables.m.preprocessor.py 383 - Imputation taken 0.538067102432251s\n",
      "04-25 18:06:28 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "04-25 18:06:28 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.28375935554504395s\n",
      "04-25 18:06:28 I deeptables.m.preprocessor.py 196 - fit_transform taken 1.4401960372924805s\n",
      "04-25 18:06:28 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 18:06:28 I deeptables.m.preprocessor.py 249 - transform_X taken 0.13164758682250977s\n",
      "04-25 18:06:28 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 18:06:29 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009975433349609375s\n",
      "04-25 18:06:29 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 18:06:29 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 18:06:29 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 18:06:29 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 18:06:29 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 18:06:29 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 18:06:29 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 45ms/step - loss: 1.9416 - auc: 0.7206 - accuracy: 0.4660 - val_loss: 1.0377 - val_auc: 0.7961 - val_accuracy: 0.5107\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.4315 - auc: 0.8567 - accuracy: 0.6090 - val_loss: 0.9525 - val_auc: 0.8347 - val_accuracy: 0.5670\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.0988 - auc: 0.9126 - accuracy: 0.7114 - val_loss: 0.9582 - val_auc: 0.8479 - val_accuracy: 0.5938\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.8407 - auc: 0.9451 - accuracy: 0.7742 - val_loss: 0.9878 - val_auc: 0.8569 - val_accuracy: 0.6138\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.6910 - auc: 0.9607 - accuracy: 0.8065 - val_loss: 1.0207 - val_auc: 0.8621 - val_accuracy: 0.6284\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5935 - auc: 0.9701 - accuracy: 0.8312 - val_loss: 1.0305 - val_auc: 0.8695 - val_accuracy: 0.6439\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5245 - auc: 0.9761 - accuracy: 0.8499 - val_loss: 1.0686 - val_auc: 0.8708 - val_accuracy: 0.6499\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.4712 - auc: 0.9804 - accuracy: 0.8649 - val_loss: 1.0830 - val_auc: 0.8729 - val_accuracy: 0.6556\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4292 - auc: 0.9836 - accuracy: 0.8769 - val_loss: 1.1138 - val_auc: 0.8744 - val_accuracy: 0.6641\n",
      "Epoch 10/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3985 - auc: 0.9858 - accuracy: 0.8862 - val_loss: 1.1317 - val_auc: 0.8754 - val_accuracy: 0.6686\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3678 - auc: 0.9878 - accuracy: 0.8941 - val_loss: 1.1571 - val_auc: 0.8759 - val_accuracy: 0.6707\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3471 - auc: 0.9892 - accuracy: 0.8995 - val_loss: 1.1686 - val_auc: 0.8766 - val_accuracy: 0.6737\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.3269 - auc: 0.9904 - accuracy: 0.9051 - val_loss: 1.1866 - val_auc: 0.8769 - val_accuracy: 0.6747\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3078 - auc: 0.9914 - accuracy: 0.9095 - val_loss: 1.2081 - val_auc: 0.8774 - val_accuracy: 0.6790\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2947 - auc: 0.9921 - accuracy: 0.9131 - val_loss: 1.2093 - val_auc: 0.8796 - val_accuracy: 0.6840\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2822 - auc: 0.9928 - accuracy: 0.9166 - val_loss: 1.2277 - val_auc: 0.8797 - val_accuracy: 0.6845\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2698 - auc: 0.9934 - accuracy: 0.9199 - val_loss: 1.2683 - val_auc: 0.8783 - val_accuracy: 0.6852\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2608 - auc: 0.9939 - accuracy: 0.9218 - val_loss: 1.2736 - val_auc: 0.8787 - val_accuracy: 0.6863\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2512 - auc: 0.9943 - accuracy: 0.9234 - val_loss: 1.2803 - val_auc: 0.8793 - val_accuracy: 0.6892\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2431 - auc: 0.9947 - accuracy: 0.9255 - val_loss: 1.2969 - val_auc: 0.8792 - val_accuracy: 0.6886\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2385 - auc: 0.9949 - accuracy: 0.9271 - val_loss: 1.3019 - val_auc: 0.8806 - val_accuracy: 0.6910\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2303 - auc: 0.9952 - accuracy: 0.9289 - val_loss: 1.3126 - val_auc: 0.8801 - val_accuracy: 0.6904\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2254 - auc: 0.9954 - accuracy: 0.9304 - val_loss: 1.3225 - val_auc: 0.8803 - val_accuracy: 0.6925\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2213 - auc: 0.9956 - accuracy: 0.9310 - val_loss: 1.3437 - val_auc: 0.8796 - val_accuracy: 0.6916\n",
      "Epoch 25/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2158 - auc: 0.9958 - accuracy: 0.9321 - val_loss: 1.3503 - val_auc: 0.8809 - val_accuracy: 0.6935\n",
      "Epoch 26/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2136 - auc: 0.9959 - accuracy: 0.9325 - val_loss: 1.3552 - val_auc: 0.8801 - val_accuracy: 0.6943\n",
      "Epoch 27/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2085 - auc: 0.9961 - accuracy: 0.9339 - val_loss: 1.3666 - val_auc: 0.8801 - val_accuracy: 0.6921\n",
      "Epoch 28/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2062 - auc: 0.9962 - accuracy: 0.9342 - val_loss: 1.3871 - val_auc: 0.8795 - val_accuracy: 0.6922\n",
      "Epoch 29/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.2027 - auc: 0.9963 - accuracy: 0.9350 - val_loss: 1.3846 - val_auc: 0.8801 - val_accuracy: 0.6923\n",
      "Epoch 30/200\n",
      "277/277 [==============================] - 12s 42ms/step - loss: 0.1991 - auc: 0.9964 - accuracy: 0.9360 - val_loss: 1.4005 - val_auc: 0.8798 - val_accuracy: 0.6936\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00030: early stopping\n",
      "04-25 18:12:27 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 18:12:27 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 18:12:28 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425180627_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9416106939315796, 1.4314606189727783, 1.0987963676452637, 0.8406773209571838, 0.6909683346748352, 0.5935454964637756, 0.5244834423065186, 0.471221923828125, 0.4292103052139282, 0.39854735136032104, 0.3677932918071747, 0.3470925986766815, 0.32685792446136475, 0.30781030654907227, 0.29467517137527466, 0.28217965364456177, 0.2697597146034241, 0.26077815890312195, 0.2512456774711609, 0.24310258030891418, 0.23848110437393188, 0.23031717538833618, 0.22541654109954834, 0.22131231427192688, 0.2157742828130722, 0.21359969675540924, 0.2085312008857727, 0.20615078508853912, 0.20274192094802856, 0.19910533726215363], 'auc': [0.7205522656440735, 0.8567436933517456, 0.912642240524292, 0.9450673460960388, 0.9606999754905701, 0.9700772762298584, 0.976145327091217, 0.980368435382843, 0.9835659265518188, 0.9858241677284241, 0.9878408312797546, 0.9892152547836304, 0.990395188331604, 0.9914035201072693, 0.9921437501907349, 0.9928252100944519, 0.9934034943580627, 0.9938879013061523, 0.9942895174026489, 0.9946892857551575, 0.994905948638916, 0.995237410068512, 0.9954190850257874, 0.9956152439117432, 0.9958248138427734, 0.9959170818328857, 0.9961004257202148, 0.996177613735199, 0.9962795376777649, 0.9964395761489868], 'accuracy': [0.46595075726509094, 0.6089942455291748, 0.711445152759552, 0.7742025256156921, 0.8065348863601685, 0.8311957120895386, 0.8499196171760559, 0.8648571372032166, 0.8768755793571472, 0.886175811290741, 0.8940588235855103, 0.8995233774185181, 0.9051077365875244, 0.9095110893249512, 0.9131247401237488, 0.9165867567062378, 0.919861912727356, 0.9218397736549377, 0.9233803749084473, 0.9254815578460693, 0.9271315336227417, 0.9289224743843079, 0.9303714632987976, 0.9310271739959717, 0.9321024417877197, 0.9325149655342102, 0.9338969588279724, 0.934150755405426, 0.9350462555885315, 0.9359628558158875], 'val_loss': [1.0376895666122437, 0.9525318145751953, 0.9581725001335144, 0.9878305196762085, 1.0206713676452637, 1.0305439233779907, 1.0686436891555786, 1.083034873008728, 1.1138057708740234, 1.1316685676574707, 1.1570961475372314, 1.1685543060302734, 1.1866170167922974, 1.2080745697021484, 1.2093110084533691, 1.2276973724365234, 1.2683109045028687, 1.273633599281311, 1.2803118228912354, 1.2969300746917725, 1.301859974861145, 1.3125802278518677, 1.3224668502807617, 1.343726634979248, 1.3502565622329712, 1.3552361726760864, 1.3666081428527832, 1.3871092796325684, 1.3845932483673096, 1.4004799127578735], 'val_auc': [0.7960540056228638, 0.8346679210662842, 0.847851037979126, 0.8568525910377502, 0.8620678782463074, 0.8694586753845215, 0.8707524538040161, 0.8729033470153809, 0.8744009733200073, 0.8753858208656311, 0.8759021759033203, 0.8766359090805054, 0.8769127130508423, 0.8773989677429199, 0.8796011805534363, 0.8797179460525513, 0.8782748579978943, 0.8786558508872986, 0.8793041706085205, 0.8791552782058716, 0.8806164264678955, 0.8800529837608337, 0.8803300857543945, 0.8796289563179016, 0.8808723092079163, 0.8800792694091797, 0.880123496055603, 0.8794504404067993, 0.8801422715187073, 0.8798472881317139], 'val_accuracy': [0.5107421875, 0.5670276880264282, 0.59375, 0.6138435006141663, 0.6284031867980957, 0.6438801884651184, 0.6498579382896423, 0.6555693745613098, 0.6640920639038086, 0.6685606241226196, 0.6706913113594055, 0.6737097501754761, 0.6746863126754761, 0.6790364384651184, 0.6840376257896423, 0.6844815611839294, 0.6851917505264282, 0.6862571239471436, 0.6892163753509521, 0.6886245012283325, 0.6910215616226196, 0.6904001235961914, 0.6924715638160706, 0.6915838122367859, 0.6935073137283325, 0.6943359375, 0.6921164989471436, 0.6922052502632141, 0.6922940611839294, 0.6936256885528564]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 2, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 18:12:28 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 2, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 18:12:28 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 18:12:28 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 18:12:29 I deeptables.m.preprocessor.py 249 - transform_X taken 0.9689149856567383s\n",
      "04-25 18:12:29 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 18:12:29 I deeptables.m.preprocessor.py 236 - transform_y taken 0.006981372833251953s\n",
      "04-25 18:12:29 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 18:12:30 I deeptables.m.preprocessor.py 249 - transform_X taken 0.14162158966064453s\n",
      "04-25 18:12:30 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 18:12:30 I deeptables.m.preprocessor.py 236 - transform_y taken 0.000997781753540039s\n",
      "04-25 18:12:30 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 18:12:30 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 18:12:30 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 18:12:30 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 18:12:30 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 18:12:30 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 18:12:30 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 44ms/step - loss: 1.9465 - auc: 0.7197 - accuracy: 0.4832 - val_loss: 1.0310 - val_auc: 0.7991 - val_accuracy: 0.5282\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.4284 - auc: 0.8567 - accuracy: 0.6075 - val_loss: 0.9474 - val_auc: 0.8371 - val_accuracy: 0.5722\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.0764 - auc: 0.9159 - accuracy: 0.7192 - val_loss: 0.9537 - val_auc: 0.8496 - val_accuracy: 0.5954\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.8269 - auc: 0.9461 - accuracy: 0.7773 - val_loss: 0.9881 - val_auc: 0.8569 - val_accuracy: 0.6117\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.6950 - auc: 0.9600 - accuracy: 0.8057 - val_loss: 1.0151 - val_auc: 0.8626 - val_accuracy: 0.6332\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.6071 - auc: 0.9686 - accuracy: 0.8270 - val_loss: 1.0258 - val_auc: 0.8678 - val_accuracy: 0.6423\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5404 - auc: 0.9745 - accuracy: 0.8447 - val_loss: 1.0518 - val_auc: 0.8705 - val_accuracy: 0.6497\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4901 - auc: 0.9787 - accuracy: 0.8587 - val_loss: 1.0577 - val_auc: 0.8739 - val_accuracy: 0.6578\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4468 - auc: 0.9819 - accuracy: 0.8707 - val_loss: 1.0876 - val_auc: 0.8745 - val_accuracy: 0.6613\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4108 - auc: 0.9847 - accuracy: 0.8806 - val_loss: 1.1019 - val_auc: 0.8764 - val_accuracy: 0.6695\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3814 - auc: 0.9868 - accuracy: 0.8902 - val_loss: 1.1331 - val_auc: 0.8766 - val_accuracy: 0.6729\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3543 - auc: 0.9886 - accuracy: 0.8972 - val_loss: 1.1440 - val_auc: 0.8784 - val_accuracy: 0.6777\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3316 - auc: 0.9900 - accuracy: 0.9034 - val_loss: 1.1617 - val_auc: 0.8793 - val_accuracy: 0.6816\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3151 - auc: 0.9910 - accuracy: 0.9082 - val_loss: 1.1818 - val_auc: 0.8791 - val_accuracy: 0.6844\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2997 - auc: 0.9919 - accuracy: 0.9109 - val_loss: 1.2005 - val_auc: 0.8793 - val_accuracy: 0.6860\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2833 - auc: 0.9927 - accuracy: 0.9162 - val_loss: 1.2032 - val_auc: 0.8810 - val_accuracy: 0.6893\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2731 - auc: 0.9933 - accuracy: 0.9182 - val_loss: 1.2360 - val_auc: 0.8794 - val_accuracy: 0.6883\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2629 - auc: 0.9938 - accuracy: 0.9218 - val_loss: 1.2517 - val_auc: 0.8796 - val_accuracy: 0.6897\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2531 - auc: 0.9942 - accuracy: 0.9231 - val_loss: 1.2517 - val_auc: 0.8808 - val_accuracy: 0.6915\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2442 - auc: 0.9946 - accuracy: 0.9256 - val_loss: 1.2698 - val_auc: 0.8805 - val_accuracy: 0.6942\n",
      "Epoch 21/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2377 - auc: 0.9949 - accuracy: 0.9273 - val_loss: 1.2790 - val_auc: 0.8814 - val_accuracy: 0.6972\n",
      "Epoch 22/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2326 - auc: 0.9952 - accuracy: 0.9283 - val_loss: 1.2897 - val_auc: 0.8809 - val_accuracy: 0.6951\n",
      "Epoch 23/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2259 - auc: 0.9954 - accuracy: 0.9296 - val_loss: 1.3048 - val_auc: 0.8806 - val_accuracy: 0.6962\n",
      "Epoch 24/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2224 - auc: 0.9956 - accuracy: 0.9306 - val_loss: 1.3031 - val_auc: 0.8814 - val_accuracy: 0.6961\n",
      "Epoch 25/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2162 - auc: 0.9958 - accuracy: 0.9324 - val_loss: 1.3201 - val_auc: 0.8813 - val_accuracy: 0.6957\n",
      "Epoch 26/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2117 - auc: 0.9960 - accuracy: 0.9331 - val_loss: 1.3332 - val_auc: 0.8817 - val_accuracy: 0.6980\n",
      "Epoch 27/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2087 - auc: 0.9961 - accuracy: 0.9341 - val_loss: 1.3480 - val_auc: 0.8811 - val_accuracy: 0.6982\n",
      "Epoch 28/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2056 - auc: 0.9962 - accuracy: 0.9341 - val_loss: 1.3530 - val_auc: 0.8808 - val_accuracy: 0.7098\n",
      "Epoch 29/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.2011 - auc: 0.9963 - accuracy: 0.9353 - val_loss: 1.3629 - val_auc: 0.8818 - val_accuracy: 0.7001\n",
      "Epoch 30/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1988 - auc: 0.9964 - accuracy: 0.9358 - val_loss: 1.3858 - val_auc: 0.8813 - val_accuracy: 0.6979\n",
      "Epoch 31/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1963 - auc: 0.9965 - accuracy: 0.9364 - val_loss: 1.3773 - val_auc: 0.8815 - val_accuracy: 0.6984\n",
      "Epoch 32/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1937 - auc: 0.9966 - accuracy: 0.9370 - val_loss: 1.3990 - val_auc: 0.8811 - val_accuracy: 0.6982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.1909 - auc: 0.9967 - accuracy: 0.9377 - val_loss: 1.4017 - val_auc: 0.8802 - val_accuracy: 0.6977\n",
      "Epoch 34/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.1890 - auc: 0.9968 - accuracy: 0.9379 - val_loss: 1.4184 - val_auc: 0.8814 - val_accuracy: 0.6998\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00034: early stopping\n",
      "04-25 18:19:18 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 18:19:18 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 18:19:19 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425181228_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9465446472167969, 1.4283761978149414, 1.0763881206512451, 0.8268869519233704, 0.6949884295463562, 0.6071399450302124, 0.5404301881790161, 0.49006256461143494, 0.4468214809894562, 0.4107854962348938, 0.38138633966445923, 0.35429975390434265, 0.33158355951309204, 0.31511858105659485, 0.29974666237831116, 0.2832586467266083, 0.27307143807411194, 0.26289576292037964, 0.25313687324523926, 0.24420702457427979, 0.23766472935676575, 0.23259291052818298, 0.22588856518268585, 0.22240570187568665, 0.21622039377689362, 0.21174663305282593, 0.20871371030807495, 0.2056093066930771, 0.20106656849384308, 0.19881212711334229, 0.19625237584114075, 0.19370773434638977, 0.19090726971626282, 0.18901407718658447], 'auc': [0.7196581363677979, 0.8567481637001038, 0.9159025549888611, 0.9461289644241333, 0.9600480794906616, 0.968641459941864, 0.9745131731033325, 0.9786550998687744, 0.9819445610046387, 0.9846721291542053, 0.9867734909057617, 0.9885948300361633, 0.989995539188385, 0.9909939169883728, 0.9918553233146667, 0.9927325248718262, 0.9932900667190552, 0.9937816858291626, 0.9942290186882019, 0.9946256279945374, 0.9949482083320618, 0.9951896071434021, 0.9953970313072205, 0.9955635666847229, 0.9958415627479553, 0.9959730505943298, 0.9960618615150452, 0.996211051940918, 0.996344804763794, 0.996422290802002, 0.9965266585350037, 0.9966206550598145, 0.9967252016067505, 0.9967617988586426], 'accuracy': [0.48321157693862915, 0.607481837272644, 0.7191871404647827, 0.7773085236549377, 0.805688738822937, 0.8270144462585449, 0.8447089195251465, 0.8587439656257629, 0.8707129955291748, 0.8805914521217346, 0.8902442455291748, 0.8972282409667969, 0.9034401774406433, 0.9081678986549377, 0.9109036326408386, 0.9161636829376221, 0.9182296395301819, 0.9218186140060425, 0.9231019020080566, 0.9256190657615662, 0.9272549152374268, 0.928291380405426, 0.9296099543571472, 0.9306076765060425, 0.9324444532394409, 0.933061420917511, 0.934087336063385, 0.9341049194335938, 0.9352789521217346, 0.9358359575271606, 0.9363753795623779, 0.9369782209396362, 0.9376938939094543, 0.9378913044929504], 'val_loss': [1.0309901237487793, 0.9474223852157593, 0.9537097215652466, 0.9881005883216858, 1.015054702758789, 1.0257618427276611, 1.0518319606781006, 1.0576902627944946, 1.087559461593628, 1.1019072532653809, 1.1331062316894531, 1.1440012454986572, 1.1616616249084473, 1.1817823648452759, 1.2004653215408325, 1.203157901763916, 1.236039161682129, 1.2516818046569824, 1.251671552658081, 1.2697774171829224, 1.2790204286575317, 1.2896987199783325, 1.304797887802124, 1.3030511140823364, 1.3201266527175903, 1.333164095878601, 1.3480291366577148, 1.3530479669570923, 1.3629279136657715, 1.385801911354065, 1.377346396446228, 1.398966908454895, 1.4017362594604492, 1.4184331893920898], 'val_auc': [0.7991277575492859, 0.8370729088783264, 0.8495804667472839, 0.8568626642227173, 0.862637996673584, 0.8677978515625, 0.8705103993415833, 0.8739303350448608, 0.8744500279426575, 0.8764069080352783, 0.8765624165534973, 0.8784098625183105, 0.8793026804924011, 0.8790544271469116, 0.8793095946311951, 0.8809539079666138, 0.8794366121292114, 0.8795981407165527, 0.8808016777038574, 0.8805150389671326, 0.8813601732254028, 0.8808817863464355, 0.880622386932373, 0.881421685218811, 0.8813189268112183, 0.881663978099823, 0.8811203837394714, 0.8808306455612183, 0.8817859292030334, 0.8812642693519592, 0.881507396697998, 0.8811172246932983, 0.8802202343940735, 0.8814002871513367], 'val_accuracy': [0.5282315611839294, 0.5722360610961914, 0.5953776240348816, 0.6116536259651184, 0.6331676244735718, 0.6423118114471436, 0.6497395634651184, 0.6578480005264282, 0.6612511873245239, 0.6695371866226196, 0.672910749912262, 0.677675187587738, 0.681640625, 0.6844223737716675, 0.6860203742980957, 0.6893051862716675, 0.6883286237716675, 0.6896602511405945, 0.6915246248245239, 0.6941879987716675, 0.697206437587738, 0.6950757503509521, 0.6961706876754761, 0.6960819363594055, 0.6956971883773804, 0.6980054378509521, 0.6982126235961914, 0.7097538113594055, 0.7000769376754761, 0.6978574991226196, 0.6983901262283325, 0.698183000087738, 0.6976503133773804, 0.6998106241226196]}\n",
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 2, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 18:19:19 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'max', 'senet_reduction_ratio': 2, 'bilinear_type': 'full'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 18:19:19 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 18:19:19 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 18:19:21 I deeptables.m.preprocessor.py 249 - transform_X taken 1.0103240013122559s\n",
      "04-25 18:19:21 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 18:19:21 I deeptables.m.preprocessor.py 236 - transform_y taken 0.006982088088989258s\n",
      "04-25 18:19:21 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 18:19:21 I deeptables.m.preprocessor.py 249 - transform_X taken 0.15059685707092285s\n",
      "04-25 18:19:21 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 18:19:21 I deeptables.m.preprocessor.py 236 - transform_y taken 0.000997781753540039s\n",
      "04-25 18:19:21 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 18:19:21 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 18:19:21 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 18:19:21 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 18:19:21 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 18:19:21 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 18:19:21 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 45ms/step - loss: 1.9404 - auc: 0.7221 - accuracy: 0.4800 - val_loss: 1.0430 - val_auc: 0.7921 - val_accuracy: 0.5064\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.4311 - auc: 0.8571 - accuracy: 0.6091 - val_loss: 0.9645 - val_auc: 0.8294 - val_accuracy: 0.5600\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1.0739 - auc: 0.9164 - accuracy: 0.7194 - val_loss: 0.9236 - val_auc: 0.8551 - val_accuracy: 0.6035\n",
      "Epoch 4/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.7878 - auc: 0.9518 - accuracy: 0.7892 - val_loss: 0.9331 - val_auc: 0.8648 - val_accuracy: 0.6237\n",
      "Epoch 5/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.6096 - auc: 0.9692 - accuracy: 0.8310 - val_loss: 0.9449 - val_auc: 0.8743 - val_accuracy: 0.6440\n",
      "Epoch 6/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.5024 - auc: 0.9784 - accuracy: 0.8595 - val_loss: 0.9563 - val_auc: 0.8790 - val_accuracy: 0.6573\n",
      "Epoch 7/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.4291 - auc: 0.9839 - accuracy: 0.8795 - val_loss: 0.9700 - val_auc: 0.8828 - val_accuracy: 0.6661\n",
      "Epoch 8/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3769 - auc: 0.9874 - accuracy: 0.8931 - val_loss: 0.9777 - val_auc: 0.8867 - val_accuracy: 0.6901\n",
      "Epoch 9/200\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 0.3398 - auc: 0.9898 - accuracy: 0.9028 - val_loss: 1.0024 - val_auc: 0.8880 - val_accuracy: 0.6820\n",
      "Epoch 10/200\n",
      "277/277 [==============================] - 13s 46ms/step - loss: 0.3083 - auc: 0.9915 - accuracy: 0.9104 - val_loss: 1.0169 - val_auc: 0.8896 - val_accuracy: 0.6833\n",
      "Epoch 11/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.2834 - auc: 0.9928 - accuracy: 0.9170 - val_loss: 1.0387 - val_auc: 0.8900 - val_accuracy: 0.6850\n",
      "Epoch 12/200\n",
      "277/277 [==============================] - 12s 45ms/step - loss: 0.2662 - auc: 0.9936 - accuracy: 0.9207 - val_loss: 1.0576 - val_auc: 0.8902 - val_accuracy: 0.7021\n",
      "Epoch 13/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 0.2529 - auc: 0.9942 - accuracy: 0.9236 - val_loss: 1.0553 - val_auc: 0.8919 - val_accuracy: 0.6918\n",
      "Epoch 14/200\n",
      "277/277 [==============================] - 13s 47ms/step - loss: 0.2419 - auc: 0.9947 - accuracy: 0.9270 - val_loss: 1.0667 - val_auc: 0.8925 - val_accuracy: 0.6922\n",
      "Epoch 15/200\n",
      "277/277 [==============================] - 12s 45ms/step - loss: 0.2303 - auc: 0.9952 - accuracy: 0.9292 - val_loss: 1.0731 - val_auc: 0.8940 - val_accuracy: 0.6934\n",
      "Epoch 16/200\n",
      "277/277 [==============================] - 13s 46ms/step - loss: 0.2237 - auc: 0.9955 - accuracy: 0.9302 - val_loss: 1.0934 - val_auc: 0.8914 - val_accuracy: 0.6921\n",
      "Epoch 17/200\n",
      "277/277 [==============================] - 13s 46ms/step - loss: 0.2172 - auc: 0.9958 - accuracy: 0.9324 - val_loss: 1.1003 - val_auc: 0.8918 - val_accuracy: 0.6931\n",
      "Epoch 18/200\n",
      "277/277 [==============================] - 13s 46ms/step - loss: 0.2111 - auc: 0.9959 - accuracy: 0.9336 - val_loss: 1.0912 - val_auc: 0.8934 - val_accuracy: 0.6944\n",
      "Epoch 19/200\n",
      "277/277 [==============================] - 13s 45ms/step - loss: 0.2074 - auc: 0.9961 - accuracy: 0.9347 - val_loss: 1.1175 - val_auc: 0.8914 - val_accuracy: 0.6921\n",
      "Epoch 20/200\n",
      "277/277 [==============================] - 13s 46ms/step - loss: 0.2035 - auc: 0.9962 - accuracy: 0.9352 - val_loss: 1.1183 - val_auc: 0.8920 - val_accuracy: 0.6939\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "04-25 18:23:31 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "04-25 18:23:31 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "04-25 18:23:31 I deeptables.m.deeptable.py 704 - Model has been saved to:E:/temp/dt_20230425181919_fibi_nets/fibi_nets.h5\n",
      "{'loss': [1.9403924942016602, 1.4311250448226929, 1.0739295482635498, 0.787805438041687, 0.60955810546875, 0.502439022064209, 0.42908090353012085, 0.3768634796142578, 0.3398132920265198, 0.3082652688026428, 0.2834186851978302, 0.26616770029067993, 0.2529342770576477, 0.24193459749221802, 0.23033085465431213, 0.2237403392791748, 0.2171918749809265, 0.21109633147716522, 0.2074326127767563, 0.20349130034446716], 'auc': [0.7221073508262634, 0.8570883870124817, 0.9163910746574402, 0.9517902135848999, 0.9692288041114807, 0.9783995747566223, 0.9838806390762329, 0.9873691201210022, 0.9897745251655579, 0.9914905428886414, 0.9928427338600159, 0.9935693144798279, 0.9942184090614319, 0.99470454454422, 0.9951632618904114, 0.9954501986503601, 0.9957572817802429, 0.9959427714347839, 0.9961157441139221, 0.9962314963340759], 'accuracy': [0.47999632358551025, 0.6090788841247559, 0.7193563580513, 0.7891576886177063, 0.8310018181800842, 0.8594878315925598, 0.8795232176780701, 0.893061101436615, 0.9027597308158875, 0.9103677868843079, 0.9170450568199158, 0.9206975102424622, 0.9235637187957764, 0.9270151853561401, 0.9291622042655945, 0.9302269220352173, 0.9323704242706299, 0.93359375, 0.9347430467605591, 0.9352471828460693], 'val_loss': [1.0430173873901367, 0.9645281434059143, 0.9235701560974121, 0.9330510497093201, 0.9448632597923279, 0.9562519788742065, 0.9699925780296326, 0.9777002930641174, 1.0023607015609741, 1.016916275024414, 1.0387166738510132, 1.0576159954071045, 1.0553228855133057, 1.0666751861572266, 1.0730595588684082, 1.093393325805664, 1.1002575159072876, 1.091166615486145, 1.1174699068069458, 1.1182804107666016], 'val_auc': [0.7921083569526672, 0.829403817653656, 0.8550593852996826, 0.8648313879966736, 0.8742895722389221, 0.8790177702903748, 0.8828070759773254, 0.8867263197898865, 0.8880448937416077, 0.8895561695098877, 0.8899825215339661, 0.890202522277832, 0.8918524980545044, 0.8925333619117737, 0.8939952850341797, 0.8914247751235962, 0.8918417692184448, 0.8934236764907837, 0.8913518190383911, 0.8919658660888672], 'val_accuracy': [0.5063624382019043, 0.5599846243858337, 0.603515625, 0.6236683130264282, 0.6439689993858337, 0.6573449373245239, 0.6661339998245239, 0.6901337504386902, 0.6819661259651184, 0.6832682490348816, 0.6849846243858337, 0.7021484375, 0.6917613744735718, 0.6921756863594055, 0.693418562412262, 0.6920868754386902, 0.6930634379386902, 0.694395124912262, 0.6920573115348816, 0.6938920617103577]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 2, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "===================================================\n",
      "04-25 18:23:32 I deeptables.m.deeptable.py 338 - X.Shape=(284500, 80), y.Shape=(284500,), batch_size=1024, config=ModelConfig(name='conf-1', nets=['fibi_nets'], categorical_columns=['Protocol', 'Fwd PSH Flags', 'Source IP', 'Destination IP', 'Source Port', 'Destination Port', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags'], exclude_columns=[], task='auto', pos_label=None, metrics=['AUC', 'accuracy'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=100, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0.2, embedding_dropout=0.3, stacking_op='add', output_use_bias=True, apply_class_weight=True, optimizer='auto', loss='auto', dnn_params={'hidden_units': ((128, 0.2, False), (128, 0.2, False)), 'activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_widths': (7, 7), 'fg_pool_widths': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 2, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 5}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 5, 'dropout_rate': 0.3}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir='E:/temp', monitor_metric=None, earlystopping_patience=5, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "04-25 18:23:32 I deeptables.m.deeptable.py 339 - metrics:['AUC', 'accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-25 18:23:32 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 18:23:33 I deeptables.m.preprocessor.py 249 - transform_X taken 0.990351676940918s\n",
      "04-25 18:23:33 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 18:23:33 I deeptables.m.preprocessor.py 236 - transform_y taken 0.007977962493896484s\n",
      "04-25 18:23:33 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "04-25 18:23:34 I deeptables.m.preprocessor.py 249 - transform_X taken 0.1381371021270752s\n",
      "04-25 18:23:34 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "04-25 18:23:34 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009980201721191406s\n",
      "04-25 18:23:34 I deeptables.m.deeptable.py 354 - Training...\n",
      "04-25 18:23:34 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_auc, patience:5, mode:max\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "04-25 18:23:34 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 18:23:34 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=1024, shuffle=True, drop_remainder=True\n",
      "04-25 18:23:34 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "04-25 18:23:34 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (6)', 'input_continuous_all: (64)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [2365, 48333, 4509, 17154, 5, 4]\n",
      "output_dims: [100, 100, 100, 100, 100, 100]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0.2\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 664)\n",
      "---------------------------------------------------------\n",
      "nets: ['fibi_nets']\n",
      "---------------------------------------------------------\n",
      "fibi: input_shape (None, 6, 100), output_shape (None, 30, 100)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: softmax, output_shape: (None, 4), use_bias: True\n",
      "loss: categorical_crossentropy\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "04-25 18:23:34 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/200\n",
      "277/277 [==============================] - 15s 46ms/step - loss: 1.9464 - auc: 0.7198 - accuracy: 0.4820 - val_loss: 1.0328 - val_auc: 0.7985 - val_accuracy: 0.5144\n",
      "Epoch 2/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 1.4344 - auc: 0.8561 - accuracy: 0.6083 - val_loss: 0.9632 - val_auc: 0.8316 - val_accuracy: 0.5645\n",
      "Epoch 3/200\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 1.1007 - auc: 0.9117 - accuracy: 0.7094 - val_loss: 0.9483 - val_auc: 0.8504 - val_accuracy: 0.5943\n",
      "Epoch 4/200\n",
      " 89/277 [========>.....................] - ETA: 8s - loss: 0.7840 - auc: 0.9518 - accuracy: 0.7906"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25068\\616260947.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 运行超参数搜索\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgp_minimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective_with_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\skopt\\optimizer\\gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         callback=callback, n_jobs=n_jobs, model_queue_size=model_queue_size)\n\u001b[0m",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\skopt\\optimizer\\base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m         \u001b[0mnext_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25068\\3391492355.py\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(params, X_train, y_train, X_val, y_val, task)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDeepTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     model, history = dt.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val),class_weight=class_weight,\n\u001b[1;32m---> 62\u001b[1;33m                         batch_size=1024,shuffle=True)\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\deeptables\\models\\deeptable.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    366\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m                             \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m                             max_queue_size=max_queue_size, workers=workers, use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    369\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'{\"+\".join(self.nets)}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training finished.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\deeptables\\models\\deepmodel.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    118\u001b[0m                                  \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                                  \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                                  \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m                                  )\n\u001b[0;32m    122\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training finished.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3024\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1961\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\dxtf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "objective_with_data = partial(objective, X_train=x_train, y_train=y_train, X_val=x_test, y_val=y_test)\n",
    "\n",
    "# 运行超参数搜索\n",
    "result = gp_minimize(objective_with_data, param_space, n_calls=50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3773609d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  ['max', 5, 'full']\n"
     ]
    }
   ],
   "source": [
    "best_params = result.x\n",
    "print(\"Best parameters found: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c057f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dxtf",
   "language": "python",
   "name": "dxtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
